"""
æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨
ä¸“é—¨ä¼˜åŒ–ç”¨äºDOCXæ–‡ä»¶æ ¼å¼çš„å‚è€ƒæ–‡çŒ®æå–
- ç²¾ç¡®è¾¹ç•Œæ£€æµ‹ï¼šæ™ºèƒ½è¯†åˆ«"è‡´è°¢ä¸å£°æ˜"ç­‰ç»“æŸæ ‡è®°
- å¤šæ¨¡å¼åŒ¹é…ï¼šæ”¯æŒï¼»1ï¼½ã€[1]ã€1.ã€(1)ç­‰å¤šç§ç¼–å·æ ¼å¼
- å†…å®¹éªŒè¯ï¼šéªŒè¯å‚è€ƒæ–‡çŒ®çš„å®Œæ•´æ€§å’Œæ ¼å¼æ­£ç¡®æ€§
"""

import re
import os
import time
from typing import List, Optional, Tuple, Dict, Any
from pathlib import Path

class SmartReferenceExtractor:
    """æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨ - ä¸“é—¨ç”¨äºDOCXæ–‡ä»¶"""
    
    def __init__(self, ai_client=None):
        # ä¸å†ä½¿ç”¨AIå®¢æˆ·ç«¯ï¼Œä¸“æ³¨äºDOCXå¤„ç†
        self.extraction_stats = {
            'method_used': '',
            'total_found': 0,
            'processing_time': 0.0,
            'success': False
        }
    
    def extract_references(self, text: str, source_format: str = 'docx', 
                          source_path: str = '') -> Tuple[List[str], Dict[str, Any]]:
        """
        æ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®ï¼ˆä¸“é—¨ç”¨äºDOCXæ–‡ä»¶ï¼‰
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
            source_format: æºæ ¼å¼ (å›ºå®šä¸º'docx')
            source_path: æºæ–‡ä»¶è·¯å¾„
        
        Returns:
            Tuple[List[str], Dict[str, Any]]: (å‚è€ƒæ–‡çŒ®åˆ—è¡¨, æå–ç»Ÿè®¡ä¿¡æ¯)
        """
        start_time = time.time()
        
        print(f"ğŸ“„ æ£€æµ‹åˆ°æ–‡æ¡£æ ¼å¼: DOCX")
        
        # å®šä½å‚è€ƒæ–‡çŒ®åŒºåŸŸ
        ref_section = self._locate_reference_section(text)
        if not ref_section:
            print("âŒ æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®åŒºåŸŸ")
            return [], {'error': 'æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®åŒºåŸŸ'}
        
        print(f"ğŸ“ å‚è€ƒæ–‡çŒ®åŒºåŸŸé•¿åº¦: {len(ref_section):,} å­—ç¬¦")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ­£åˆ™æå–æ–¹æ³•
        references = self._extract_with_regex(ref_section)
        method = 'ä¼ ç»Ÿæ­£åˆ™æå–'
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.extraction_stats.update({
            'method_used': method,
            'total_found': len(references),
            'processing_time': processing_time,
            'success': len(references) > 0
        })
        
        print(f"âœ… æå–å®Œæˆ: {len(references)} æ¡å‚è€ƒæ–‡çŒ® (ç”¨æ—¶ {processing_time:.2f}ç§’)")
        print(f"ğŸ”§ ä½¿ç”¨æ–¹æ³•: {method}")
        
        return references, self.extraction_stats
    
    def _locate_reference_section(self, text: str) -> str:
        """æ™ºèƒ½å®šä½å‚è€ƒæ–‡çŒ®åŒºåŸŸï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äºdocxæ ¼å¼"""
        start_pos, end_pos = self._get_reference_boundaries(text)
        if start_pos != -1 and end_pos != -1:
            return text[start_pos:end_pos]
        return ""
    
    def _get_reference_boundaries(self, text: str) -> tuple[int, int]:
        """è·å–å‚è€ƒæ–‡çŒ®çš„å¼€å§‹å’Œç»“æŸä½ç½®åæ ‡"""
        print("ğŸ” å¼€å§‹å®šä½å‚è€ƒæ–‡çŒ®åŒºåŸŸ...")
        
        # å¤šç§æ¨¡å¼æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®æ ‡é¢˜
        patterns = [
            r'#+\s*å‚è€ƒæ–‡çŒ®\s*\n',
            r'å‚è€ƒæ–‡çŒ®\s*\n',
            r'References\s*\n',
            r'REFERENCES\s*\n',
            r'å‚\s*è€ƒ\s*æ–‡\s*çŒ®',
        ]
        
        ref_start_pos = None
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                ref_start_pos = match.start()
                print(f"âœ… æ‰¾åˆ°å‚è€ƒæ–‡çŒ®æ ‡é¢˜: '{match.group().strip()}' (ä½ç½®: {ref_start_pos})")
                break
        
        if ref_start_pos is None:
            print("âŒ æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®æ ‡é¢˜")
            return -1, -1
        
        # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®ç»“æŸä½ç½®
        # ä¸“é—¨å¤„ç†"è‡´è°¢ä¸å£°æ˜"è¾¹ç•Œé—®é¢˜
        end_markers = [
            'è‡´è°¢ä¸å£°æ˜',     # ä¼˜å…ˆåŒ¹é…å®Œæ•´çš„"è‡´è°¢ä¸å£°æ˜"
            'è‡´è°¢',          # å…¶æ¬¡åŒ¹é…"è‡´è°¢"
            'ACKNOWLEDGMENT',
            'ACKNOWLEDGEMENT', 
            'é™„å½•',
            'APPENDIX',
            'ä¸ªäººç®€å†',
            'ä½œè€…ç®€ä»‹',
            'æ”»è¯»å­¦ä½æœŸé—´å‘è¡¨',
        ]
        
        remaining_text = text[ref_start_pos:]
        ref_end_relative = len(remaining_text)  # é»˜è®¤åˆ°æ–‡æ¡£æœ«å°¾
        
        print("ğŸ” æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®ç»“æŸæ ‡è®°...")
        for marker in end_markers:
            marker_pos = remaining_text.find(marker)
            if marker_pos != -1:
                print(f"   âœ… æ‰¾åˆ°ç»“æŸæ ‡è®°: '{marker}' (ç›¸å¯¹ä½ç½®: {marker_pos})")
                ref_end_relative = marker_pos
                break
        
        # è®¡ç®—ç»å¯¹ç»“æŸä½ç½®
        ref_end_pos = ref_start_pos + ref_end_relative
        
        print(f"ğŸ“ å‚è€ƒæ–‡çŒ®åŒºåŸŸå®šä½å®Œæˆ:")
        print(f"   ğŸ“ æ€»é•¿åº¦: {ref_end_relative:,} å­—ç¬¦")
        print(f"   ğŸ“‹ å¼€å§‹ä½ç½®: {ref_start_pos}")
        print(f"   ğŸ“‹ ç»“æŸä½ç½®: {ref_end_pos}")
        
        # éªŒè¯æå–çš„å†…å®¹
        if ref_end_relative < 100:
            print("âš ï¸ å‚è€ƒæ–‡çŒ®åŒºåŸŸè¿‡çŸ­ï¼Œå¯èƒ½æå–æœ‰è¯¯")
            return -1, -1
        
        # ç»Ÿè®¡å‚è€ƒæ–‡çŒ®æ¡ç›®æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
        ref_section = text[ref_start_pos:ref_end_pos]
        ref_count_estimate = len(re.findall(r'ï¼»\d+ï¼½|\[\d+\]|\n\d+\.', ref_section))
        print(f"   ğŸ“Š ä¼°è®¡å‚è€ƒæ–‡çŒ®æ¡ç›®æ•°: {ref_count_estimate}")
        
        return ref_start_pos, ref_end_pos
    
    def _extract_with_ai(self, ref_text: str) -> List[str]:
        """ä½¿ç”¨AIæ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®ï¼ˆé€‚åˆPDFæ ¼å¼ï¼‰"""
        print("ğŸ¤– ä½¿ç”¨AIæ™ºèƒ½æå–æ–¹æ³•...")
        
        if not self.ai_client:
            print("âŒ AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ­£åˆ™æ–¹æ³•")
            return self._extract_with_regex(ref_text)
        
        # åˆ†æ®µå¤„ç†å¤§æ–‡æ¡£
        if len(ref_text) > 20000:
            return self._extract_with_ai_chunked(ref_text)
        
        # æ„å»ºæ™ºèƒ½æç¤ºè¯
        prompt = f"""è¯·ä»ä»¥ä¸‹å‚è€ƒæ–‡çŒ®æ–‡æœ¬ä¸­ç²¾ç¡®æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ã€‚è¿™æ®µæ–‡æœ¬å¯èƒ½æ¥è‡ªPDFè½¬æ¢ï¼Œæ ¼å¼å¯èƒ½ä¸è§„æ•´ã€‚

é‡è¦è¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼Œæ— è®ºç¼–å·æ ¼å¼ï¼ˆï¼»1ï¼½ã€[1]ã€(1)ã€1.ç­‰ï¼‰
2. å¤„ç†å…¨è§’åŠè§’å­—ç¬¦æ··ç”¨é—®é¢˜
3. æ¸…ç†PDFè½¬æ¢äº§ç”Ÿçš„æ ¼å¼é”™è¯¯ï¼ˆå¤šä½™ç©ºæ ¼ã€æ–­è¡Œç­‰ï¼‰
4. æ¯æ¡å‚è€ƒæ–‡çŒ®é‡ç»„ä¸ºä¸€è¡Œå®Œæ•´è®°å½•
5. ä¿æŒä½œè€…ã€æ ‡é¢˜ã€æœŸåˆŠã€å¹´ä»½ç­‰å…³é”®ä¿¡æ¯å®Œæ•´
6. æŒ‰åŸç¼–å·é¡ºåºè¾“å‡º
7. è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸€æ¡å‚è€ƒæ–‡çŒ®ï¼Œç¼–å·åœ¨å‰

æ–‡æœ¬å†…å®¹ï¼š
{ref_text[:15000]}

è¯·æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®ï¼š"""

        try:
            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                content = response.content.strip()
                references = self._parse_ai_response(content)
                print(f"   âœ… AIæå–åˆ° {len(references)} æ¡å‚è€ƒæ–‡çŒ®")
                return references
        except Exception as e:
            print(f"   âŒ AIæå–å¤±è´¥: {e}")
        
        # AIå¤±è´¥æ—¶å›é€€åˆ°æ­£åˆ™æ–¹æ³•
        return self._extract_with_regex(ref_text)
    
    def _extract_with_ai_chunked(self, ref_text: str) -> List[str]:
        """åˆ†å—å¤„ç†å¤§å‹å‚è€ƒæ–‡çŒ®æ–‡æœ¬"""
        print("ğŸ“¦ æ–‡æ¡£è¾ƒå¤§ï¼Œåˆ†å—å¤„ç†...")
        
        # æ™ºèƒ½åˆ†å—ï¼šåœ¨å¼•ç”¨æ¡ç›®è¾¹ç•Œåˆ†å‰²
        chunks = self._smart_chunk_text(ref_text, max_size=15000)
        all_references = []
        
        for i, chunk in enumerate(chunks, 1):
            print(f"ğŸ”„ å¤„ç†ç¬¬ {i}/{len(chunks)} å—...")
            
            prompt = f"""ä»ä»¥ä¸‹å‚è€ƒæ–‡çŒ®ç‰‡æ®µä¸­æå–æ‰€æœ‰å®Œæ•´çš„å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼š

è¦æ±‚ï¼š
1. åªæå–å®Œæ•´çš„å‚è€ƒæ–‡çŒ®æ¡ç›®
2. å¤„ç†æ ¼å¼é—®é¢˜ï¼ˆå…¨è§’æ‹¬å·ã€æ¢è¡Œç­‰ï¼‰
3. æ¯è¡Œè¾“å‡ºä¸€æ¡å‚è€ƒæ–‡çŒ®
4. ä¿æŒç¼–å·é¡ºåº

æ–‡æœ¬ç‰‡æ®µï¼š
{chunk}

å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼š"""
            
            try:
                response = self.ai_client.send_message(prompt)
                if response and hasattr(response, 'content'):
                    chunk_refs = self._parse_ai_response(response.content)
                    all_references.extend(chunk_refs)
                    print(f"   âœ… æå–åˆ° {len(chunk_refs)} æ¡å‚è€ƒæ–‡çŒ®")
            except Exception as e:
                print(f"   âŒ ç¬¬{i}å—æå–å¤±è´¥: {e}")
        
        # å»é‡å¹¶æ’åº
        unique_refs = self._deduplicate_references(all_references)
        print(f"ğŸ“Š åˆå¹¶ç»“æœ: {len(unique_refs)} æ¡å”¯ä¸€å‚è€ƒæ–‡çŒ®")
        
        return unique_refs
    
    def _extract_with_regex(self, ref_text: str) -> List[str]:
        """ä½¿ç”¨ä¼ ç»Ÿæ­£åˆ™è¡¨è¾¾å¼æå–ï¼ˆä¸“é—¨ä¼˜åŒ–ç”¨äºWordæ ¼å¼ï¼‰"""
        print("âš¡ ä½¿ç”¨ä¼ ç»Ÿæ­£åˆ™æå–æ–¹æ³• (docxä¼˜åŒ–ç‰ˆ)...")

        references = []

        # ä¸“é—¨ä¸ºdocxä¼˜åŒ–çš„æ­£åˆ™æ¨¡å¼ - æ›´ç²¾ç¡®ï¼Œå¤„ç†è‡´è°¢ä¸å£°æ˜è¾¹ç•Œ
        patterns = [
            # å…¨è§’æ‹¬å· - è¡Œé¦–æˆ–æ¢è¡Œåï¼Œé¿å…åŒ¹é…æœŸåˆŠå·
            r'(?:^|\n)ï¼»(\d+)ï¼½([^\n]+(?:\n(?!ï¼»\d+ï¼½|è‡´è°¢ä¸å£°æ˜|è‡´è°¢|ä¸ªäººç®€å†)[^\n]+)*)',  
            # åŠè§’æ‹¬å· - è¡Œé¦–æˆ–æ¢è¡Œå
            r'(?:^|\n)\[(\d+)\]([^\n]+(?:\n(?!\[\d+\]|è‡´è°¢ä¸å£°æ˜|è‡´è°¢|ä¸ªäººç®€å†)[^\n]+)*)',   
            # æ•°å­—ç‚¹å· - è¡Œé¦–
            r'(?:^|\n)(\d+)\.([^\n]+(?:\n(?!^\d+\.|è‡´è°¢ä¸å£°æ˜|è‡´è°¢|ä¸ªäººç®€å†)[^\n]+)*)',     
            # åœ†æ‹¬å· - è¡Œé¦–æˆ–æ¢è¡Œå
            r'(?:^|\n)\((\d+)\)([^\n]+(?:\n(?!\(\d+\)|è‡´è°¢ä¸å£°æ˜|è‡´è°¢|ä¸ªäººç®€å†)[^\n]+)*)',   
        ]

        print(f"ğŸ“„ å‚è€ƒæ–‡çŒ®åŒºåŸŸæ€»é•¿åº¦: {len(ref_text):,} å­—ç¬¦")

        # è®°å½•æ‰€æœ‰åŒ¹é…çš„ç¼–å·åŠå…¶ä½ç½®
        all_matches = []
        for i, pattern in enumerate(patterns):
            matches = list(re.finditer(pattern, ref_text, re.MULTILINE))
            print(f"   ğŸ” æ¨¡å¼ {i+1}: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
            
            for m in matches:
                try:
                    num = int(m.group(1))
                    content = m.group(2).strip()
                    start = m.start()
                    end = m.end()
                    all_matches.append((num, content, start, end, i))
                except (ValueError, IndexError):
                    continue

        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(all_matches)} ä¸ªæ½œåœ¨å‚è€ƒæ–‡çŒ®")

        # æŒ‰ä½ç½®æ’åºï¼Œç¡®ä¿é¡ºåº
        all_matches.sort(key=lambda x: x[2])  # æŒ‰startä½ç½®æ’åº
        
        # æ™ºèƒ½è¿‡æ»¤ï¼šåªä¿ç•™ç¼–å·è¿ç»­ä¸”åˆç†çš„å‚è€ƒæ–‡çŒ®
        valid_refs = []
        expected_num = 1
        tolerance = 5  # å…è®¸çš„ç¼–å·è·³è·ƒå®¹å¿åº¦
        
        for num, content, start, end, pattern_type in all_matches:
            # æ£€æŸ¥ç¼–å·æ˜¯å¦è¿‡å¤§ï¼ˆå¯èƒ½æ˜¯æœŸåˆŠå·ç­‰ï¼‰
            if num > 1000:
                print(f"   âš ï¸ è·³è¿‡å¼‚å¸¸ç¼–å· [{num}]ï¼ˆå¯èƒ½æ˜¯æœŸåˆŠå·ç­‰ï¼‰")
                continue
                
            # æ£€æŸ¥ç¼–å·æ˜¯å¦å¤§è‡´è¿ç»­
            if valid_refs and num > expected_num + tolerance:
                print(f"   âš ï¸ è·³è¿‡ä¸è¿ç»­ç¼–å· [{num}]ï¼ˆæœŸæœ›â‰¤{expected_num + tolerance}ï¼Œå®é™…{num}ï¼‰")
                continue
                
            # æ£€æŸ¥å†…å®¹é•¿åº¦
            if len(content) < 20:
                print(f"   âš ï¸ è·³è¿‡è¿‡çŸ­å†…å®¹ [{num}]ï¼ˆ{len(content)} å­—ç¬¦ï¼‰")
                continue
                
            # æ¸…ç†å†…å®¹
            clean_content = re.sub(r'\s+', ' ', content).strip()
            
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå‚è€ƒæ–‡çŒ®
            if self._is_valid_reference(f"[{num}] {clean_content}"):
                ref_line = f"[{num}] {clean_content}"
                valid_refs.append((num, ref_line, end))
                expected_num = max(expected_num, num + 1)
                print(f"   âœ… æ·»åŠ å‚è€ƒæ–‡çŒ® [{num}]: {clean_content[:50]}...")

        # æŒ‰ç¼–å·æ’åº
        valid_refs.sort(key=lambda x: x[0])
        references = [ref for _, ref, _ in valid_refs]

        # è´¨é‡æ£€æŸ¥
        if references:
            min_num = min(ref[0] for ref in valid_refs)
            max_num = max(ref[0] for ref in valid_refs)
            print(f"   ğŸ“‹ æå–è´¨é‡æŠ¥å‘Š:")
            print(f"      - å‚è€ƒæ–‡çŒ®æ•°é‡: {len(references)}")
            print(f"      - ç¼–å·èŒƒå›´: {min_num}-{max_num}")
            print(f"      - å¹³å‡é•¿åº¦: {sum(len(ref) for _, ref, _ in valid_refs) // len(valid_refs)} å­—ç¬¦")
        
        return references
    
    def _extract_with_hybrid(self, ref_text: str) -> List[str]:
        """æ··åˆç­–ç•¥æå–"""
        print("ğŸ”„ ä½¿ç”¨æ··åˆç­–ç•¥æå–...")
        
        # å…ˆå°è¯•æ­£åˆ™æ–¹æ³•
        regex_refs = self._extract_with_regex(ref_text)
        
        # è¯„ä¼°æ­£åˆ™ç»“æœè´¨é‡
        if len(regex_refs) >= 20:  # å¦‚æœæ‰¾åˆ°è¶³å¤Ÿå¤šçš„å¼•ç”¨
            # æ£€æŸ¥æ ¼å¼è´¨é‡
            quality_score = self._assess_reference_quality(regex_refs)
            if quality_score > 0.7:  # è´¨é‡è‰¯å¥½
                print(f"   âœ… æ­£åˆ™æ–¹æ³•è´¨é‡è‰¯å¥½ (è¯„åˆ†: {quality_score:.2f})")
                return regex_refs
        
        # æ­£åˆ™ç»“æœä¸ç†æƒ³ï¼Œä½¿ç”¨AIæ–¹æ³•
        print("   ğŸ¤– æ­£åˆ™ç»“æœä¸ç†æƒ³ï¼Œåˆ‡æ¢åˆ°AIæ–¹æ³•...")
        return self._extract_with_ai(ref_text)
    
    def _smart_chunk_text(self, text: str, max_size: int = 15000) -> List[str]:
        """æ™ºèƒ½åˆ†å—æ–‡æœ¬ï¼Œåœ¨å¼•ç”¨è¾¹ç•Œåˆ†å‰²"""
        if len(text) <= max_size:
            return [text]
        
        chunks = []
        current_pos = 0
        
        while current_pos < len(text):
            end_pos = min(current_pos + max_size, len(text))
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¼•ç”¨è¾¹ç•Œåˆ†å‰²
            if end_pos < len(text):
                # å¯»æ‰¾æœ€è¿‘çš„å¼•ç”¨å¼€å§‹ä½ç½®
                search_start = max(end_pos - 1000, current_pos + max_size // 2)
                ref_pattern = r'ï¼»\d+ï¼½|\[\d+\]|\n\d+\.'
                
                matches = list(re.finditer(ref_pattern, text[search_start:end_pos + 500]))
                if matches:
                    # æ‰¾åˆ°æœ€åˆé€‚çš„åˆ†å‰²ç‚¹
                    best_match = matches[len(matches) // 2]  # é€‰æ‹©ä¸­é—´çš„åŒ¹é…
                    match_start = best_match.start()
                    if match_start is not None:
                        end_pos = search_start + match_start
            
            chunk = text[current_pos:end_pos].strip()
            if chunk:
                chunks.append(chunk)
            
            current_pos = end_pos
        
        return chunks
    
    def _parse_ai_response(self, content: str) -> List[str]:
        """è§£æAIå“åº”å†…å®¹"""
        references = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å‚è€ƒæ–‡çŒ®æ¡ç›®
            if self._is_valid_reference(line):
                # æ¸…ç†æ ¼å¼
                cleaned_ref = re.sub(r'\s+', ' ', line).strip()
                references.append(cleaned_ref)
        
        return references
    
    def _is_valid_reference(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å‚è€ƒæ–‡çŒ®æ¡ç›®"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(line) < 20:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–å·æ ¼å¼
        has_number = bool(re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.|ï¼ˆ\d+ï¼‰|\(\d+\))', line))
        if not has_number:
            return False
        
        # æå–ç¼–å·
        number_match = re.search(r'(?:ï¼»(\d+)ï¼½|\[(\d+)\]|^(\d+)\.|ï¼ˆ(\d+)ï¼‰|\((\d+)\))', line)
        if number_match:
            num = None
            for group in number_match.groups():
                if group:
                    num = int(group)
                    break
            
            # å¦‚æœç¼–å·è¿‡å¤§ï¼Œå¯èƒ½æ˜¯æœŸåˆŠå·è¯¯è¯†åˆ«
            if num and num > 500:
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸåˆŠã€ä¼šè®®ã€å‡ºç‰ˆç¤¾ç­‰å…³é”®è¯ï¼ˆæ‰©å±•ä¸­æ–‡å…³é”®è¯ï¼‰
        publication_keywords = [
            # è‹±æ–‡å…³é”®è¯
            'Journal', 'Proceedings', 'Conference', 'IEEE', 'ACM', 'Optics',
            'Nature', 'Science', 'Physical Review', 'Applied Physics', 'et al',
            # ä¸­æ–‡å…³é”®è¯
            'æœŸåˆŠ', 'ä¼šè®®', 'å­¦æŠ¥', 'å¤§å­¦å­¦æŠ¥', 'è®ºæ–‡é›†', 'å‡ºç‰ˆç¤¾', 'å­¦é™¢å­¦æŠ¥',
            'éŸ³ä¹å­¦é™¢', 'è‰ºæœ¯å­¦é™¢', 'å¸ˆèŒƒå¤§å­¦', 'å¤§å­¦å‡ºç‰ˆç¤¾', 'äººæ°‘å‡ºç‰ˆç¤¾',
            'æ–‡è‰ºå‡ºç‰ˆç¤¾', 'éŸ³ä¹å‡ºç‰ˆç¤¾', 'ä¸Šæµ·éŸ³ä¹', 'ä¸­å¤®éŸ³ä¹', 'æ²§æ¡‘',
            'æˆå‰§', 'éŸ³ä¹', 'è‰ºæœ¯', 'æ–‡åŒ–', 'ç ”ç©¶', 'åˆ†æ', 'æ¢è®¨', 'è®ºæ–‡',
            'ç¡•å£«', 'åšå£«', 'å­¦ä½è®ºæ–‡', 'æ¯•ä¸šè®ºæ–‡', 'å¹´ç¬¬', 'æœŸ', 'é¡µ',
            'ç¼–', 'è‘—', 'ä¸»ç¼–', 'æ€»ç¼–', 'å·', 'å†Œ', 'ç‰ˆ'
        ]
        
        has_publication = any(keyword in line for keyword in publication_keywords)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
        has_year = bool(re.search(r'19\d{2}|20\d{2}', line))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä½œè€…ä¿¡æ¯ï¼ˆæ‰©å±•ä¸­æ–‡ä½œè€…æ¨¡å¼ï¼‰
        author_patterns = [
            r'[A-Z]\.\s*[A-Z]',  # è‹±æ–‡ç¼©å†™
            r'et al',            # è‹±æ–‡ç­‰
            r'ç­‰',               # ä¸­æ–‡ç­‰
            r'[\u4e00-\u9fff]{2,4}ï¼š',  # ä¸­æ–‡å§“ååè·Ÿå†’å·
            r'[\u4e00-\u9fff]{2,4}ç¼–',  # ç¼–è€…
            r'[\u4e00-\u9fff]{2,4}è‘—',  # è‘—è€…
            r'[\u4e00-\u9fff]{2,4}ä¸»ç¼–', # ä¸»ç¼–
        ]
        
        has_author = any(re.search(pattern, line) for pattern in author_patterns)
        
        # æ”¾å®½éªŒè¯æ¡ä»¶ï¼šåªéœ€è¦æ»¡è¶³å…¶ä¸­ä¸¤ä¸ªæ¡ä»¶å³å¯
        valid_conditions = [has_publication, has_year, has_author]
        return sum(valid_conditions) >= 2
    
    def _deduplicate_references(self, references: List[str]) -> List[str]:
        """å»é‡å‚è€ƒæ–‡çŒ®"""
        seen = set()
        unique_refs = []
        
        for ref in references:
            # ä½¿ç”¨å‰50ä¸ªå­—ç¬¦ä½œä¸ºé‡å¤æ£€æµ‹çš„é”®
            key = re.sub(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.)', '', ref)[:50].strip()
            if key not in seen and key:
                seen.add(key)
                unique_refs.append(ref)
        
        # æŒ‰ç¼–å·æ’åº
        unique_refs.sort(key=lambda x: self._extract_number(x))
        return unique_refs
    
    def _extract_number(self, ref: str) -> int:
        """ä»å‚è€ƒæ–‡çŒ®ä¸­æå–ç¼–å·"""
        match = re.search(r'ï¼»(\d+)ï¼½|\[(\d+)\]|^(\d+)\.', ref)
        if match:
            for group in match.groups():
                if group:
                    return int(group)
        return 999999  # æ— ç¼–å·çš„æ”¾åˆ°æœ€å
    
    def _assess_reference_quality(self, references: List[str]) -> float:
        """è¯„ä¼°å‚è€ƒæ–‡çŒ®æå–è´¨é‡"""
        if not references:
            return 0.0
        
        score = 0.0
        total_checks = len(references)
        
        for ref in references:
            # æ£€æŸ¥ç¼–å·è¿ç»­æ€§
            if re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.)', ref):
                score += 0.3
            
            # æ£€æŸ¥åŒ…å«å…³é”®ä¿¡æ¯
            if any(keyword in ref for keyword in ['Journal', 'æœŸåˆŠ', 'Conference', 'Proceedings']):
                score += 0.3
            
            # æ£€æŸ¥åŒ…å«å¹´ä»½
            if re.search(r'19\d{2}|20\d{2}', ref):
                score += 0.2
            
            # æ£€æŸ¥é•¿åº¦åˆç†
            if 30 <= len(ref) <= 500:
                score += 0.2
        
        return score / total_checks if total_checks > 0 else 0.0
