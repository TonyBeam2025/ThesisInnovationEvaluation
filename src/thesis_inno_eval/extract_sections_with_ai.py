"""
ä½¿ç”¨AIæ¨¡å‹ä»è®ºæ–‡æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
æ”¯æŒå¤šç§AIæ¨¡å‹ - å®Œå–„ç‰ˆæŠ½å–ç³»ç»Ÿ
é›†æˆ: åˆ†æ­¥æŠ½å–ç­–ç•¥ã€ç»“æ„åŒ–åˆ†æã€å¿«é€Ÿå®šä½ã€æ­£åˆ™åŒ¹é…ã€å‚è€ƒæ–‡çŒ®è§£æã€æ™ºèƒ½ä¿®å¤
"""

import re
import json
import logging
import hashlib
import os
import time
import asyncio
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, List, Tuple, Any
from docx import Document

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨
try:
    from .smart_reference_extractor import SmartReferenceExtractor
except ImportError:
    try:
        from smart_reference_extractor import SmartReferenceExtractor
    except ImportError:
        print("âš ï¸ æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
        SmartReferenceExtractor = None


class ThesisExtractorPro:
    """
    ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡æå–å™¨
    æ•´åˆæ‰€æœ‰éªŒè¯è¿‡çš„æŠ€æœ¯: åˆ†æ­¥æŠ½å–ã€ç»“æ„åŒ–åˆ†æã€å¿«é€Ÿå®šä½ã€æ­£åˆ™åŒ¹é…ã€å‚è€ƒæ–‡çŒ®è§£æã€æ™ºèƒ½ä¿®å¤
    """

    def __init__(self):
        self.extraction_stats = {
            'total_fields': 33,
            'extracted_fields': 0,
            'confidence': 0.0,
            'processing_time': 0.0,
        }

        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.ai_client = None
        self._init_ai_client()

        # åˆå§‹åŒ–æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨
        self.smart_ref_extractor = None
        self._init_smart_ref_extractor()

        # ä¼˜åŒ–åçš„22ä¸ªæ ¸å¿ƒå­—æ®µå®šä¹‰ (ä½¿ç”¨snake_caseå‘½åæ³•å’Œè¯­è¨€åç¼€)
        self.standard_fields = [
            'thesis_number',
            'title_cn',
            'author_cn',
            'title_en',
            'author_en',
            'university_cn',
            'university_en',
            'degree_level',
            'major_cn',
            'college',
            'supervisor_cn',
            'supervisor_en',
            'defense_date',
            'submission_date',
            'abstract_cn',
            'abstract_en',
            'keywords_cn',
            'keywords_en',
            'theoretical_framework',
            'acknowledgement',
            'references',
            'author_contributions',
        ]

        # åˆå§‹åŒ–æ­£åˆ™æ¨¡å¼åº“
        self._init_regex_patterns()

    def _init_ai_client(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        try:
            # å°è¯•å¤šç§å¯¼å…¥è·¯å¾„
            try:
                from .ai_client import get_ai_client
            except ImportError:
                from thesis_inno_eval.ai_client import get_ai_client

            self.ai_client = get_ai_client()
            print("   ğŸ¤– AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            self.ai_client = None

    def _init_smart_ref_extractor(self):
        """åˆå§‹åŒ–æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨"""
        try:
            if SmartReferenceExtractor:
                self.smart_ref_extractor = SmartReferenceExtractor(
                    ai_client=self.ai_client
                )
                print("   ğŸ“š æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("   âš ï¸ æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨ç±»ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                self.smart_ref_extractor = None
        except Exception as e:
            print(f"   âš ï¸ æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.smart_ref_extractor = None

    def _init_regex_patterns(self):
        """åˆå§‹åŒ–14ä¸ªå­—æ®µçš„ä¸“ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åº“"""
        self.patterns = {
            # åŸºç¡€ä¿¡æ¯æ¨¡å¼
            'thesis_number': [
                r'è®ºæ–‡ç¼–å·[ï¼š:]\s*([A-Z0-9\-\.]+)',
                r'ç¼–å·[ï¼š:]\s*([A-Z0-9\-\.]+)',
                r'åˆ†ç±»å·[ï¼š:]\s*([A-Z0-9\-\.]+)',
                r'å¯†çº§[ï¼š:]\s*([A-Z0-9\-\.]+)',
                r'UDC[ï¼š:]\s*([A-Z0-9\-\.]+)',
            ],
            'title_cn': [
                r'(?:ä¸­æ–‡)?(?:è®ºæ–‡)?é¢˜ç›®[ï¼š:\s]*([^\n\r]{10,200})',
                r'(?:è®ºæ–‡)?æ ‡é¢˜[ï¼š:\s]*([^\n\r]{10,200})',
                # æ”¹è¿›ï¼šåŒ¹é…å¤šè¡Œä¸­æ–‡æ ‡é¢˜ï¼ˆå¦‚ï¼šOsåŸºé«˜æ¸©éæ™¶åˆé‡‘çš„è®¾è®¡åˆ¶å¤‡\nåŠåŠ›å­¦æ€§èƒ½ç ”ç©¶ï¼‰
                r'([^\n\r\d]{8,50}\n[^\n\r\d]{8,50})',
                # åŒ¹é…ç‹¬ç«‹è¡Œçš„ä¸­æ–‡æ ‡é¢˜
                r'^([^A-Za-z\n\r]{10,100})$',
            ],
            'author_cn': [
                r'(?:ä½œè€…|å§“å)[ï¼š:\s]*([^\d\n\r]{2,10})',
                r'ç ”ç©¶ç”Ÿ[ï¼š:\s]*([^\d\n\r]{2,10})',
            ],
            'title_en': [
                r'(?:English\s+)?(?:Title|TITLE)[ï¼š:\s]*([A-Za-z\s\-:]{10,200})',
                # æ”¹è¿›ï¼šåŒ¹é…å¤šè¡Œè‹±æ–‡æ ‡é¢˜
                r'([A-Z][A-Za-z\s\-,]{15,200}(?:\n[A-Za-z\s\-]{10,200})*)',
                r'^([A-Z][A-Za-z\s\-:]{15,200})$',
            ],
            'author_en': [
                r'(?:Author|Name)[ï¼š:\s]*([A-Za-z\s]{2,30})',
                r'(?:By|by)[ï¼š:\s]*([A-Za-z\s]{2,30})',
            ],
            'university_cn': [
                r'([^A-Za-z\n\r]*å¤§å­¦[^A-Za-z\n\r]*)',
                r'([^A-Za-z\n\r]*å­¦é™¢[^A-Za-z\n\r]*)',
            ],
            'degree_level': [
                r'(åšå£«|ç¡•å£«|å­¦å£«)(?:å­¦ä½|ç ”ç©¶ç”Ÿ)?',
                r'(PhD|Master|Bachelor)',
            ],
            'major_cn': [
                r'(?:ä¸“ä¸š|å­¦ç§‘)[ï¼š:\s]*([^\n\r]{2,50})',
                r'(?:Major|MAJOR)[ï¼š:\s]*([^\n\r]{2,50})',
            ],
            'college': [
                r'(?:å­¦é™¢|ç³»)[ï¼š:\s]*([^\n\r]{2,50})',
                r'(?:College|School)[ï¼š:\s]*([^\n\r]{2,50})',
            ],
            'supervisor_cn': [
                r'(?:å¯¼å¸ˆ|æŒ‡å¯¼æ•™å¸ˆ)[ï¼š:\s]*([^\d\n\r]{2,10})',
                r'(?:Supervisor|SUPERVISOR)[ï¼š:\s]*([^\d\n\r]{2,10})',
            ],
            'supervisor_en': [
                r'(?:Supervisor|SUPERVISOR)[ï¼š:\s]*([A-Za-z\s\.]+?)(?:\n|$|[ï¼Œ,])',
                r'(?:Advisor|ADVISOR)[ï¼š:\s]*([A-Za-z\s\.]+?)(?:\n|$|[ï¼Œ,])',
                r'(?:Directed\s+by|DIRECTED\s+BY)[ï¼š:\s]*([A-Za-z\s\.]+?)(?:\n|$|[ï¼Œ,])',
                r'(?:Under\s+the\s+guidance\s+of)[ï¼š:\s]*([A-Za-z\s\.]+?)(?:\n|$|[ï¼Œ,])',
                r'((Prof\.|Professor|Dr\.)\s+[A-Za-z\s]+?)(?:\n|$|[ï¼Œ,])',  # ä¿®æ”¹ï¼šä¿ç•™å®Œæ•´èŒç§°
            ],
            'defense_date': [
                r'(?:ç­”è¾©|Defense)(?:æ—¥æœŸ|Date)[ï¼š:\s]*(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
            ],
            'keywords_cn': [
                r'(?:å…³é”®è¯|Keywords?)[ï¼š:\s]*([^\n\r]{5,200})',
                r'(?:Key\s+words?)[ï¼š:\s]*([^\n\r]{5,200})',
            ],
            'keywords_en': [
                r'(?:Keywords?|KEY\s+WORDS?)[ï¼š:\s]*([A-Za-z\s,;]{5,200})',
            ],
        }

    def extract_with_integrated_strategy(
        self, text: str, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨é›†æˆç­–ç•¥è¿›è¡Œæå–
        åˆ†æ­¥æŠ½å–ç­–ç•¥: å‰ç½®ä¿¡æ¯â†’ç»“æ„åŒ–ç« èŠ‚â†’å†…å®¹æå–â†’åå¤„ç†ä¿®å¤
        """
        start_time = time.time()
        print(f"ğŸš€ å¯åŠ¨ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–ç³»ç»Ÿ")
        print("=" * 60)
        logger.info(f"å¼€å§‹ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–ï¼Œæ–‡ä»¶è·¯å¾„: {file_path}, æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")

        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯å’Œç›¸å…³ç»„ä»¶
        print("ğŸ”§ åˆå§‹åŒ–AIç»„ä»¶...")
        logger.info("åˆå§‹åŒ–AIç»„ä»¶å’Œç›¸å…³ç»„ä»¶")
        self._init_ai_client()
        self._init_smart_ref_extractor()

        # æ­¥éª¤1: å‰ç½®ä¿¡æ¯å¿«é€Ÿå®šä½
        print("ğŸ¯ æ­¥éª¤1: å‰ç½®ä¿¡æ¯å¿«é€Ÿå®šä½")
        logger.info("æ­¥éª¤1: å¼€å§‹å‰ç½®ä¿¡æ¯å¿«é€Ÿå®šä½")
        metadata = self._extract_front_metadata(text)
        logger.info(f"æ­¥éª¤1å®Œæˆ: æå–åˆ° {len(metadata)} ä¸ªå‰ç½®ä¿¡æ¯å­—æ®µ")

        # æ­¥éª¤2: æ™ºèƒ½ç›®å½•æå–å’Œç¬¬ä¸€å±‚çº§ç« èŠ‚åˆ†æ
        print("ğŸ“‹ æ­¥éª¤2: æ™ºèƒ½ç›®å½•æå–å’Œç¬¬ä¸€å±‚çº§ç« èŠ‚åˆ†æ")
        logger.info("æ­¥éª¤2: å¼€å§‹æ™ºèƒ½ç›®å½•æå–å’Œç¬¬ä¸€å±‚çº§ç« èŠ‚åˆ†æ")
        toc_analysis = self._extract_and_analyze_toc_with_content_boundaries(text, file_path)
        logger.info(f"æ­¥éª¤2å®Œæˆ: ç›®å½•åˆ†æç»“æœåŒ…å« {len(toc_analysis.get('chapters', []))} ä¸ªç« èŠ‚")

        # æ­¥éª¤3: ç»“æ„åŒ–ç« èŠ‚åˆ†æ
        print("ğŸ” æ­¥éª¤3: ç»“æ„åŒ–ç« èŠ‚åˆ†æ")
        logger.info("æ­¥éª¤3: å¼€å§‹ç»“æ„åŒ–ç« èŠ‚åˆ†æ")
        sections = self._analyze_document_structure(text)
        logger.info(f"æ­¥éª¤3å®Œæˆ: è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚ç»“æ„")

        # æ­¥éª¤3.5: åŸºäºç»“æ„åˆ†æç»“æœè¿›è¡Œç« èŠ‚å†…å®¹AIæ™ºèƒ½åˆ†æ
        print("ğŸ§  æ­¥éª¤3.5: åŸºäºç»“æ„åˆ†æçš„AIæ™ºèƒ½å†…å®¹åˆ†æ")
        logger.info("æ­¥éª¤3.5: å¼€å§‹åŸºäºç»“æ„åˆ†æçš„AIæ™ºèƒ½å†…å®¹åˆ†æ")
        ai_analysis = self._conduct_ai_analysis_on_sections(text, sections)
        logger.info(f"æ­¥éª¤3.5å®Œæˆ: AIåˆ†æäº† {len(ai_analysis)} ä¸ªç« èŠ‚")

        # æ­¥éª¤4: å†…å®¹åˆ†å—æå–
        print("ğŸ“„ æ­¥éª¤4: åˆ†å—å†…å®¹æå–")
        logger.info("æ­¥éª¤4: å¼€å§‹åˆ†å—å†…å®¹æå–")
        content_info = self._extract_content_by_sections(text, sections)
        logger.info(f"æ­¥éª¤4å®Œæˆ: æå–åˆ° {len(content_info)} ä¸ªå†…å®¹å­—æ®µ")

        # æ­¥éª¤5: å‚è€ƒæ–‡çŒ®è§£æ
        print("ğŸ“š æ­¥éª¤5: å‚è€ƒæ–‡çŒ®è§£æ")
        logger.info("æ­¥éª¤5: å¼€å§‹å‚è€ƒæ–‡çŒ®è§£æ")
        
        # ä½¿ç”¨æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨
        if self.smart_ref_extractor:
            print("   ğŸ¤– ä½¿ç”¨SmartReferenceExtractorè¿›è¡Œæ™ºèƒ½æå–")
            references_result = self._extract_references_enhanced_disciplinary(
                text, discipline='æœªçŸ¥', source_path=''
            )
            references = references_result.get('references', [])
        else:
            print("   âš ï¸ æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            references = self._extract_references_enhanced(text)
        
        logger.info(f"æ­¥éª¤5å®Œæˆ: æå–åˆ° {len(references)} æ¡å‚è€ƒæ–‡çŒ®")

        # æ­¥éª¤6: æ™ºèƒ½ä¿®å¤å’ŒéªŒè¯
        print("ğŸ”§ æ­¥éª¤6: æ™ºèƒ½ä¿®å¤å’ŒéªŒè¯")
        logger.info("æ­¥éª¤6: å¼€å§‹æ™ºèƒ½ä¿®å¤å’ŒéªŒè¯")
        final_result = self._intelligent_repair_and_validate(
            metadata, content_info, references, toc_analysis, text, ai_analysis
        )
        logger.info("æ­¥éª¤6å®Œæˆ: æ™ºèƒ½ä¿®å¤å’ŒéªŒè¯å®Œæˆ")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        logger.info(f"å¼€å§‹è®¡ç®—æå–ç»Ÿè®¡ä¿¡æ¯ï¼Œæ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        self._calculate_extraction_stats(final_result, processing_time)

        # ç”ŸæˆæŠ¥å‘Š
        logger.info("å¼€å§‹ç”Ÿæˆæå–æŠ¥å‘Š")
        self._generate_extraction_report(final_result, file_path, processing_time)
        
        logger.info(f"ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–å®Œæˆï¼Œæ€»è€—æ—¶: {processing_time:.2f}ç§’ï¼Œæå–å­—æ®µæ•°: {self.extraction_stats['extracted_fields']}/{self.extraction_stats['total_fields']}")

        return final_result

    def _extract_front_metadata(self, text: str) -> Dict[str, Any]:
        """æ­¥éª¤1: ç²¾å‡†å®šä½å°é¢ä¿¡æ¯ + AIæ™ºèƒ½è¯†åˆ«"""
        metadata = {}

        print("   ğŸ“ ç²¾å‡†å®šä½å°é¢ä¿¡æ¯åŒºåŸŸ...")

        # ç²¾å‡†å®šä½å°é¢åŒºåŸŸï¼šåœ¨"å­¦ä½è®ºæ–‡ä½¿ç”¨æˆæƒä¹¦"ä¹‹å‰çš„å†…å®¹
        cover_end_markers = [
            'å­¦ä½è®ºæ–‡ä½¿ç”¨æˆæƒä¹¦',
            'å­¦ä½è®ºæ–‡åŸåˆ›æ€§å£°æ˜',
            'ç‹¬åˆ›æ€§å£°æ˜',
            'ç‰ˆæƒä½¿ç”¨æˆæƒä¹¦',
            'ä¸­æ–‡æ‘˜è¦',
            'æ‘˜è¦',
            'ABSTRACT',
        ]

        cover_text = text
        for marker in cover_end_markers:
            pos = text.find(marker)
            if pos > 0:
                cover_text = text[:pos]
                print(
                    f"   ğŸ¯ å°é¢åŒºåŸŸå®šä½: åœ¨'{marker}'ä¹‹å‰ï¼Œé•¿åº¦ {len(cover_text)} å­—ç¬¦"
                )
                break

        # å¦‚æœæ²¡æ‰¾åˆ°æ ‡è®°ï¼Œå–å‰10%å†…å®¹ä½œä¸ºå°é¢
        if cover_text == text:
            cover_text = text[: len(text) // 10]
            print(f"   ğŸ“„ ä½¿ç”¨å‰10%å†…å®¹ä½œä¸ºå°é¢åŒºåŸŸï¼Œé•¿åº¦ {len(cover_text)} å­—ç¬¦")

        # ä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«å°é¢å…ƒæ•°æ®
        if hasattr(self, 'ai_client') and self.ai_client:
            metadata = self._ai_extract_cover_metadata(cover_text)
        else:
            # ç¡®ä¿å§‹ç»ˆè¿”å›å­—å…¸ï¼Œå³ä½¿AIä¸å¯ç”¨
            metadata = {}
            print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¿”å›ç©ºå…ƒæ•°æ®å­—å…¸")
            logger.warning("AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•æå–å‰ç½®å…ƒæ•°æ®")

        # ç¡®ä¿è¿”å›å€¼ä¸ä¸ºNone
        if metadata is None:
            metadata = {}
            print("   âš ï¸ å…ƒæ•°æ®æå–å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸")
            logger.error("å‰ç½®å…ƒæ•°æ®æå–å¤±è´¥ï¼Œè¿”å›å€¼ä¸ºNone")

        return metadata

    def _ai_extract_cover_metadata(self, cover_text: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«å°é¢å…ƒæ•°æ®"""
        print("   ğŸ§  ä½¿ç”¨AIæ™ºèƒ½è¯†åˆ«å°é¢ä¿¡æ¯...")

        prompt = f"""
è¯·ä»ä»¥ä¸‹å­¦ä½è®ºæ–‡å°é¢å†…å®¹ä¸­æå–è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œåªæå–ç¡®å®å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ï¼š

å°é¢å†…å®¹ï¼š
{cover_text[:2000]}

è¯·æå–ä»¥ä¸‹å­—æ®µï¼ˆå¦‚æœæŸä¸ªå­—æ®µä¸å­˜åœ¨ï¼Œè¯·è®¾ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰ï¼š
{{
  "thesis_number": "è®ºæ–‡ç¼–å·",
  "title_cn": "ä¸­æ–‡è®ºæ–‡æ ‡é¢˜",
  "title_en": "è‹±æ–‡è®ºæ–‡æ ‡é¢˜", 
  "author_cn": "ä½œè€…ä¸­æ–‡å§“å",
  "author_en": "ä½œè€…è‹±æ–‡å§“å",
  "university_cn": "ä¸­æ–‡å­¦æ ¡åç§°",
  "university_en": "è‹±æ–‡å­¦æ ¡åç§°",
  "degree_level": "å­¦ä½çº§åˆ«ï¼ˆå¦‚ï¼šåšå£«ã€ç¡•å£«ï¼‰",
  "major_cn": "ä¸­æ–‡ä¸“ä¸šåç§°",
  "college": "å­¦é™¢åç§°",
  "supervisor_cn": "ä¸­æ–‡å¯¼å¸ˆå§“å",
  "supervisor_en": "è‹±æ–‡å¯¼å¸ˆå§“å",
  "defense_date": "ç­”è¾©æ—¥æœŸ",
  "submission_date": "æäº¤æ—¥æœŸ"
}}

æ³¨æ„ï¼š
- åªæå–æ˜ç¡®å­˜åœ¨çš„ä¿¡æ¯ï¼Œä¸è¦æ¨æµ‹
- å§“åä¸è¦åŒ…å«"å§“åï¼š"ç­‰æ ‡ç­¾
- å­¦æ ¡åç§°ä¸è¦åŒ…å«"å­¦ä½æˆäºˆå•ä½ï¼š"ç­‰æ ‡ç­¾
- æ ‡é¢˜è¦å®Œæ•´ï¼Œä¸è¦åŒ…å«æ—¶é—´æˆ³ç­‰æ— å…³ä¿¡æ¯
- æ—¥æœŸæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚æœåªæœ‰å¹´ä»½åˆ™ä¸ºYYYY

è¿”å›JSONï¼š"""

        try:
            if not self.ai_client:
                raise Exception("AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

            response = self.ai_client.send_message(prompt)
            if response and response.content:
                # æå–JSONå†…å®¹
                content = response.content.strip()
                if content.startswith('```json'):
                    content = content[7:]
                if content.endswith('```'):
                    content = content[:-3]

                import json

                metadata = json.loads(content.strip())

                # éªŒè¯å’Œæ¸…ç†ç»“æœ
                for key, value in metadata.items():
                    if value and isinstance(value, str):
                        metadata[key] = value.strip()
                        if metadata[key]:
                            print(f"   âœ… {key}: {metadata[key]}")

                return metadata
            else:
                # responseä¸ºç©ºæˆ–contentä¸ºç©º
                print("   âš ï¸ AIå“åº”ä¸ºç©º")
                logger.warning("AIå°é¢å…ƒæ•°æ®æå–å“åº”ä¸ºç©º")

        except Exception as e:
            print(f"   âš ï¸ AIæå–å¤±è´¥: {e}")
            logger.error(f"AIå°é¢å…ƒæ•°æ®æå–å¤±è´¥: {e}", exc_info=True)

        # AIæå–å¤±è´¥æ—¶ï¼Œè¿”å›ç©ºå­—å…¸è€Œä¸æ˜¯None
        print("   ğŸ”§ AIæå–å¤±è´¥ï¼Œè¿”å›ç©ºå…ƒæ•°æ®å­—å…¸")
        logger.info("AIå°é¢å…ƒæ•°æ®æå–å¤±è´¥ï¼Œè¿”å›ç©ºå­—å…¸")
        return {}

    def _clean_extracted_value(self, value: str, field: str) -> str:
        """æ¸…ç†æå–çš„å€¼ï¼Œç§»é™¤æ ‡ç­¾å’Œæ ¼å¼æ–‡å­—"""
        if not value:
            return ""

        cleaned = value.strip()

        # é€šç”¨æ¸…ç†ï¼šç§»é™¤å¸¸è§æ ‡ç­¾
        label_patterns = [
            r'^(?:å­¦å·|å§“å|ä½œè€…|æ ‡é¢˜|é¢˜ç›®|ä¸“ä¸š|å­¦é™¢|å¤§å­¦)[ï¼š:\s]*',
            r'^(?:Student|Author|Title|Name|Major|College|University)[ï¼š:\s]*',
            r'^(?:å­¦ä½æˆäºˆå•ä½|å­¦ä½æˆäºˆæ—¥æœŸ)[ï¼š:\s]*',
            r'å¹´\s*æœˆ\s*æ—¥\s*$',  # ç§»é™¤ç»“å°¾çš„å¹´æœˆæ—¥
            r'^\*\*[^*]+\*\*[ï¼š:\s]*',  # Markdownæ ‡è®°
        ]

        for pattern in label_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE).strip()

        # ç‰¹å®šå­—æ®µçš„ç‰¹æ®Šæ¸…ç†
        if field == 'author_cn':
            # ç§»é™¤"å§“å"æ ‡ç­¾ï¼Œæå–çº¯å§“å
            cleaned = re.sub(r'(?:å§“å|ä½œè€…|å­¦ç”Ÿ|ç ”ç©¶ç”Ÿ)[ï¼š:\s]*', '', cleaned).strip()
            # å¦‚æœè¿˜åŒ…å«å…¶ä»–å­—æ®µæ ‡è¯†ï¼Œåªå–ä¸­æ–‡å§“åéƒ¨åˆ†
            name_match = re.search(r'([^\s\d]{2,4})', cleaned)
            if name_match:
                cleaned = name_match.group(1)

        elif field == 'title_cn':
            # ç§»é™¤è½¬æ¢æ—¶é—´å’Œå…¶ä»–æ— å…³ä¿¡æ¯
            cleaned = re.sub(r'\*\*è½¬æ¢æ—¶é—´\*\*[^*]*', '', cleaned)
            cleaned = re.sub(r'^\d{4}-\d{2}-\d{2}.*', '', cleaned)
            cleaned = re.sub(r'ä½œè€…å§“å.*', '', cleaned)  # ç§»é™¤ä½œè€…å§“åéƒ¨åˆ†
            # å¤„ç†å¤šè¡Œæ ‡é¢˜ï¼Œåˆå¹¶ä¸ºä¸€è¡Œ
            cleaned = re.sub(r'\n+', '', cleaned).strip()
            # æå–çœŸæ­£çš„æ ‡é¢˜
            if len(cleaned) < 8:  # å¦‚æœå¤ªçŸ­ï¼Œå°è¯•ä»å°é¢æ–‡æœ¬ä¸­é‡æ–°æå–
                return ""

        elif field == 'title_en':
            # å¤„ç†å¤šè¡Œè‹±æ–‡æ ‡é¢˜
            cleaned = re.sub(r'\n+', ' ', cleaned).strip()
            # ç§»é™¤æ˜æ˜¾çš„éæ ‡é¢˜å†…å®¹
            if 'Dissertation' in cleaned and 'Degree' in cleaned:
                # å¦‚æœåŒ…å«å­¦ä½ä¿¡æ¯ï¼Œå¯èƒ½ä¸æ˜¯çœŸæ­£çš„æ ‡é¢˜
                if len(cleaned) > 100:
                    return ""

        elif field == 'university_cn':
            # æå–çº¯å¤§å­¦åç§°
            university_match = re.search(r'([^\s\d]*å¤§å­¦)', cleaned)
            if university_match:
                cleaned = university_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¤§å­¦ï¼Œå°è¯•å­¦é™¢
                college_match = re.search(r'([^\s\d]*å­¦é™¢)', cleaned)
                if college_match and 'ææ–™' not in college_match.group(
                    1
                ):  # æ’é™¤ä¸“ä¸šå­¦é™¢
                    cleaned = college_match.group(1)

        elif field == 'major_cn':
            # æå–çº¯ä¸“ä¸šåç§°
            cleaned = re.sub(r'(?:ä¸“ä¸š|å­¦ç§‘)[ï¼š:\s]*', '', cleaned).strip()

        elif field == 'College':
            # ç¡®ä¿æ˜¯å­¦é™¢åç§°
            if 'å­¦é™¢' not in cleaned:
                cleaned = ""

        return cleaned.strip()

    def _analyze_document_structure(self, text: str) -> Dict[str, Any]:
        """æ­¥éª¤2: æ™ºèƒ½è¯†åˆ«è®ºæ–‡æ ‡å‡†ç« èŠ‚ï¼Œç²¾ç¡®å®šä½å…³é”®å†…å®¹"""
        sections = {}

        # ç« èŠ‚è¯†åˆ«æ¨¡å¼ï¼ŒåŒ…å«æ ‡é¢˜æå– - å¢å¼ºç‰ˆæ”¯æŒå¤šç§æ ¼å¼
        section_patterns = {
            'cover': r'(^[\s\S]{200,2000}?)(?=æ‘˜\s*è¦|ABSTRACT|Abstract)',  # å°é¢ä¿¡æ¯
            'abstract_cn': r'((?:ä¸­æ–‡)?æ‘˜\s*è¦[\s\S]{100,5000}?)(?=å…³é”®è¯|è‹±æ–‡æ‘˜è¦|ABSTRACT|ç›®\s*å½•)',
            'abstract_en': r'((?:ABSTRACT|Abstract)[\s\S]{100,5000}?)(?=Keywords?|Key\s+Words?|ç›®\s*å½•|1\s)',
            'keywords_cn': r'(å…³é”®è¯[ï¼š:\s]*[^\n\r]{5,200})',
            'keywords_en': r'((?:Keywords?|KEY\s+WORDS?|Key\s+Words?)[ï¼š:\s]*[^\n\r]{5,200})',
            # ç›®å½•è¯†åˆ« - å¢å¼ºæ¨¡å¼
            'toc': r'(ç›®\s*å½•[\s\S]{200,3000}?)(?=æ‘˜\s*è¦|Abstract|1\s+ç»ªè®º|ç¬¬ä¸€ç« )',
            # æ•°å­—ç« èŠ‚æ ¼å¼ - ç²¾ç¡®åŒ¹é…å¿ƒè„å»ºæ¨¡è®ºæ–‡ç»“æ„
            'chapter_1_introduction': r'((?:^|\n)\s*1\s+ç»ª\s*è®º[\s\S]{500,15000}?)(?=2\s+å¿ƒè„å»ºæ¨¡|2\s+[\u4e00-\u9fff]|$)',
            'chapter_2_theory': r'((?:^|\n)\s*2\s+å¿ƒè„å»ºæ¨¡çš„åŸºç¡€ç†è®º[\s\S]{1000,25000}?)(?=3\s+å¿ƒè„CTA|3\s+[\u4e00-\u9fff]|$)',
            'chapter_3_segmentation': r'((?:^|\n)\s*3\s+å¿ƒè„CTAå›¾åƒåˆ†å‰²[\s\S]{1000,20000}?)(?=4\s+å››ç»´åŠ¨æ€|4\s+[\u4e00-\u9fff]|$)',
            'chapter_4_modeling': r'((?:^|\n)\s*4\s+å››ç»´åŠ¨æ€ç»Ÿè®¡ä½“å½¢å¿ƒè„æ¨¡å‹çš„æ„å»º[\s\S]{1000,20000}?)(?=5\s+ç»“|5\s+[\u4e00-\u9fff]|$)',
            'chapter_5_conclusion': r'((?:^|\n)\s*5\s+ç»“\s*è®º[\s\S]{200,8000}?)(?=å‚\s*è€ƒ\s*æ–‡\s*çŒ®|è‡´è°¢|æ”»è¯»|$)',
            # é€šç”¨æ•°å­—ç« èŠ‚æ ¼å¼ - å¤‡é€‰æ¨¡å¼
            'chapter_1': r'((?:^|\n)\s*1\s+ç»ª\s*è®º[\s\S]{200,10000}?)(?=2\s+|$)',
            'chapter_2': r'((?:^|\n)\s*2\s+[\u4e00-\u9fff].*?åŸºç¡€ç†è®º[\s\S]{500,20000}?)(?=3\s+|$)',
            'chapter_3': r'((?:^|\n)\s*3\s+[\u4e00-\u9fff].*?å›¾åƒåˆ†å‰²[\s\S]{500,15000}?)(?=4\s+|$)',
            'chapter_4': r'((?:^|\n)\s*4\s+å››ç»´åŠ¨æ€[\s\S]{500,15000}?)(?=5\s+|ç»“\s*è®º|$)',
            'chapter_5': r'((?:^|\n)\s*5\s+ç»“\s*è®º[\s\S]{100,8000}?)(?=å‚\s*è€ƒ\s*æ–‡\s*çŒ®|è‡´è°¢|$)',
            # Markdownå­ç« èŠ‚æ ¼å¼ - æ”¯æŒ ### 1.1ã€### 1.2 ç­‰
            'subsection_1_1': r'(###\s*1\.1\s*[^\n\r]*[\s\S]{200,8000}?)(?=###\s*1\.2|###\s*2\.|2\s+|$)',
            'subsection_1_2': r'(###\s*1\.2\s*[^\n\r]*[\s\S]{500,12000}?)(?=###\s*1\.3|###\s*2\.|2\s+|$)',
            'subsection_1_3': r'(###\s*1\.3\s*[^\n\r]*[\s\S]{200,8000}?)(?=###\s*2\.|2\s+|$)',
            'subsection_2_1': r'(###\s*2\.1\s*[^\n\r]*[\s\S]{500,15000}?)(?=###\s*2\.2|###\s*3\.|3\s+|$)',
            'subsection_2_2': r'(###\s*2\.2\s*[^\n\r]*[\s\S]{500,12000}?)(?=###\s*2\.3|###\s*3\.|3\s+|$)',
            'subsection_2_3': r'(###\s*2\.3\s*[^\n\r]*[\s\S]{200,10000}?)(?=###\s*3\.|3\s+|$)',
            'subsection_3_1': r'(###\s*3\.1\s*[^\n\r]*[\s\S]{200,5000}?)(?=###\s*3\.2|###\s*4\.|4\s+|$)',
            'subsection_3_2': r'(###\s*3\.2\s*[^\n\r]*[\s\S]{200,5000}?)(?=###\s*3\.3|###\s*4\.|4\s+|$)',
            'subsection_3_3': r'(###\s*3\.3\s*[^\n\r]*[\s\S]{500,10000}?)(?=###\s*3\.4|###\s*4\.|4\s+|$)',
            'subsection_3_4': r'(###\s*3\.4\s*[^\n\r]*[\s\S]{500,10000}?)(?=###\s*3\.5|###\s*4\.|4\s+|$)',
            'subsection_3_5': r'(###\s*3\.5\s*[^\n\r]*[\s\S]{200,5000}?)(?=###\s*4\.|4\s+|$)',
            'subsection_4_1': r'(###\s*4\.1\s*[^\n\r]*[\s\S]{200,5000}?)(?=###\s*4\.2|###\s*5\.|5\s+|$)',
            'subsection_4_2': r'(###\s*4\.2\s*[^\n\r]*[\s\S]{500,12000}?)(?=###\s*4\.3|###\s*5\.|5\s+|$)',
            'subsection_4_3': r'(###\s*4\.3\s*[^\n\r]*[\s\S]{500,12000}?)(?=###\s*4\.4|###\s*5\.|5\s+|$)',
            # ä¼ ç»Ÿç« èŠ‚æ ¼å¼ä½œä¸ºå¤‡é€‰
            'introduction': r'((?:ç¬¬ä¸€ç« |ç¬¬1ç« |å¼•\s*è¨€|ç»ª\s*è®º|æ¦‚\s*è¿°)[\s\S]{500,10000}?)(?=ç¬¬äºŒç« |ç¬¬2ç« |2\s)',
            'literature': r'((?:ç¬¬äºŒç« |ç¬¬2ç« |æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|åŸºç¡€ç†è®º)[\s\S]{1000,20000}?)(?=ç¬¬ä¸‰ç« |ç¬¬3ç« |3\s)',
            'methodology': r'((?:ç¬¬ä¸‰ç« |ç¬¬3ç« |ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å›¾åƒåˆ†å‰²)[\s\S]{1000,15000}?)(?=ç¬¬å››ç« |ç¬¬4ç« |4\s)',
            'results': r'((?:ç¬¬å››ç« |ç¬¬4ç« |å®éªŒç»“æœ|ç»“æœåˆ†æ|æ¨¡å‹æ„å»º)[\s\S]{1000,15000}?)(?=ç¬¬äº”ç« |ç¬¬5ç« |5\s|ç»“è®º)',
            # å…¶ä»–é‡è¦ç« èŠ‚
            'conclusion': r'((?:ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›|æ€»ç»“ä¸å±•æœ›|ç»“è®ºä¸å»ºè®®|ç ”ç©¶æ€»ç»“|ä¸»è¦ç»“è®º|æœ¬æ–‡ç»“è®º)[\s\S]{200,8000}?)(?=å‚\s*è€ƒ\s*æ–‡\s*çŒ®|è‡´è°¢|æ”»è¯»|é™„å½•|$)',
            'references': r'((?:å‚\s*è€ƒ\s*æ–‡\s*çŒ®|REFERENCES?|References?)[\s\S]*?)(?=\n+\s*(?:è‡´\s*è°¢\s*ä¸\s*å£°\s*æ˜|è‡´\s*è°¢|æ”»è¯»|é™„\s*å½•|ACKNOWLEDGMENT|é™„ä»¶|ä¸ªäººç®€å†|ä½œè€…ç®€ä»‹|$))',
            'acknowledgement': r'(è‡´\s*è°¢[\s\S]{100,2000}?)(?=æ”»è¯»|é™„å½•|å¤§è¿ç†å·¥å¤§å­¦|$)',
            'publications': r'(æ”»è¯».*?å­¦ä½æœŸé—´å‘è¡¨.*?è®ºæ–‡[\s\S]{100,2000}?)(?=è‡´\s*è°¢|é™„å½•|$)',
            # ä¼ ç»Ÿç« èŠ‚æ ¼å¼ä½œä¸ºå¤‡é€‰
            'introduction': r'((?:ç¬¬ä¸€ç« |ç¬¬1ç« |å¼•\s*è¨€|ç»ª\s*è®º|æ¦‚\s*è¿°)[\s\S]{500,10000}?)(?=ç¬¬äºŒç« |ç¬¬2ç« |2\s)',
            'literature': r'((?:ç¬¬äºŒç« |ç¬¬2ç« |æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|åŸºç¡€ç†è®º)[\s\S]{1000,20000}?)(?=ç¬¬ä¸‰ç« |ç¬¬3ç« |3\s)',
            'methodology': r'((?:ç¬¬ä¸‰ç« |ç¬¬3ç« |ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å›¾åƒåˆ†å‰²)[\s\S]{1000,15000}?)(?=ç¬¬å››ç« |ç¬¬4ç« |4\s)',
            'results': r'((?:ç¬¬å››ç« |ç¬¬4ç« |å®éªŒç»“æœ|ç»“æœåˆ†æ|æ¨¡å‹æ„å»º)[\s\S]{1000,15000}?)(?=ç¬¬äº”ç« |ç¬¬5ç« |5\s|ç»“è®º)',
        }

        # è¯†åˆ«å¹¶æå–ç« èŠ‚å†…å®¹ä¸æ ‡é¢˜ä¿¡æ¯
        for section_name, pattern in section_patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                section_content = match.group(1).strip()
                sections[section_name] = section_content

                # æå–ç« èŠ‚æ ‡é¢˜å’Œè¾¹ç•Œä¿¡æ¯
                section_info = self._extract_section_title_and_boundaries(
                    section_content, section_name, text, match.start(), match.end()
                )

                # ä¿å­˜è¯¦ç»†çš„ç« èŠ‚ä¿¡æ¯
                sections[f"{section_name}_info"] = section_info
                print(
                    f"   ğŸ“ è¯†åˆ«ç« èŠ‚: {section_name} | æ ‡é¢˜: {section_info.get('title', 'N/A')}"
                )

        return sections

    def _extract_section_title_and_boundaries(
        self,
        section_content: str,
        section_name: str,
        full_text: str,
        start_pos: int,
        end_pos: int,
    ) -> Dict[str, Any]:
        """æå–ç« èŠ‚æ ‡é¢˜å’Œè¾¹ç•Œä¿¡æ¯"""
        section_info = {
            'section_name': section_name,
            'title': '',
            'start_position': start_pos,
            'end_position': end_pos,
            'content_length': len(section_content),
            'boundaries': {
                'start_line': full_text[:start_pos].count('\n') + 1,
                'end_line': full_text[:end_pos].count('\n') + 1,
            },
        }

        # ç« èŠ‚æ ‡é¢˜æå–æ¨¡å¼ - å¢å¼ºç‰ˆæ”¯æŒæ•°å­—æ ¼å¼
        title_patterns = {
            'cover': [
                r'^([^\n\r]*(?:æŠ€æœ¯|ç ”ç©¶|åˆ†æ|ç³»ç»Ÿ|æ–¹æ³•|ç†è®º|åº”ç”¨|è®¾è®¡|å¼€å‘|å®ç°|æ€§èƒ½|å»ºæ¨¡|ä¼˜åŒ–|è¯„ä¼°|æ¢ç´¢|æ¢è®¨)[^\n\r]*)',  # åŒ¹é…è®ºæ–‡æ ‡é¢˜å…³é”®è¯
                r'^([^\n\r]*(?:çš„|åœ¨|åŸºäº|å…³äº)[^\n\r]*(?:ç ”ç©¶|åˆ†æ|åº”ç”¨|è®¾è®¡|ç³»ç»Ÿ|æ–¹æ³•)[^\n\r]*)',  # åŒ¹é…æ ‡é¢˜æ ¼å¼
                r'^([^\n\r]*(?:åŠ›å­¦|éŸ§å¸¦|å…³èŠ‚|ææ–™|æœºæ¢°)[^\n\r]*(?:æ€§èƒ½|ç‰¹æ€§|åˆ†æ|ç ”ç©¶)[^\n\r]*)',  # åŒ¹é…åŠ›å­¦ç›¸å…³æ ‡é¢˜
                r'è†å…³èŠ‚éŸ§å¸¦çš„åŠ›å­¦æ€§èƒ½',  # ç›´æ¥åŒ¹é…å·²çŸ¥æ ‡é¢˜
            ],
            'abstract_cn': [
                r'^((?:ä¸­æ–‡)?æ‘˜\s*è¦)\s*',
                r'(æ‘˜\s*è¦)',
            ],
            'abstract_en': [
                r'^(ABSTRACT|Abstract)\s*',
            ],
            'keywords_cn': [
                r'^(å…³é”®è¯)[ï¼š:\s]*',
            ],
            'keywords_en': [
                r'^(Keywords?|KEY\s+WORDS?|Key\s+Words?)[ï¼š:\s]*',
            ],
            'toc': [
                r'^(ç›®\s*å½•)\s*',
            ],
            # Markdownæ ¼å¼ç« èŠ‚ - ä¸»è¦æ¨¡å¼
            'section_1': [
                r'###\s*1\.\s*([^\n\r]*)',
            ],
            'section_2': [
                r'###\s*2\.\s*([^\n\r]*)',
            ],
            # æ•°å­—æ ¼å¼ç« èŠ‚ - å¿ƒè„å»ºæ¨¡è®ºæ–‡ä¸“ç”¨æ¨¡å¼
            'chapter_1_introduction': [
                r'^(1\s+ç»ª\s*è®º)',
                r'^(\d+\s+ç»ª\s*è®º)',
            ],
            'chapter_2_theory': [
                r'^(2\s+å¿ƒè„å»ºæ¨¡çš„åŸºç¡€ç†è®º)',
                r'^(\d+\s+å¿ƒè„å»ºæ¨¡çš„åŸºç¡€ç†è®º)',
            ],
            'chapter_3_segmentation': [
                r'^(3\s+å¿ƒè„CTAå›¾åƒåˆ†å‰²)',
                r'^(\d+\s+å¿ƒè„CTAå›¾åƒåˆ†å‰²)',
            ],
            'chapter_4_modeling': [
                r'^(4\s+å››ç»´åŠ¨æ€ç»Ÿè®¡ä½“å½¢å¿ƒè„æ¨¡å‹çš„æ„å»º)',
                r'^(\d+\s+å››ç»´åŠ¨æ€ç»Ÿè®¡ä½“å½¢å¿ƒè„æ¨¡å‹çš„æ„å»º)',
            ],
            'chapter_5_conclusion': [
                r'^(5\s+ç»“\s*è®º)',
                r'^(\d+\s+ç»“\s*è®º)',
            ],
            # é€šç”¨æ•°å­—æ ¼å¼ç« èŠ‚ - å¤‡é€‰æ¨¡å¼
            'chapter_1': [
                r'^(\d+\s+ç»ª\s*è®º)',
                r'^(1\s+[^\n\r]*)',
            ],
            'chapter_2': [
                r'^(\d+\s+[\u4e00-\u9fff].*?åŸºç¡€ç†è®º)',
                r'^(2\s+[^\n\r]*)',
            ],
            'chapter_3': [
                r'^(\d+\s+[\u4e00-\u9fff].*?å›¾åƒåˆ†å‰²)',
                r'^(3\s+[^\n\r]*)',
            ],
            'chapter_4': [
                r'^(\d+\s+å››ç»´åŠ¨æ€[^\n\r]*)',
                r'^(4\s+[^\n\r]*)',
            ],
            'chapter_5': [
                r'^(\d+\s+ç»“\s*è®º[^\n\r]*)',
                r'^(5\s+[^\n\r]*)',
            ],
            # Markdownå­ç« èŠ‚æ ¼å¼
            'subsection_1_1': [
                r'###\s*1\.1\s*([^\n\r]*)',
            ],
            'subsection_1_2': [
                r'###\s*1\.2\s*([^\n\r]*)',
            ],
            'subsection_1_3': [
                r'###\s*1\.3\s*([^\n\r]*)',
            ],
            'subsection_2_1': [
                r'###\s*2\.1\s*([^\n\r]*)',
            ],
            'subsection_2_2': [
                r'###\s*2\.2\s*([^\n\r]*)',
            ],
            'subsection_2_3': [
                r'###\s*2\.3\s*([^\n\r]*)',
            ],
            'subsection_3_1': [
                r'###\s*3\.1\s*([^\n\r]*)',
            ],
            'subsection_3_2': [
                r'###\s*3\.2\s*([^\n\r]*)',
            ],
            'subsection_3_3': [
                r'###\s*3\.3\s*([^\n\r]*)',
            ],
            'subsection_3_4': [
                r'###\s*3\.4\s*([^\n\r]*)',
            ],
            'subsection_3_5': [
                r'###\s*3\.5\s*([^\n\r]*)',
            ],
            'subsection_4_1': [
                r'###\s*4\.1\s*([^\n\r]*)',
            ],
            'subsection_4_2': [
                r'###\s*4\.2\s*([^\n\r]*)',
            ],
            'subsection_4_3': [
                r'###\s*4\.3\s*([^\n\r]*)',
            ],
            'subsection_4_4': [
                r'###\s*4\.4\s*([^\n\r]*)',
            ],
            # ä¼ ç»Ÿæ ¼å¼ç« èŠ‚ - å¤‡é€‰æ¨¡å¼
            'introduction': [
                r'^(ç¬¬[ä¸€1]ç« )\s*([^\n\r]*)',
                r'^(å¼•\s*è¨€|ç»ª\s*è®º|æ¦‚\s*è¿°)\s*',
            ],
            'literature': [
                r'^(ç¬¬[äºŒ2]ç« )\s*([^\n\r]*)',
                r'^(æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|åŸºç¡€ç†è®º)\s*',
            ],
            'methodology': [
                r'^(ç¬¬[ä¸‰3]ç« )\s*([^\n\r]*)',
                r'^(ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å›¾åƒåˆ†å‰²)\s*',
            ],
            'results': [
                r'^(ç¬¬[å››4]ç« )\s*([^\n\r]*)',
                r'^(å®éªŒç»“æœ|ç»“æœåˆ†æ|æ¨¡å‹æ„å»º)\s*',
            ],
            'conclusion': [
                r'^(ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›|æ€»ç»“ä¸å±•æœ›|ç»“è®ºä¸å»ºè®®|ç ”ç©¶æ€»ç»“|ä¸»è¦ç»“è®º|æœ¬æ–‡ç»“è®º)\s*',
            ],
            'references': [
                r'^(å‚\s*è€ƒ\s*æ–‡\s*çŒ®|REFERENCES?|References?)\s*',
                r'(å‚\s*è€ƒ\s*æ–‡\s*çŒ®)',
            ],
            'acknowledgement': [
                r'^(è‡´\s*è°¢)\s*',
            ],
            'publications': [
                r'^(æ”»è¯».*?å­¦ä½æœŸé—´å‘è¡¨.*?è®ºæ–‡)\s*',
            ],
            # ä¼ ç»Ÿæ ¼å¼ç« èŠ‚ - å¤‡é€‰æ¨¡å¼
            'introduction_alt': [
                r'^(ç¬¬[ä¸€1]ç« )\s*([^\n\r]*)',
                r'^(å¼•\s*è¨€|ç»ª\s*è®º|æ¦‚\s*è¿°)\s*',
            ],
            'literature_alt': [
                r'^(ç¬¬[äºŒ2]ç« )\s*([^\n\r]*)',
                r'^(æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|åŸºç¡€ç†è®º)\s*',
            ],
            'methodology_alt': [
                r'^(ç¬¬[ä¸‰3]ç« )\s*([^\n\r]*)',
                r'^(ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å›¾åƒåˆ†å‰²)\s*',
            ],
            'results_alt': [
                r'^(ç¬¬[å››4]ç« )\s*([^\n\r]*)',
                r'^(å®éªŒç»“æœ|ç»“æœåˆ†æ|æ¨¡å‹æ„å»º)\s*',
            ],
            'conclusion_alt': [
                r'^(ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›|æ€»ç»“ä¸å±•æœ›|ç»“è®ºä¸å»ºè®®|ç ”ç©¶æ€»ç»“|ä¸»è¦ç»“è®º|æœ¬æ–‡ç»“è®º)\s*',
            ],
            'references_alt': [
                r'^(å‚\s*è€ƒ\s*æ–‡\s*çŒ®|REFERENCES?|References?)\s*',
                r'(å‚\s*è€ƒ\s*æ–‡\s*çŒ®)',
            ],
            'acknowledgement_alt': [
                r'^(è‡´\s*è°¢)\s*',
            ],
        }

        # æå–æ ‡é¢˜ - ç‰¹æ®Šå¤„ç†å°é¢
        if section_name == 'cover':
            # å¯¹äºå°é¢ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†æ‰¾åˆ°çœŸæ­£çš„è®ºæ–‡æ ‡é¢˜
            title_found = False
            lines = section_content.split('\n')
            for line in lines:
                line = line.strip()
                # å¯»æ‰¾è®ºæ–‡æ ‡é¢˜ç‰¹å¾
                if (
                    len(line) > 8
                    and len(line) < 100
                    and any(
                        keyword in line
                        for keyword in [
                            'æŠ€æœ¯',
                            'ç ”ç©¶',
                            'åˆ†æ',
                            'ç³»ç»Ÿ',
                            'æ–¹æ³•',
                            'ç†è®º',
                            'åº”ç”¨',
                            'è®¾è®¡',
                            'å¼€å‘',
                            'å®ç°',
                            'æ€§èƒ½',
                            'åŠ›å­¦',
                            'éŸ§å¸¦',
                            'å…³èŠ‚',
                            'ææ–™',
                            'æœºæ¢°',
                        ]
                    )
                    and not any(
                        exclude in line
                        for exclude in [
                            '#',
                            '**',
                            'æºæ–‡ä»¶',
                            'è½¬æ¢',
                            'å­¦æ ¡',
                            'å­¦å·',
                            'å£°æ˜',
                            'å¯¼å¸ˆ',
                            'å®Œæˆ',
                            'æ—¥æœŸ',
                            'å§“å',
                            'ä½œè€…',
                            'ç­¾å',
                            'æ‰¿æ‹…',
                            'æ³•å¾‹',
                        ]
                    )
                ):
                    section_info['title'] = line
                    title_found = True
                    break

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤çš„æ¨¡å¼åŒ¹é…
            if not title_found and section_name in title_patterns:
                for pattern in title_patterns[section_name]:
                    match = re.search(
                        pattern, section_content, re.IGNORECASE | re.MULTILINE
                    )
                    if match:
                        section_info['title'] = match.group(1).strip()
                        break
        else:
            # å…¶ä»–ç« èŠ‚ä½¿ç”¨æ ‡å‡†çš„æ ‡é¢˜æå–
            if section_name in title_patterns:
                for pattern in title_patterns[section_name]:
                    match = re.search(
                        pattern, section_content, re.IGNORECASE | re.MULTILINE
                    )
                    if match:
                        if match.lastindex and match.lastindex >= 2:  # æœ‰ç« èŠ‚å·å’Œæ ‡é¢˜
                            section_info['title'] = (
                                f"{match.group(1)} {match.group(2)}".strip()
                            )
                        else:  # åªæœ‰æ ‡é¢˜
                            section_info['title'] = match.group(1).strip()
                        break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»å†…å®¹é¦–è¡Œæå–
        if not section_info['title']:
            first_line = section_content.split('\n')[0].strip()
            if len(first_line) < 100:  # æ ‡é¢˜é€šå¸¸ä¸ä¼šå¤ªé•¿
                section_info['title'] = first_line

        # æ£€æµ‹ç« èŠ‚è¾¹ç•Œçš„ç²¾ç¡®æ€§
        section_info['boundary_confidence'] = self._calculate_boundary_confidence(
            section_content, section_name, full_text, start_pos, end_pos
        )

        return section_info

    def _calculate_boundary_confidence(
        self,
        section_content: str,
        section_name: str,
        full_text: str,
        start_pos: int,
        end_pos: int,
    ) -> float:
        """è®¡ç®—ç« èŠ‚è¾¹ç•Œè¯†åˆ«çš„ç½®ä¿¡åº¦"""
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦

        # æ£€æŸ¥å¼€å§‹è¾¹ç•Œ
        if start_pos > 0:
            before_text = full_text[max(0, start_pos - 100) : start_pos]
            if re.search(r'\n\s*$', before_text):  # å‰é¢æœ‰æ¢è¡Œ
                confidence += 0.2

        # æ£€æŸ¥ç»“æŸè¾¹ç•Œ
        if end_pos < len(full_text):
            after_text = full_text[end_pos : min(len(full_text), end_pos + 100)]
            if re.search(r'^\s*\n', after_text):  # åé¢æœ‰æ¢è¡Œ
                confidence += 0.2

        # æ£€æŸ¥å†…å®¹å®Œæ•´æ€§
        content_lines = section_content.split('\n')
        if len(content_lines) > 3:  # å†…å®¹æœ‰ä¸€å®šé•¿åº¦
            confidence += 0.1

        # æ£€æŸ¥ç‰¹å®šç« èŠ‚çš„ç‰¹å¾
        if section_name == 'references':
            # å‚è€ƒæ–‡çŒ®åº”è¯¥åŒ…å«å¼•ç”¨æ ¼å¼
            if re.search(r'\[\d+\]|\d+\.', section_content):
                confidence += 0.2
        elif section_name in ['abstract_cn', 'abstract_en']:
            # æ‘˜è¦åº”è¯¥æœ‰ä¸€å®šé•¿åº¦ä¸”ä¸åŒ…å«å¼•ç”¨
            if 100 <= len(section_content) <= 2000 and not re.search(
                r'\[\d+\]', section_content
            ):
                confidence += 0.2

        return min(1.0, confidence)

    def _extract_content_by_sections(
        self, text: str, sections: Dict[str, str]
    ) -> Dict[str, Any]:
        """æ­¥éª¤3: åŸºäºç« èŠ‚ç»“æ„è¿›è¡Œå†…å®¹æå–"""
        content_info = {}

        # ä»æ‘˜è¦éƒ¨åˆ†æå–
        if 'abstract_cn' in sections:
            content_info['abstract_cn'] = self._clean_abstract(sections['abstract_cn'])
            print(f"   âœ… ä¸­æ–‡æ‘˜è¦: {len(content_info['abstract_cn'])} å­—ç¬¦")

            # æ˜¾ç¤ºç« èŠ‚è¾¹ç•Œä¿¡æ¯
            if 'abstract_cn_info' in sections:
                section_info = sections['abstract_cn_info']
                if isinstance(section_info, dict):
                    print(f"      ğŸ“‹ æ ‡é¢˜: {section_info.get('title', 'N/A')}")
                    boundaries = section_info.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(
                            f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}"
                        )
                    print(
                        f"      ğŸ¯ ç½®ä¿¡åº¦: {section_info.get('boundary_confidence', 0):.2f}"
                    )

        if 'abstract_en' in sections:
            content_info['abstract_en'] = self._clean_abstract(sections['abstract_en'])
            print(f"   âœ… è‹±æ–‡æ‘˜è¦: {len(content_info['abstract_en'])} å­—ç¬¦")

            # æ˜¾ç¤ºç« èŠ‚è¾¹ç•Œä¿¡æ¯
            if 'abstract_en_info' in sections:
                section_info = sections['abstract_en_info']
                if isinstance(section_info, dict):
                    print(f"      ğŸ“‹ æ ‡é¢˜: {section_info.get('title', 'N/A')}")
                    boundaries = section_info.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(
                            f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}"
                        )
                    print(
                        f"      ğŸ¯ ç½®ä¿¡åº¦: {section_info.get('boundary_confidence', 0):.2f}"
                    )

        # ä»å…³é”®è¯éƒ¨åˆ†æå–
        if 'keywords_cn' in sections:
            keywords = self._extract_keywords(sections['keywords_cn'], 'chinese')
            content_info['keywords_cn'] = keywords
            print(f"   âœ… ä¸­æ–‡å…³é”®è¯: {keywords}")

            # æ˜¾ç¤ºç« èŠ‚è¾¹ç•Œä¿¡æ¯
            if 'keywords_cn_info' in sections:
                section_info = sections['keywords_cn_info']
                if isinstance(section_info, dict):
                    print(f"      ğŸ“‹ æ ‡é¢˜: {section_info.get('title', 'N/A')}")
                    boundaries = section_info.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(
                            f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}"
                        )

        if 'keywords_en' in sections:
            keywords = self._extract_keywords(sections['keywords_en'], 'english')
            content_info['keywords_en'] = keywords
            print(f"   âœ… è‹±æ–‡å…³é”®è¯: {keywords}")

            # æ˜¾ç¤ºç« èŠ‚è¾¹ç•Œä¿¡æ¯
            if 'keywords_en_info' in sections:
                section_info = sections['keywords_en_info']
                if isinstance(section_info, dict):
                    print(f"      ğŸ“‹ æ ‡é¢˜: {section_info.get('title', 'N/A')}")
                    boundaries = section_info.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(
                            f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}"
                        )

        # æ˜¾ç¤ºå…¶ä»–é‡è¦ç« èŠ‚çš„è¾¹ç•Œä¿¡æ¯
        major_sections = [
            'introduction',
            'literature',
            'methodology',
            'results',
            'conclusion',
            'references',
        ]
        for section_name in major_sections:
            info_key = f'{section_name}_info'
            if info_key in sections:
                section_info = sections[info_key]
                if isinstance(section_info, dict):
                    print(
                        f"   ğŸ“– {section_name.title()}: {section_info.get('title', 'N/A')}"
                    )
                    boundaries = section_info.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(
                            f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}"
                        )
                    print(
                        f"      ğŸ“ é•¿åº¦: {section_info.get('content_length', 0)} å­—ç¬¦"
                    )
                    print(
                        f"      ğŸ¯ ç½®ä¿¡åº¦: {section_info.get('boundary_confidence', 0):.2f}"
                    )

        # ä¿å­˜ç« èŠ‚è¾¹ç•Œä¿¡æ¯åˆ°è¿”å›ç»“æœä¸­
        content_info['section_boundaries'] = {}
        for key, value in sections.items():
            if key.endswith('_info') and isinstance(value, dict):
                section_name = key.replace('_info', '')
                content_info['section_boundaries'][section_name] = value

        return content_info

    def _conduct_ai_analysis_on_sections(
        self, text: str, sections: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åŸºäºæ­¥éª¤3ç»“æ„åˆ†æç»“æœè¿›è¡ŒAIæ™ºèƒ½å†…å®¹åˆ†æ - ä¼˜åŒ–ç‰ˆæ”¯æŒå¹¶å‘å¤„ç†"""
        ai_analysis = {
            'section_analysis': {},
            'content_quality': {},
            'academic_insights': {},
            'structure_evaluation': {},
        }

        if not self.ai_client:
            print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡AIåˆ†æ")
            return ai_analysis

        print("   ğŸ¤– å¯åŠ¨åŸºäºç« èŠ‚ç»“æ„çš„AIæ™ºèƒ½åˆ†æï¼ˆå¹¶å‘æ¨¡å¼ï¼‰...")

        # åˆ†æå„ä¸ªä¸»è¦ç« èŠ‚
        key_sections = [
            'abstract_cn',
            'abstract_en',
            'introduction',
            'literature',
            'methodology',
            'results',
            'conclusion',
            'references',
        ]

        # å‡†å¤‡å¹¶å‘ä»»åŠ¡
        section_tasks = []
        for section_name in key_sections:
            if section_name in sections and sections[section_name]:
                content = sections[section_name]
                info_key = f"{section_name}_info"
                section_info = sections.get(info_key, {})

                section_tasks.append(
                    {
                        'section_name': section_name,
                        'content': content,
                        'section_info': section_info,
                    }
                )

        if section_tasks:
            print(f"   ğŸš€ å‡†å¤‡å¹¶å‘åˆ†æ {len(section_tasks)} ä¸ªç« èŠ‚...")
            # ä½¿ç”¨å¹¶å‘å¤„ç†ç« èŠ‚åˆ†æ
            section_results = self._analyze_sections_concurrently(section_tasks)
            ai_analysis['section_analysis'] = section_results

            success_count = len([r for r in section_results.values() if r])
            print(
                f"   âœ… ç« èŠ‚å¹¶å‘åˆ†æå®Œæˆ: {success_count}/{len(section_tasks)} ä¸ªç« èŠ‚æˆåŠŸ"
            )

        # å¹¶å‘æ‰§è¡Œæ•´ä½“è¯„ä¼°ä»»åŠ¡
        print("   ğŸ“Š æ‰§è¡Œæ•´ä½“è¯„ä¼°ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰...")

        # å‡†å¤‡å¹¶å‘è¯„ä¼°ä»»åŠ¡
        evaluation_tasks = [
            (
                'structure_evaluation',
                self._evaluate_document_structure_with_ai,
                sections,
            ),
            ('content_quality', self._assess_academic_quality_with_ai, sections),
        ]

        # å¹¶å‘æ‰§è¡Œè¯„ä¼°
        evaluation_results = self._execute_evaluation_tasks_concurrently(
            evaluation_tasks
        )
        ai_analysis.update(evaluation_results)

        print(
            f"   âœ… AIæ™ºèƒ½åˆ†æå®Œæˆ: å·²åˆ†æ {len(ai_analysis['section_analysis'])} ä¸ªç« èŠ‚"
        )

        return ai_analysis

    def _analyze_sections_concurrently(
        self, section_tasks: List[Dict]
    ) -> Dict[str, Any]:
        """å¹¶å‘åˆ†æå¤šä¸ªç« èŠ‚å†…å®¹"""
        import concurrent.futures
        import time

        results = {}
        max_workers = min(4, len(section_tasks))  # é™åˆ¶å¹¶å‘æ•°é¿å…APIé™åˆ¶

        print(f"   ğŸ”„ å¯åŠ¨ {max_workers} ä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹...")

        def analyze_single_section(task):
            """åˆ†æå•ä¸ªç« èŠ‚çš„å·¥ä½œå‡½æ•°"""
            section_name = task['section_name']
            content = task['content']
            section_info = task['section_info']

            try:
                print(f"      ğŸ” [{section_name}] å¼€å§‹åˆ†æ...")
                start_time = time.time()

                result = self._analyze_section_content_with_ai(
                    section_name, content, section_info
                )

                elapsed = time.time() - start_time
                if result:
                    print(
                        f"      âœ… [{section_name}] åˆ†æå®Œæˆ ({elapsed:.1f}s, {len(content)} å­—ç¬¦)"
                    )
                    return section_name, result
                else:
                    print(f"      âš ï¸ [{section_name}] åˆ†æå¤±è´¥ ({elapsed:.1f}s)")
                    return section_name, None

            except Exception as e:
                print(f"      âŒ [{section_name}] åˆ†æå¼‚å¸¸: {e}")
                return section_name, None

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘åˆ†æ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            start_time = time.time()

            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_section = {
                executor.submit(analyze_single_section, task): task['section_name']
                for task in section_tasks
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_section):
                section_name = future_to_section[future]
                try:
                    returned_name, result = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                    if result:
                        results[returned_name] = result
                except concurrent.futures.TimeoutError:
                    print(f"      â° [{section_name}] åˆ†æè¶…æ—¶")
                except Exception as e:
                    print(f"      âŒ [{section_name}] å¹¶å‘æ‰§è¡Œå¼‚å¸¸: {e}")

            total_time = time.time() - start_time
            print(
                f"   âš¡ å¹¶å‘ç« èŠ‚åˆ†æå®Œæˆ: {len(results)}/{len(section_tasks)} æˆåŠŸï¼Œæ€»è€—æ—¶ {total_time:.1f}s"
            )

        return results

    def _execute_evaluation_tasks_concurrently(
        self, evaluation_tasks: List[tuple]
    ) -> Dict[str, Any]:
        """å¹¶å‘æ‰§è¡Œè¯„ä¼°ä»»åŠ¡"""
        import concurrent.futures
        import time

        results = {}

        def execute_evaluation_task(task):
            """æ‰§è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡"""
            task_name, task_func, task_data = task

            try:
                print(f"      ğŸ” [{task_name}] å¼€å§‹è¯„ä¼°...")
                start_time = time.time()

                result = task_func(task_data)

                elapsed = time.time() - start_time
                if result:
                    print(f"      âœ… [{task_name}] è¯„ä¼°å®Œæˆ ({elapsed:.1f}s)")
                    return task_name, result
                else:
                    print(f"      âš ï¸ [{task_name}] è¯„ä¼°å¤±è´¥ ({elapsed:.1f}s)")
                    return task_name, {}

            except Exception as e:
                print(f"      âŒ [{task_name}] è¯„ä¼°å¼‚å¸¸: {e}")
                return task_name, {}

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯„ä¼°
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            start_time = time.time()

            # æäº¤æ‰€æœ‰è¯„ä¼°ä»»åŠ¡
            future_to_task = {
                executor.submit(execute_evaluation_task, task): task[0]
                for task in evaluation_tasks
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_task):
                task_name = future_to_task[future]
                try:
                    returned_name, result = future.result(timeout=45)  # 45ç§’è¶…æ—¶
                    results[returned_name] = result
                except concurrent.futures.TimeoutError:
                    print(f"      â° [{task_name}] è¯„ä¼°è¶…æ—¶")
                    results[task_name] = {}
                except Exception as e:
                    print(f"      âŒ [{task_name}] å¹¶å‘æ‰§è¡Œå¼‚å¸¸: {e}")
                    results[task_name] = {}

            total_time = time.time() - start_time
            print(f"   âš¡ å¹¶å‘è¯„ä¼°å®Œæˆ: æ€»è€—æ—¶ {total_time:.1f}s")

        return results

    def _analyze_section_content_with_ai(
        self, section_name: str, content: str, section_info: Dict
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†æå•ä¸ªç« èŠ‚å†…å®¹"""
        try:
            # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not self.ai_client:
                logger.warning(f"AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡ç« èŠ‚ {section_name} åˆ†æ")
                return {}

            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = self._build_section_analysis_prompt(
                section_name, content, section_info
            )

            # è°ƒç”¨AIè¿›è¡Œåˆ†æ
            response = self.ai_client.send_message(analysis_prompt)

            if response and response.content:
                # è§£æAIå“åº”
                analysis_result = self._parse_ai_section_analysis(
                    response.content.strip(), section_name
                )

                # æ·»åŠ å…ƒæ•°æ®
                analysis_result['section_name'] = section_name
                analysis_result['content_length'] = len(content)
                analysis_result['analysis_timestamp'] = datetime.now().isoformat()

                if isinstance(section_info, dict):
                    analysis_result['boundary_confidence'] = section_info.get(
                        'boundary_confidence', 0
                    )
                    analysis_result['section_title'] = section_info.get('title', '')

                return analysis_result

        except Exception as e:
            logger.error(f"AIåˆ†æç« èŠ‚ {section_name} å¤±è´¥: {e}")

        return {}
        """ä½¿ç”¨AIåˆ†æå•ä¸ªç« èŠ‚å†…å®¹"""
        try:
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = self._build_section_analysis_prompt(
                section_name, content, section_info
            )

            # è°ƒç”¨AIè¿›è¡Œåˆ†æ
            response = self.ai_client.send_message(analysis_prompt)

            if response and response.content:
                # è§£æAIå“åº”
                analysis_result = self._parse_ai_section_analysis(
                    response.content.strip(), section_name
                )

                # æ·»åŠ å…ƒæ•°æ®
                analysis_result['section_name'] = section_name
                analysis_result['content_length'] = len(content)
                analysis_result['analysis_timestamp'] = datetime.now().isoformat()

                if isinstance(section_info, dict):
                    analysis_result['boundary_confidence'] = section_info.get(
                        'boundary_confidence', 0
                    )
                    analysis_result['section_title'] = section_info.get('title', '')

                return analysis_result

        except Exception as e:
            logger.error(f"AIåˆ†æç« èŠ‚ {section_name} å¤±è´¥: {e}")

        return {}

    def _build_section_analysis_prompt(
        self, section_name: str, content: str, section_info: Dict
    ) -> str:
        """æ„å»ºç« èŠ‚åˆ†æAIæç¤º"""

        # æ ¹æ®ç« èŠ‚ç±»å‹å®šåˆ¶åˆ†æé‡ç‚¹
        analysis_focus = {
            'abstract_cn': 'æ‘˜è¦çš„å®Œæ•´æ€§ã€æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬ã€ç ”ç©¶ä»·å€¼',
            'abstract_en': 'è‹±æ–‡æ‘˜è¦çš„è¯­è¨€è´¨é‡ã€ä¸ä¸­æ–‡æ‘˜è¦çš„ä¸€è‡´æ€§',
            'introduction': 'ç ”ç©¶èƒŒæ™¯ã€é—®é¢˜æå‡ºã€ç ”ç©¶æ„ä¹‰å’Œåˆ›æ–°ç‚¹',
            'literature': 'æ–‡çŒ®ç»¼è¿°çš„å…¨é¢æ€§ã€æ‰¹åˆ¤æ€§åˆ†æã€ç ”ç©¶ç©ºç™½è¯†åˆ«',
            'methodology': 'ç ”ç©¶æ–¹æ³•çš„ç§‘å­¦æ€§ã€å¯è¡Œæ€§ã€åˆ›æ–°æ€§',
            'results': 'å®éªŒç»“æœçš„å®Œæ•´æ€§ã€æ•°æ®åˆ†æçš„æ·±åº¦',
            'conclusion': 'ç»“è®ºçš„é€»è¾‘æ€§ã€ç ”ç©¶è´¡çŒ®çš„æ€»ç»“ã€æœªæ¥å±•æœ›',
            'references': 'å‚è€ƒæ–‡çŒ®çš„è´¨é‡ã€æ•°é‡ã€æ—¶æ•ˆæ€§å’Œæƒå¨æ€§',
        }

        focus = analysis_focus.get(section_name, 'å†…å®¹çš„å­¦æœ¯è´¨é‡å’Œç»“æ„åˆç†æ€§')

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ç« èŠ‚å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨{focus}ï¼š

ç« èŠ‚ç±»å‹ï¼š{section_name}
ç« èŠ‚æ ‡é¢˜ï¼š{section_info.get('title', '') if isinstance(section_info, dict) else ''}
å†…å®¹é•¿åº¦ï¼š{len(content)}å­—ç¬¦

ç« èŠ‚å†…å®¹ï¼š
{content[:2000]}{'...' if len(content) > 2000 else ''}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼š
1. å†…å®¹è´¨é‡ (1-10åˆ†)ï¼šå­¦æœ¯æ€§ã€é€»è¾‘æ€§ã€å®Œæ•´æ€§
2. ç»“æ„åˆç†æ€§ (1-10åˆ†)ï¼šç»„ç»‡ç»“æ„ã€å±‚æ¬¡æ¸…æ™°åº¦
3. å­¦æœ¯ä»·å€¼ (1-10åˆ†)ï¼šåˆ›æ–°æ€§ã€å®ç”¨æ€§ã€ç†è®ºè´¡çŒ®
4. è¯­è¨€è¡¨è¾¾ (1-10åˆ†)ï¼šå‡†ç¡®æ€§ã€æµç•…æ€§ã€è§„èŒƒæ€§
5. ä¸»è¦ä¼˜ç‚¹ï¼šåˆ—ä¸¾2-3ä¸ªä¼˜ç‚¹
6. æ”¹è¿›å»ºè®®ï¼šæå‡º2-3ä¸ªå…·ä½“å»ºè®®
7. æ ¸å¿ƒå†…å®¹æ‘˜è¦ï¼šç”¨100å­—ä»¥å†…æ¦‚æ‹¬ä¸»è¦å†…å®¹

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
"""

        return prompt

    def _parse_ai_section_analysis(
        self, response: str, section_name: str
    ) -> Dict[str, Any]:
        """è§£æAIç« èŠ‚åˆ†æå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if response.strip().startswith('{'):
                return json.loads(response)

            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œè¿›è¡Œæ–‡æœ¬è§£æ
            analysis = {
                'content_quality_score': 0,
                'structure_score': 0,
                'academic_value_score': 0,
                'language_score': 0,
                'strengths': [],
                'improvement_suggestions': [],
                'summary': '',
                'overall_score': 0,
            }

            # æå–è¯„åˆ†
            scores = re.findall(r'(\d+)åˆ†', response)
            if len(scores) >= 4:
                analysis['content_quality_score'] = int(scores[0])
                analysis['structure_score'] = int(scores[1])
                analysis['academic_value_score'] = int(scores[2])
                analysis['language_score'] = int(scores[3])

                # è®¡ç®—æ€»åˆ†
                analysis['overall_score'] = (
                    sum(
                        [
                            analysis['content_quality_score'],
                            analysis['structure_score'],
                            analysis['academic_value_score'],
                            analysis['language_score'],
                        ]
                    )
                    / 4
                )

            # æå–ä¼˜ç‚¹
            strengths_match = re.search(
                r'ä¸»è¦ä¼˜ç‚¹[ï¼š:](.*?)æ”¹è¿›å»ºè®®', response, re.DOTALL
            )
            if strengths_match:
                strengths_text = strengths_match.group(1).strip()
                analysis['strengths'] = [
                    s.strip() for s in re.split(r'[1-3]\.', strengths_text) if s.strip()
                ]

            # æå–å»ºè®®
            suggestions_match = re.search(
                r'æ”¹è¿›å»ºè®®[ï¼š:](.*?)æ ¸å¿ƒå†…å®¹æ‘˜è¦', response, re.DOTALL
            )
            if suggestions_match:
                suggestions_text = suggestions_match.group(1).strip()
                analysis['improvement_suggestions'] = [
                    s.strip()
                    for s in re.split(r'[1-3]\.', suggestions_text)
                    if s.strip()
                ]

            # æå–æ‘˜è¦
            summary_match = re.search(r'æ ¸å¿ƒå†…å®¹æ‘˜è¦[ï¼š:](.*?)$', response, re.DOTALL)
            if summary_match:
                analysis['summary'] = summary_match.group(1).strip()

            return analysis

        except Exception as e:
            logger.error(f"è§£æAIåˆ†æå“åº”å¤±è´¥: {e}")
            return {
                'content_quality_score': 5,
                'structure_score': 5,
                'academic_value_score': 5,
                'language_score': 5,
                'overall_score': 5,
                'strengths': [],
                'improvement_suggestions': [],
                'summary': 'è§£æå¤±è´¥',
                'error': str(e),
            }

    def _evaluate_document_structure_with_ai(
        self, sections: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIè¯„ä¼°æ•´ä½“æ–‡æ¡£ç»“æ„"""
        try:
            # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not self.ai_client:
                logger.warning("AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æ¡£ç»“æ„è¯„ä¼°")
                return {
                    'structure_completeness': 5,
                    'logical_order': 5,
                    'section_balance': 5,
                    'academic_standard': 5,
                    'overall_structure_score': 5,
                    'recommendations': ['AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œç»“æ„è¯„ä¼°'],
                    'error': 'AIå®¢æˆ·ç«¯ä¸å¯ç”¨',
                }

            # æ„å»ºç»“æ„ä¿¡æ¯
            structure_info = []
            for key, value in sections.items():
                if not key.endswith('_info') and isinstance(value, str):
                    info_key = f"{key}_info"
                    info = sections.get(info_key, {})

                    structure_info.append(
                        {
                            'section': key,
                            'title': (
                                info.get('title', '') if isinstance(info, dict) else ''
                            ),
                            'length': len(value),
                            'confidence': (
                                info.get('boundary_confidence', 0)
                                if isinstance(info, dict)
                                else 0
                            ),
                        }
                    )

            structure_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹è®ºæ–‡çš„æ•´ä½“ç»“æ„ï¼š

æ–‡æ¡£ç« èŠ‚ç»“æ„ï¼š
{json.dumps(structure_info, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š
1. ç»“æ„å®Œæ•´æ€§ (1-10åˆ†)ï¼šæ˜¯å¦åŒ…å«å¿…è¦çš„ç« èŠ‚
2. é€»è¾‘é¡ºåº (1-10åˆ†)ï¼šç« èŠ‚æ’åˆ—æ˜¯å¦åˆç†
3. ç« èŠ‚å¹³è¡¡æ€§ (1-10åˆ†)ï¼šå„ç« èŠ‚é•¿åº¦æ˜¯å¦é€‚å½“
4. å­¦æœ¯è§„èŒƒæ€§ (1-10åˆ†)ï¼šæ˜¯å¦ç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ŒåŒ…å«å„é¡¹è¯„åˆ†å’Œæ€»ä½“å»ºè®®ã€‚
"""

            response = self.ai_client.send_message(structure_prompt)

            if response and response.content:
                content = response.content.strip()
                try:
                    return json.loads(content)
                except:
                    # æ–‡æœ¬è§£æå¤‡é€‰æ–¹æ¡ˆ
                    return {
                        'structure_completeness': 7,
                        'logical_order': 7,
                        'section_balance': 7,
                        'academic_standard': 7,
                        'overall_structure_score': 7,
                        'recommendations': ['ç»“æ„è¯„ä¼°å®Œæˆ'],
                        'raw_response': content,
                    }

        except Exception as e:
            logger.error(f"AIç»“æ„è¯„ä¼°å¤±è´¥: {e}")

        return {
            'structure_completeness': 5,
            'logical_order': 5,
            'section_balance': 5,
            'academic_standard': 5,
            'overall_structure_score': 5,
            'recommendations': ['è¯„ä¼°åŠŸèƒ½æš‚ä¸å¯ç”¨'],
            'error': 'è¯„ä¼°å¤±è´¥',
        }

    def _assess_academic_quality_with_ai(
        self, sections: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIè¯„ä¼°å­¦æœ¯è´¨é‡"""
        try:
            # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if not self.ai_client:
                logger.warning("AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡å­¦æœ¯è´¨é‡è¯„ä¼°")
                return {
                    'innovation_score': 5,
                    'methodology_score': 5,
                    'argumentation_score': 5,
                    'practical_value_score': 5,
                    'academic_standard_score': 5,
                    'overall_quality_score': 5,
                    'error': 'AIå®¢æˆ·ç«¯ä¸å¯ç”¨',
                }

            # æ”¶é›†å…³é”®å†…å®¹
            key_contents = {}
            for section in ['abstract_cn', 'introduction', 'methodology', 'conclusion']:
                if section in sections:
                    content = sections[section]
                    key_contents[section] = (
                        content[:500] if len(content) > 500 else content
                    )

            quality_prompt = f"""
åŸºäºä»¥ä¸‹è®ºæ–‡å…³é”®ç« èŠ‚å†…å®¹ï¼Œè¯„ä¼°å…¶å­¦æœ¯è´¨é‡ï¼š

{json.dumps(key_contents, ensure_ascii=False, indent=2)}

è¯·è¯„ä¼°ä»¥ä¸‹æ–¹é¢ï¼š
1. ç ”ç©¶åˆ›æ–°æ€§ (1-10åˆ†)
2. æ–¹æ³•ç§‘å­¦æ€§ (1-10åˆ†) 
3. è®ºè¯å……åˆ†æ€§ (1-10åˆ†)
4. å®ç”¨ä»·å€¼ (1-10åˆ†)
5. å­¦æœ¯è§„èŒƒæ€§ (1-10åˆ†)

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœã€‚
"""

            response = self.ai_client.send_message(quality_prompt)

            if response and response.content:
                content = response.content.strip()
                try:
                    return json.loads(content)
                except:
                    return {
                        'innovation_score': 6,
                        'methodology_score': 6,
                        'argumentation_score': 6,
                        'practical_value_score': 6,
                        'academic_standard_score': 6,
                        'overall_quality_score': 6,
                        'raw_response': content,
                    }

        except Exception as e:
            logger.error(f"AIè´¨é‡è¯„ä¼°å¤±è´¥: {e}")

        return {
            'innovation_score': 5,
            'methodology_score': 5,
            'argumentation_score': 5,
            'practical_value_score': 5,
            'academic_standard_score': 5,
            'overall_quality_score': 5,
            'error': 'è¯„ä¼°å¤±è´¥',
        }

    def _extract_ai_insights(self, section_analysis: Dict[str, Any]) -> List[str]:
        """ä»AIç« èŠ‚åˆ†æä¸­æå–å…³é”®æ´å¯Ÿå’Œå»ºè®®"""
        insights = []

        for section_name, analysis in section_analysis.items():
            if isinstance(analysis, dict):
                # æå–ä¼˜ç‚¹
                strengths = analysis.get('strengths', [])
                for strength in strengths[:2]:  # æœ€å¤šå–å‰2ä¸ªä¼˜ç‚¹
                    if strength and isinstance(strength, str):
                        insights.append(f"âœ… {section_name}: {strength}")

                # æå–æ”¹è¿›å»ºè®®
                suggestions = analysis.get('improvement_suggestions', [])
                for suggestion in suggestions[:2]:  # æœ€å¤šå–å‰2ä¸ªå»ºè®®
                    if suggestion and isinstance(suggestion, str):
                        insights.append(f"ğŸ’¡ {section_name}: {suggestion}")

                # å¦‚æœè¯„åˆ†è¾ƒä½ï¼Œæ·»åŠ ç‰¹åˆ«å…³æ³¨
                overall_score = analysis.get('overall_score', 5)
                if overall_score < 6:
                    insights.append(
                        f"âš ï¸ {section_name}: éœ€è¦é‡ç‚¹æ”¹è¿› (è¯„åˆ†: {overall_score:.1f}/10)"
                    )

        return insights[:10]  # æœ€å¤šè¿”å›10æ¡æ´å¯Ÿ

    def _extract_and_analyze_toc(
        self, text: str, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """æ­¥éª¤2: ä½¿ç”¨æ™ºèƒ½ç›®å½•æå–ç±»è¿›è¡Œç›®å½•æå–å¹¶åŸºäºç»“æœè¿›è¡Œç« èŠ‚åˆ†æ"""
        toc_analysis = {
            'table_of_contents': [],
            'chapter_summaries': {},
            'literature_analysis': {},
            'methodology_analysis': {},
            'experimental_analysis': {},
            'results_analysis': {},
            'conclusion_analysis': {},
            'extraction_method': 'unknown',
            'confidence_score': 0.0,
        }

        # 1. ä½¿ç”¨æ™ºèƒ½ç›®å½•æå–ç±» - ä¼˜å…ˆä»Wordæ–‡æ¡£ç›´æ¥æå–
        print("   ğŸ“‹ ä½¿ç”¨æ™ºèƒ½ç›®å½•æå–ç±»...")

        # åˆå§‹åŒ–chapterså˜é‡
        chapters = []

        # å°è¯•ä½¿ç”¨AITocExtractorä»Wordæ–‡æ¡£æå–
        if file_path and file_path.endswith('.docx'):
            try:
                # å¯¼å…¥æ™ºèƒ½ç›®å½•æå–å™¨
                try:
                    from .ai_toc_extractor import AITocExtractor
                except ImportError:
                    from thesis_inno_eval.ai_toc_extractor import AITocExtractor

                # åˆ›å»ºæ™ºèƒ½æå–å™¨å®ä¾‹
                toc_extractor = AITocExtractor()
                print("   ğŸ¤– åˆå§‹åŒ–æ™ºèƒ½ç›®å½•æå–å™¨...")

                # æå–ç›®å½•ç»“æ„
                toc_result = toc_extractor.extract_toc(file_path)

                if toc_result and toc_result.entries:
                    print(f"   âœ… æ™ºèƒ½æå–åˆ° {len(toc_result.entries)} ä¸ªç›®å½•æ¡ç›®")
                    print(f"   ğŸ“Š æå–æ–¹æ³•: {toc_result.extraction_method}")
                    print(f"   ğŸ“ˆ ç½®ä¿¡åº¦: {toc_result.confidence_score:.2f}")

                    # è½¬æ¢ä¸ºç« èŠ‚æ ¼å¼
                    chapters = self._convert_toc_entries_to_chapters(toc_result.entries)

                    # è®°å½•æå–ä¿¡æ¯
                    toc_analysis['extraction_method'] = toc_result.extraction_method
                    toc_analysis['confidence_score'] = toc_result.confidence_score

                    # ä¿å­˜å®Œæ•´çš„ç›®å½•ä¿¡æ¯
                    toc_analysis['raw_toc_content'] = toc_result.toc_content
                    toc_analysis['total_entries'] = toc_result.total_entries
                    toc_analysis['max_level'] = toc_result.max_level

                else:
                    print("   âš ï¸ æ™ºèƒ½æå–å™¨æœªæ‰¾åˆ°ç›®å½•æ¡ç›®ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")

            except Exception as e:
                print(f"   âŒ æ™ºèƒ½ç›®å½•æå–å¤±è´¥: {e}")
                logger.error(f"æ™ºèƒ½ç›®å½•æå–å¤±è´¥: {e}")

        # 2. å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚æœæ™ºèƒ½æå–å¤±è´¥ï¼‰
        if not chapters:
            print("   ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æå–ç« èŠ‚ç»“æ„...")
            word_chapters = (
                self._extract_chapters_from_word(file_path) if file_path else []
            )
            if word_chapters:
                chapters = word_chapters
                print(f"   âœ… ä¼ ç»Ÿæ–¹æ³•è¯†åˆ«åˆ° {len(chapters)} ä¸ªç« èŠ‚")
                toc_analysis['extraction_method'] = 'traditional_word_parsing'

        toc_analysis['table_of_contents'] = chapters

        if not chapters:
            print("   âš ï¸ æœªè¯†åˆ«åˆ°æ˜ç¡®çš„ç« èŠ‚ç»“æ„")
            logger.warning("æ–‡æ¡£ç»“æ„åˆ†ææœªè¯†åˆ«åˆ°ä»»ä½•ç« èŠ‚ç»“æ„")
            return toc_analysis

        # 3. åŸºäºæå–çš„ç›®å½•ç»“æ„è¿›è¡Œç« èŠ‚åˆ†æ
        print(f"   ğŸ§  åŸºäºæå–çš„ {len(chapters)} ä¸ªç« èŠ‚è¿›è¡ŒAIæ™ºèƒ½åˆ†æ...")

        # åˆå§‹åŒ–ç»¼è¿°ç« èŠ‚åˆ—è¡¨
        review_chapters = []

        # 4. AIç« èŠ‚åˆ†æ - åŸºäºæ™ºèƒ½æå–çš„ç»“æœ
        if self.ai_client and len(chapters) > 0:
            print("   ğŸ” å¯åŠ¨åŸºäºç›®å½•çš„ç« èŠ‚AIæ™ºèƒ½åˆ†æ...")

            # ä¸€æ¬¡æ€§è¯†åˆ«ç»¼è¿°æ€§ç« èŠ‚
            review_chapters = self._identify_review_chapters(chapters)
            print(f"   ğŸ“‹ è¯†åˆ«åˆ° {len(review_chapters)} ä¸ªç»¼è¿°æ€§ç« èŠ‚")

            # ç»Ÿä¸€å¤„ç†ç»¼è¿°ç« èŠ‚ï¼ˆåŒ…å«ä¸“ä¸šç»¼è¿°åˆ†æï¼‰- ä¼˜åŒ–ç‰ˆæ”¯æŒå¹¶å‘
            if review_chapters:
                print("   ğŸ“– æ‰§è¡Œä¸“ä¸šç»¼è¿°åˆ†æï¼ˆå¹¶å‘æ¨¡å¼ï¼‰...")
                review_results = self._analyze_review_chapters_concurrently(
                    text, review_chapters
                )
                toc_analysis['chapter_summaries'].update(
                    review_results['chapter_summaries']
                )
                if review_results['literature_analysis']:
                    toc_analysis['literature_analysis'].update(
                        review_results['literature_analysis']
                    )

            # åˆ†æå…¶ä»–éç»¼è¿°ç« èŠ‚ - ä¼˜åŒ–ç‰ˆæ”¯æŒå¹¶å‘
            other_chapters = [ch for ch in chapters if ch not in review_chapters]
            if other_chapters:
                print("   ğŸ“š åˆ†æå…¶ä»–ç« èŠ‚ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰...")
                other_results = self._analyze_other_chapters_concurrently(
                    text, other_chapters[:3]
                )  # é™åˆ¶å¤„ç†æ•°é‡
                toc_analysis['chapter_summaries'].update(other_results)

        # 5. è¡¥å……æ–‡çŒ®ç»¼è¿°åˆ†æï¼ˆå¦‚æœæ²¡æœ‰æ˜ç¡®çš„ç»¼è¿°ç« èŠ‚ï¼‰
        if not review_chapters:
            print("   ğŸ” æ·±åº¦åˆ†ææ–‡çŒ®ç»¼è¿°...")
            literature_analysis = self._analyze_literature_section_with_ai(text)
            toc_analysis['literature_analysis'] = literature_analysis

        # 6. åˆ†æç ”ç©¶æ–¹æ³•å’Œå®éªŒ
        print("   ğŸ”¬ åˆ†æç ”ç©¶æ–¹æ³•å’Œå®éªŒ...")
        methodology_analysis = self._analyze_methodology_with_ai(text)
        toc_analysis['methodology_analysis'] = methodology_analysis

        experimental_analysis = self._analyze_experimental_section_with_ai(text)
        toc_analysis['experimental_analysis'] = experimental_analysis

        # 5. åˆ†æç»“æœå’Œç»“è®º
        print("   ğŸ“Š åˆ†æç»“æœå’Œç»“è®º...")
        results_analysis = self._analyze_results_with_ai(text)
        toc_analysis['results_analysis'] = results_analysis

        conclusion_analysis = self._analyze_conclusion_with_ai(text)
        toc_analysis['conclusion_analysis'] = conclusion_analysis

        return toc_analysis

    def _extract_and_analyze_toc_with_content_boundaries(
        self, text: str, file_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ™ºèƒ½ç›®å½•æå–å¹¶æ ¹æ®ç¬¬ä¸€å±‚çº§ç« èŠ‚è¾¹ç•Œæå–æ­£æ–‡å†…å®¹è¿›è¡Œåˆ†æ
        é‡ç‚¹å…³æ³¨ç¬¬ä¸€å±‚çº§ç« èŠ‚çš„å†…å®¹è¾¹ç•Œè¯†åˆ«å’Œç« èŠ‚æ­£æ–‡åˆ†æ
        """
        print("   ğŸ¯ å¯åŠ¨æ™ºèƒ½ç›®å½•æå–å’Œç¬¬ä¸€å±‚çº§ç« èŠ‚è¾¹ç•Œåˆ†æ...")
        
        toc_analysis = {
            'table_of_contents': [],
            'first_level_chapters': [],
            'chapter_content_boundaries': {},
            'chapter_summaries': {},
            'literature_analysis': {},
            'methodology_analysis': {},
            'experimental_analysis': {},
            'results_analysis': {},
            'conclusion_analysis': {},
            'extraction_method': 'unknown',
            'confidence_score': 0.0,
        }

        # 1. æ™ºèƒ½ç›®å½•æå–
        print("   ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½æå–ç›®å½•ç»“æ„...")
        chapters = self._extract_toc_intelligently(text, file_path)
        
        if not chapters:
            print("   âš ï¸ æœªèƒ½æå–åˆ°ç›®å½•ç»“æ„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•...")
            return self._extract_and_analyze_toc(text, file_path)

        toc_analysis['table_of_contents'] = chapters
        print(f"   âœ… æˆåŠŸæå– {len(chapters)} ä¸ªç« èŠ‚")

        # 2. ç­›é€‰ç¬¬ä¸€å±‚çº§ç« èŠ‚ï¼ˆlevel=1æˆ–ä¸»è¦ç« èŠ‚ï¼‰
        print("   ğŸ” ç¬¬äºŒæ­¥ï¼šè¯†åˆ«ç¬¬ä¸€å±‚çº§ç« èŠ‚...")
        first_level_chapters = self._extract_first_level_chapters(chapters)
        toc_analysis['first_level_chapters'] = first_level_chapters
        print(f"   âœ… è¯†åˆ«åˆ° {len(first_level_chapters)} ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚")

        # 3. ä¸ºæ¯ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚ç¡®å®šå†…å®¹è¾¹ç•Œ
        print("   ğŸ“ ç¬¬ä¸‰æ­¥ï¼šç¡®å®šç« èŠ‚å†…å®¹è¾¹ç•Œ...")
        chapter_boundaries = self._determine_chapter_content_boundaries(
            text, first_level_chapters
        )
        toc_analysis['chapter_content_boundaries'] = chapter_boundaries
        
        # 4. æå–æ¯ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚çš„æ­£æ–‡å†…å®¹
        print("   ğŸ“„ ç¬¬å››æ­¥ï¼šæå–ç« èŠ‚æ­£æ–‡å†…å®¹...")
        chapter_contents = self._extract_chapter_contents_by_boundaries(
            text, chapter_boundaries
        )

        # 5. å¯¹æ¯ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚è¿›è¡ŒAIåˆ†æ
        print("   ğŸ§  ç¬¬äº”æ­¥ï¼šAIæ™ºèƒ½åˆ†æç« èŠ‚å†…å®¹...")
        if self.ai_client and first_level_chapters:
            # åˆ†ç±»ç« èŠ‚ç±»å‹
            review_chapters = []
            methodology_chapters = []
            results_chapters = []
            conclusion_chapters = []
            
            for chapter in first_level_chapters:
                chapter_type = self._classify_chapter_type(chapter, chapter_contents.get(chapter['title'], ''))
                chapter['classified_type'] = chapter_type
                
                if chapter_type in ['introduction', 'literature_review', 'background']:
                    review_chapters.append(chapter)
                elif chapter_type in ['methodology', 'method', 'approach']:
                    methodology_chapters.append(chapter)
                elif chapter_type in ['results', 'findings', 'analysis']:
                    results_chapters.append(chapter)
                elif chapter_type in ['conclusion', 'summary']:
                    conclusion_chapters.append(chapter)

            # å¹¶å‘åˆ†æä¸åŒç±»å‹çš„ç« èŠ‚
            print(f"   ğŸ“– åˆ†æ {len(review_chapters)} ä¸ªç»¼è¿°ç±»ç« èŠ‚...")
            if review_chapters:
                review_results = self._analyze_chapters_with_content(
                    review_chapters, chapter_contents, 'review'
                )
                toc_analysis['chapter_summaries'].update(review_results.get('summaries', {}))
                toc_analysis['literature_analysis'].update(review_results.get('analysis', {}))

            print(f"   ğŸ”¬ åˆ†æ {len(methodology_chapters)} ä¸ªæ–¹æ³•ç±»ç« èŠ‚...")
            if methodology_chapters:
                method_results = self._analyze_chapters_with_content(
                    methodology_chapters, chapter_contents, 'methodology'
                )
                toc_analysis['chapter_summaries'].update(method_results.get('summaries', {}))
                toc_analysis['methodology_analysis'].update(method_results.get('analysis', {}))

            print(f"   ğŸ“Š åˆ†æ {len(results_chapters)} ä¸ªç»“æœç±»ç« èŠ‚...")
            if results_chapters:
                results_analysis = self._analyze_chapters_with_content(
                    results_chapters, chapter_contents, 'results'
                )
                toc_analysis['chapter_summaries'].update(results_analysis.get('summaries', {}))
                toc_analysis['results_analysis'].update(results_analysis.get('analysis', {}))

            print(f"   ğŸ“ åˆ†æ {len(conclusion_chapters)} ä¸ªç»“è®ºç±»ç« èŠ‚...")
            if conclusion_chapters:
                conclusion_analysis = self._analyze_chapters_with_content(
                    conclusion_chapters, chapter_contents, 'conclusion'
                )
                toc_analysis['chapter_summaries'].update(conclusion_analysis.get('summaries', {}))
                toc_analysis['conclusion_analysis'].update(conclusion_analysis.get('analysis', {}))

        # 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("   ğŸ“‹ ç¬¬å…­æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        self._generate_chapter_analysis_report(toc_analysis, first_level_chapters)

        return toc_analysis

    def _extract_toc_intelligently(self, text: str, file_path: Optional[str] = None) -> List[Dict]:
        """æ™ºèƒ½æå–ç›®å½•ç»“æ„ï¼Œä¼˜å…ˆä½¿ç”¨Wordæ–‡æ¡£ç›®å½•ï¼Œç„¶åä½¿ç”¨æ–‡æœ¬è§£æ"""
        chapters = []

        # æ–¹æ³•1ï¼šä»Wordæ–‡æ¡£ç›´æ¥æå–ï¼ˆæœ€å‡†ç¡®ï¼‰
        if file_path and file_path.endswith('.docx'):
            try:
                # å¯¼å…¥æ™ºèƒ½ç›®å½•æå–å™¨
                try:
                    from .ai_toc_extractor import AITocExtractor
                except ImportError:
                    from thesis_inno_eval.ai_toc_extractor import AITocExtractor

                toc_extractor = AITocExtractor()
                print("   ğŸ¤– ä½¿ç”¨æ™ºèƒ½ç›®å½•æå–å™¨...")

                toc_result = toc_extractor.extract_toc(file_path)
                if toc_result and toc_result.entries:
                    print(f"   âœ… æ™ºèƒ½æå–åˆ° {len(toc_result.entries)} ä¸ªç›®å½•æ¡ç›®")
                    chapters = self._convert_toc_entries_to_chapters(toc_result.entries)
                    return chapters

            except Exception as e:
                print(f"   âš ï¸ æ™ºèƒ½ç›®å½•æå–å¤±è´¥: {e}")

        # æ–¹æ³•2ï¼šæ–‡æœ¬æ¨¡å¼è¯†åˆ«ç›®å½•ç»“æ„
        if not chapters:
            print("   ğŸ” ä½¿ç”¨æ–‡æœ¬æ¨¡å¼è¯†åˆ«ç›®å½•...")
            chapters = self._extract_chapters_from_text_patterns(text)

        return chapters

    def _extract_chapters_from_text_patterns(self, text: str) -> List[Dict]:
        """ä½¿ç”¨æ–‡æœ¬æ¨¡å¼è¯†åˆ«ç« èŠ‚ç»“æ„"""
        chapters = []
        text_lines = text.split('\n')
        
        # ç« èŠ‚è¯†åˆ«æ¨¡å¼
        chapter_patterns = [
            r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ç« \s*(.+)',
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€.]\s*(.+)',
            r'^(\d+)[ã€.]\s*(.+)',
            r'^(Chapter\s+\d+)\s*(.+)',
            r'^(\d+\.\d*)\s*(.+)',
        ]
        
        for line_num, line in enumerate(text_lines):
            line = line.strip()
            if not line:
                continue
                
            for pattern in chapter_patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    number = match.group(1)
                    title = match.group(2).strip()
                    
                    # è¿‡æ»¤è¿‡çŸ­çš„æ ‡é¢˜å’Œæ˜æ˜¾ä¸æ˜¯ç« èŠ‚çš„å†…å®¹
                    if len(title) > 2 and len(title) < 100:
                        chapter = {
                            'title': title,
                            'number': number,
                            'level': 1,  # å‡è®¾éƒ½æ˜¯ç¬¬ä¸€çº§
                            'section_type': self._guess_section_type(title),
                            'line_number': line_num,
                            'confidence': 0.8,
                        }
                        chapters.append(chapter)
                        print(f"   ğŸ“‹ è¯†åˆ«ç« èŠ‚: {number} {title}")
                        break
        
        return chapters
    
    def _guess_section_type(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜çŒœæµ‹ç« èŠ‚ç±»å‹"""
        title_lower = title.lower()
        
        if any(keyword in title_lower for keyword in ['ç»ªè®º', 'å¼•è¨€', 'èƒŒæ™¯', 'introduction', 'background']):
            return 'introduction'
        elif any(keyword in title_lower for keyword in ['ç»¼è¿°', 'ç°çŠ¶', 'review', 'survey']):
            return 'literature_review'
        elif any(keyword in title_lower for keyword in ['æ–¹æ³•', 'è®¾è®¡', 'method', 'design', 'approach']):
            return 'methodology'
        elif any(keyword in title_lower for keyword in ['å®éªŒ', 'experiment', 'test']):
            return 'experiment'
        elif any(keyword in title_lower for keyword in ['ç»“æœ', 'åˆ†æ', 'result', 'analysis']):
            return 'results'
        elif any(keyword in title_lower for keyword in ['ç»“è®º', 'æ€»ç»“', 'conclusion', 'summary']):
            return 'conclusion'
        else:
            return 'unknown'

    def _extract_first_level_chapters(self, chapters: List[Dict]) -> List[Dict]:
        """æå–ç¬¬ä¸€å±‚çº§ç« èŠ‚ï¼ˆä¸»è¦ç« èŠ‚ï¼‰"""
        first_level = []
        
        for chapter in chapters:
            level = chapter.get('level', 1)
            number = chapter.get('number', '')
            title = chapter.get('title', '')
            
            # ç¬¬ä¸€å±‚çº§çš„åˆ¤æ–­æ¡ä»¶ - æ›´ä¸¥æ ¼çš„ç­›é€‰
            is_first_level = False
            
            # æ¡ä»¶1ï¼šæ˜ç¡®çš„ç« çº§ç¼–å·æ¨¡å¼
            chapter_level_patterns = [
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒç« ç­‰
                r'^ç¬¬\d+ç« ',                    # ç¬¬1ç« ã€ç¬¬2ç« ç­‰
                r'^Chapter\s+\d+',             # Chapter 1, Chapter 2ç­‰
            ]
            
            if any(re.match(pattern, number, re.IGNORECASE) for pattern in chapter_level_patterns):
                is_first_level = True
                
            # æ¡ä»¶2ï¼šç‰¹æ®Šçš„ä¸»è¦ç« èŠ‚ï¼ˆç»ªè®ºã€ç»“è®ºç­‰ï¼‰
            main_chapter_titles = [
                'ç»ªè®º', 'å¼•è¨€', 'æ¦‚è¿°', 'å‰è¨€', 'Introduction',
                'ç»“è®º', 'æ€»ç»“', 'ç»“è¯­', 'Conclusion',
                'å‚è€ƒæ–‡çŒ®', 'References', 'Bibliography',
                'è‡´è°¢', 'Acknowledgment', 'Acknowledgement',
                'é™„å½•', 'Appendix',
                'ä¸ªäººç®€å†', 'ä½œè€…ç®€ä»‹', 'Biography'
            ]
            
            # å®Œå…¨åŒ¹é…æˆ–é«˜åº¦ç›¸ä¼¼çš„æ ‡é¢˜
            if any(main_title in title or title in main_title for main_title in main_chapter_titles):
                is_first_level = True
            
            # æ¡ä»¶3ï¼šlevelä¸º1ä½†æ’é™¤æ˜æ˜¾çš„å­ç« èŠ‚
            if level == 1:
                # æ’é™¤æ˜æ˜¾çš„å­ç« èŠ‚ç¼–å·
                subsection_patterns = [
                    r'^\d+\.\d+',           # 1.1, 2.3ç­‰
                    r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\.',  # ä¸€.ã€äºŒ.ç­‰ï¼ˆä½†ä¸æ˜¯ç« çº§ï¼‰
                    r'^\(\d+\)',            # (1), (2)ç­‰
                    r'^ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ï¼‰', # ï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰ç­‰
                    r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚',  # ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚ç­‰
                ]
                
                # å¦‚æœä¸åŒ¹é…å­ç« èŠ‚æ¨¡å¼ï¼Œåˆ™æ˜¯ç¬¬ä¸€å±‚çº§
                if not any(re.match(pattern, number) for pattern in subsection_patterns):
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«èŠ‚çº§å…³é”®è¯
                    if not any(keyword in title for keyword in ['ç¬¬ä¸€èŠ‚', 'ç¬¬äºŒèŠ‚', 'ç¬¬ä¸‰èŠ‚', 'ç¬¬å››èŠ‚', 'ç¬¬äº”èŠ‚', 'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰', 'ï¼ˆä¸‰ï¼‰', 'ï¼ˆå››ï¼‰', 'ï¼ˆäº”ï¼‰']):
                        is_first_level = True
                
            if is_first_level:
                first_level.append(chapter)
                print(f"   ğŸ“‹ ç¬¬ä¸€å±‚çº§ç« èŠ‚: {number} {title}")
        
        return first_level

    def _determine_chapter_content_boundaries(
        self, text: str, first_level_chapters: List[Dict]
    ) -> Dict[str, Dict]:
        """ç¡®å®šæ¯ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚çš„å†…å®¹è¾¹ç•Œ"""
        boundaries = {}
        text_lines = text.split('\n')
        total_lines = len(text_lines)
        
        for i, chapter in enumerate(first_level_chapters):
            chapter_title = chapter['title']
            print(f"   ğŸ“ ç¡®å®šç« èŠ‚è¾¹ç•Œ: {chapter_title}")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¯¹å‚è€ƒæ–‡çŒ®ç« èŠ‚ä½¿ç”¨SmartReferenceExtractorç²¾ç¡®å®šä½
            if self.smart_ref_extractor and ('å‚è€ƒæ–‡çŒ®' in chapter_title or 'references' in chapter_title.lower()):
                print(f"   ğŸ¤– ä½¿ç”¨SmartReferenceExtractorç²¾ç¡®å®šä½å‚è€ƒæ–‡çŒ®è¾¹ç•Œ")
                try:
                    # ä½¿ç”¨SmartReferenceExtractorçš„è¾¹ç•Œæ£€æµ‹
                    ref_start, ref_end = self.smart_ref_extractor._get_reference_boundaries(text)
                    if ref_start != -1 and ref_end != -1:
                        # è½¬æ¢å­—ç¬¦ä½ç½®ä¸ºè¡Œå·
                        start_line = len(text[:ref_start].split('\n')) - 1
                        end_line = len(text[:ref_end].split('\n')) - 1
                        
                        # ç¡®ä¿è¾¹ç•Œåˆç†
                        start_line = max(0, start_line)
                        end_line = min(total_lines - 1, end_line)
                        
                        if end_line >= start_line:
                            total_lines_count = end_line - start_line + 1
                            boundaries[chapter_title] = {
                                'start_line': start_line,
                                'end_line': end_line,
                                'total_lines': total_lines_count,
                                'estimated_chars': ref_end - ref_start,
                            }
                            print(f"   âœ… SmartReferenceExtractorè¾¹ç•Œ: è¡Œ {start_line}-{end_line} ({total_lines_count} è¡Œ)")
                            continue
                        else:
                            print(f"   âš ï¸ SmartReferenceExtractorè¿”å›æ— æ•ˆè¾¹ç•Œï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                    else:
                        print(f"   âš ï¸ SmartReferenceExtractoræœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
                except Exception as e:
                    print(f"   âš ï¸ SmartReferenceExtractorå‡ºé”™: {e}ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•")
            
            # ä¼ ç»Ÿè¾¹ç•Œæ£€æµ‹æ–¹æ³•ï¼ˆç”¨äºéå‚è€ƒæ–‡çŒ®ç« èŠ‚æˆ–SmartReferenceExtractorå¤±è´¥æ—¶ï¼‰
            start_line = self._find_chapter_start_line(text_lines, chapter)
            
            # æŸ¥æ‰¾ç« èŠ‚ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªä¸»ç« èŠ‚çš„å¼€å§‹æˆ–æ–‡æ¡£ç»“æŸï¼‰
            if i + 1 < len(first_level_chapters):
                next_chapter = first_level_chapters[i + 1]
                next_start = self._find_chapter_start_line(text_lines, next_chapter)
                end_line = max(start_line, next_start - 1)  # ç¡®ä¿ç»“æŸè¡Œå·ä¸å°äºå¼€å§‹è¡Œå·
            else:
                end_line = total_lines - 1
            
            # è¾¹ç•Œåˆç†æ€§æ£€æŸ¥
            if end_line < start_line:
                print(f"   âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆè¾¹ç•Œ (ç»“æŸè¡Œ{end_line} < å¼€å§‹è¡Œ{start_line})ï¼Œä½¿ç”¨æœ€å°è¾¹ç•Œ")
                end_line = min(start_line + 10, total_lines - 1)  # ç»™ä¸ªæœ€å°èŒƒå›´
            
            # å¾®è°ƒè¾¹ç•Œï¼ˆæ’é™¤æ ‡é¢˜è¡Œå’Œç©ºè¡Œï¼‰
            start_line, end_line = self._refine_chapter_boundaries(
                text_lines, start_line, end_line, chapter_title
            )
            
            # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
            if end_line < start_line:
                end_line = start_line  # æœ€åçš„ä¿æŠ¤æªæ–½
            
            total_lines_count = max(0, end_line - start_line + 1)
            
            boundaries[chapter_title] = {
                'start_line': start_line,
                'end_line': end_line,
                'total_lines': total_lines_count,
                'estimated_chars': len('\n'.join(text_lines[start_line:end_line+1])) if total_lines_count > 0 else 0,
            }
            
            print(f"   âœ… è¾¹ç•Œç¡®å®š: è¡Œ {start_line}-{end_line} ({total_lines_count} è¡Œ)")
        
        return boundaries

    def _extract_chapter_contents_by_boundaries(
        self, text: str, chapter_boundaries: Dict[str, Dict]
    ) -> Dict[str, str]:
        """æ ¹æ®è¾¹ç•Œæå–æ¯ä¸ªç« èŠ‚çš„æ­£æ–‡å†…å®¹"""
        chapter_contents = {}
        text_lines = text.split('\n')
        
        for chapter_title, boundary in chapter_boundaries.items():
            start_line = boundary['start_line']
            end_line = boundary['end_line']
            
            # æå–å†…å®¹
            content_lines = text_lines[start_line:end_line+1]
            content = '\n'.join(content_lines).strip()
            
            # æ¸…ç†å†…å®¹ï¼ˆç§»é™¤è¿‡å¤šç©ºè¡Œã€æ ¼å¼åŒ–ç­‰ï¼‰
            content = self._clean_chapter_content(content)
            
            chapter_contents[chapter_title] = content
            print(f"   ğŸ“„ æå–ç« èŠ‚å†…å®¹: {chapter_title} ({len(content)} å­—ç¬¦)")
        
        return chapter_contents

    def _find_chapter_start_line(self, text_lines: List[str], chapter: Dict) -> int:
        """æŸ¥æ‰¾ç« èŠ‚åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹è¡Œå·"""
        chapter_title = chapter['title']
        chapter_number = chapter.get('number', '')
        
        # æ„å»ºæœç´¢æ¨¡å¼ï¼Œæ›´åŠ ç²¾ç¡®çš„åŒ¹é…
        search_patterns = []
        
        # å®Œæ•´æ ‡é¢˜åŒ¹é…
        if chapter_title:
            search_patterns.append(chapter_title.strip())
        
        # ç¼–å·+æ ‡é¢˜åŒ¹é…
        if chapter_number and chapter_title:
            search_patterns.extend([
                f"{chapter_number} {chapter_title}",
                f"{chapter_number}{chapter_title}",
                f"{chapter_number.strip()} {chapter_title.strip()}",
            ])
        
        # ç‰¹æ®Šå¤„ç†ä¸€äº›ç« èŠ‚æ ‡é¢˜
        title_lower = chapter_title.lower()
        if 'å‚è€ƒæ–‡çŒ®' in chapter_title:
            search_patterns.extend(['å‚è€ƒæ–‡çŒ®', 'References', 'Bibliography'])
        elif 'è‡´è°¢' in chapter_title and 'å£°æ˜' not in chapter_title:
            # ä¸“é—¨å¤„ç†"è‡´è°¢"ï¼Œé¿å…åŒ¹é…åˆ°"è‡´è°¢ä¸å£°æ˜"
            search_patterns.extend(['è‡´è°¢', 'Acknowledgment', 'Acknowledgement'])
            # ç§»é™¤åŸå§‹æ ‡é¢˜ï¼Œé¿å…è¯¯åŒ¹é…
            if chapter_title in search_patterns:
                search_patterns.remove(chapter_title)
        elif 'ä¸ªäººç®€å†' in chapter_title:
            search_patterns.extend(['ä¸ªäººç®€å†', 'ä½œè€…ç®€ä»‹', 'Biography', 'Author Profile'])
        elif 'ç»“è¯­' in chapter_title:
            search_patterns.extend(['ç»“è¯­', 'ç»“è®º', 'Conclusion'])
        
        # åœ¨æ–‡æœ¬ä¸­æœç´¢
        for line_num, line in enumerate(text_lines):
            line_clean = line.strip()
            if not line_clean:
                continue
                
            # ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«åŒ¹é…
            for pattern in search_patterns:
                if not pattern:
                    continue
                    
                # å®Œå…¨åŒ¹é…
                if line_clean == pattern:
                    print(f"     ğŸ¯ ç²¾ç¡®åŒ¹é…æ‰¾åˆ° '{pattern}' åœ¨ç¬¬ {line_num} è¡Œ")
                    return line_num
                    
                # åŒ…å«åŒ¹é…ï¼ˆä½†è¦é¿å…è¯¯åŒ¹é…ï¼‰
                if pattern in line_clean:
                    # ç‰¹æ®Šå¤„ç†"è‡´è°¢"ï¼Œä¸èƒ½åŒ¹é…åˆ°"è‡´è°¢ä¸å£°æ˜"
                    if pattern == 'è‡´è°¢' and 'å£°æ˜' in line_clean:
                        continue
                    
                    # å¯¹äºçŸ­æ¨¡å¼ï¼Œè¦æ±‚æ›´ä¸¥æ ¼çš„åŒ¹é…
                    if len(pattern) <= 10 and len(line_clean) <= len(pattern) + 20:
                        print(f"     ğŸ¯ åŒ…å«åŒ¹é…æ‰¾åˆ° '{pattern}' åœ¨ç¬¬ {line_num} è¡Œ: {line_clean}")
                        return line_num
                    elif len(pattern) > 10:  # é•¿æ ‡é¢˜ï¼ŒåŒ¹é…æ¡ä»¶æ”¾æ¾
                        print(f"     ğŸ¯ é•¿æ ‡é¢˜åŒ¹é…æ‰¾åˆ° '{pattern}' åœ¨ç¬¬ {line_num} è¡Œ")
                        return line_num
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"   âš ï¸ æœªæ‰¾åˆ°ç« èŠ‚ '{chapter_title}' çš„èµ·å§‹ä½ç½®ï¼Œæœç´¢æ¨¡å¼: {search_patterns}")
        return 0

    def _refine_chapter_boundaries(
        self, text_lines: List[str], start_line: int, end_line: int, chapter_title: str
    ) -> Tuple[int, int]:
        """å¾®è°ƒç« èŠ‚è¾¹ç•Œï¼Œæ’é™¤æ ‡é¢˜å’Œç©ºè¡Œ"""
        # è¾¹ç•Œæ£€æŸ¥
        if start_line > end_line:
            return start_line, start_line
        
        total_lines = len(text_lines)
        start_line = max(0, min(start_line, total_lines - 1))
        end_line = max(start_line, min(end_line, total_lines - 1))
        
        # è·³è¿‡æ ‡é¢˜è¡Œ
        refined_start = start_line
        while refined_start <= end_line and refined_start < total_lines:
            line = text_lines[refined_start].strip()
            # è·³è¿‡åŒ…å«ç« èŠ‚æ ‡é¢˜çš„è¡Œ
            if line and not any(keyword in line for keyword in [chapter_title] if chapter_title):
                break
            refined_start += 1
        
        # å‘å‰è·³è¿‡ç©ºè¡Œ
        while refined_start <= end_line and refined_start < total_lines:
            if text_lines[refined_start].strip():
                break
            refined_start += 1
        
        # å‘åå»é™¤ç©ºè¡Œ
        refined_end = end_line
        while refined_end > refined_start and refined_end >= 0:
            if text_lines[refined_end].strip():
                break
            refined_end -= 1
        
        # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
        if refined_end < refined_start:
            refined_end = refined_start
        
        return refined_start, refined_end

    def _clean_chapter_content(self, content: str) -> str:
        """æ¸…ç†ç« èŠ‚å†…å®¹"""
        # ç§»é™¤è¿‡å¤šçš„ç©ºè¡Œ
        lines = content.split('\n')
        cleaned_lines = []
        prev_empty = False
        
        for line in lines:
            if line.strip():
                cleaned_lines.append(line)
                prev_empty = False
            elif not prev_empty:
                cleaned_lines.append('')
                prev_empty = True
        
        return '\n'.join(cleaned_lines).strip()

    def _analyze_chapters_with_content(
        self, chapters: List[Dict], chapter_contents: Dict[str, str], analysis_type: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ç« èŠ‚å†…å®¹è¿›è¡Œä¸“é—¨çš„AIåˆ†æ"""
        results = {'summaries': {}, 'analysis': {}}
        
        for chapter in chapters:
            chapter_title = chapter['title']
            content = chapter_contents.get(chapter_title, '')
            
            if not content or len(content) < 100:
                continue
                
            print(f"   ğŸ§  åˆ†æç« èŠ‚: {chapter_title} ({len(content)} å­—ç¬¦)")
            
            # æ ¹æ®åˆ†æç±»å‹ç”Ÿæˆä¸“é—¨çš„æç¤ºè¯
            analysis_result = self._generate_specialized_chapter_analysis(
                chapter, content, analysis_type
            )
            
            if analysis_result:
                results['summaries'][chapter_title] = analysis_result.get('summary', '')
                results['analysis'][chapter_title] = analysis_result.get('detailed_analysis', {})
        
        return results

    def _generate_specialized_chapter_analysis(
        self, chapter: Dict, content: str, analysis_type: str
    ) -> Optional[Dict]:
        """ç”Ÿæˆä¸“é—¨çš„ç« èŠ‚åˆ†æ"""
        if not self.ai_client:
            return None
            
        chapter_title = chapter['title']
        
        # æ ¹æ®åˆ†æç±»å‹æ„å»ºä¸“é—¨çš„æç¤ºè¯
        if analysis_type == 'review':
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–‡çŒ®ç»¼è¿°/ç»ªè®ºç« èŠ‚ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}
ç« èŠ‚å†…å®¹ï¼š
{content[:3000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ç ”ç©¶èƒŒæ™¯å’Œç°çŠ¶
2. æ–‡çŒ®ç»¼è¿°è¦ç‚¹
3. ç ”ç©¶é—®é¢˜å’Œç›®æ ‡
4. ä¸»è¦ç†è®ºåŸºç¡€
5. æœ¬ç« èŠ‚çš„æ ¸å¿ƒè´¡çŒ®

è¿”å›JSONæ ¼å¼ï¼š{{"summary": "ç« èŠ‚æ‘˜è¦", "research_background": "ç ”ç©¶èƒŒæ™¯", "literature_points": ["è¦ç‚¹1", "è¦ç‚¹2"], "research_problems": ["é—®é¢˜1", "é—®é¢˜2"], "theoretical_foundation": "ç†è®ºåŸºç¡€", "core_contribution": "æ ¸å¿ƒè´¡çŒ®"}}
"""
        elif analysis_type == 'methodology':
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–¹æ³•è®ºç« èŠ‚ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}
ç« èŠ‚å†…å®¹ï¼š
{content[:3000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ç ”ç©¶æ–¹æ³•æ¦‚è¿°
2. æŠ€æœ¯è·¯çº¿å’Œæµç¨‹
3. å®éªŒè®¾è®¡æ–¹æ¡ˆ
4. æ•°æ®æ”¶é›†æ–¹æ³•
5. åˆ†æå·¥å…·å’ŒæŠ€æœ¯

è¿”å›JSONæ ¼å¼ï¼š{{"summary": "ç« èŠ‚æ‘˜è¦", "research_methods": ["æ–¹æ³•1", "æ–¹æ³•2"], "technical_route": "æŠ€æœ¯è·¯çº¿", "experimental_design": "å®éªŒè®¾è®¡", "data_collection": "æ•°æ®æ”¶é›†", "analysis_tools": ["å·¥å…·1", "å·¥å…·2"]}}
"""
        elif analysis_type == 'results':
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç»“æœç« èŠ‚ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}
ç« èŠ‚å†…å®¹ï¼š
{content[:3000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦ç ”ç©¶å‘ç°
2. å®éªŒç»“æœæ•°æ®
3. ç»“æœåˆ†æå’Œè§£é‡Š
4. å…³é”®æ€§èƒ½æŒ‡æ ‡
5. ç»“æœéªŒè¯æ–¹æ³•

è¿”å›JSONæ ¼å¼ï¼š{{"summary": "ç« èŠ‚æ‘˜è¦", "main_findings": ["å‘ç°1", "å‘ç°2"], "experimental_data": "å®éªŒæ•°æ®", "result_analysis": "ç»“æœåˆ†æ", "key_metrics": ["æŒ‡æ ‡1", "æŒ‡æ ‡2"], "validation_methods": "éªŒè¯æ–¹æ³•"}}
"""
        elif analysis_type == 'conclusion':
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç»“è®ºç« èŠ‚ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}
ç« èŠ‚å†…å®¹ï¼š
{content[:3000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦ç ”ç©¶ç»“è®º
2. ç†è®ºè´¡çŒ®å’Œåˆ›æ–°ç‚¹
3. å®è·µåº”ç”¨ä»·å€¼
4. ç ”ç©¶å±€é™æ€§
5. æœªæ¥ç ”ç©¶æ–¹å‘

è¿”å›JSONæ ¼å¼ï¼š{{"summary": "ç« èŠ‚æ‘˜è¦", "main_conclusions": ["ç»“è®º1", "ç»“è®º2"], "theoretical_contributions": ["è´¡çŒ®1", "è´¡çŒ®2"], "practical_value": "åº”ç”¨ä»·å€¼", "limitations": "å±€é™æ€§", "future_directions": "æœªæ¥æ–¹å‘"}}
"""
        else:
            # é€šç”¨åˆ†æ
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}
ç« èŠ‚å†…å®¹ï¼š
{content[:3000]}...

è¯·æå–ç« èŠ‚çš„æ ¸å¿ƒå†…å®¹å’Œè¦ç‚¹ã€‚
è¿”å›JSONæ ¼å¼ï¼š{{"summary": "ç« èŠ‚æ‘˜è¦", "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"], "important_content": "é‡è¦å†…å®¹"}}
"""
        
        try:
            response = self.ai_client.send_message(prompt)
            if response and response.content:
                # è§£æJSONå“åº”
                json_content = _extract_json_from_response(response.content)
                if json_content:
                    parsed_result = _parse_json_with_fallback(json_content)
                    return parsed_result
        except Exception as e:
            print(f"   âŒ ç« èŠ‚åˆ†æå¤±è´¥: {e}")
        
        return None

    def _generate_chapter_analysis_report(self, toc_analysis: Dict, first_level_chapters: List[Dict]):
        """ç”Ÿæˆç« èŠ‚åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ç¬¬ä¸€å±‚çº§ç« èŠ‚åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        print(f"ğŸ“‹ æ€»å…±åˆ†æäº† {len(first_level_chapters)} ä¸ªç¬¬ä¸€å±‚çº§ç« èŠ‚")
        
        for chapter in first_level_chapters:
            title = chapter['title']
            chapter_type = chapter.get('classified_type', 'unknown')
            
            print(f"\nğŸ“– ç« èŠ‚: {title}")
            print(f"   ç±»å‹: {chapter_type}")
            
            # æ˜¾ç¤ºåˆ†æç»“æœ
            if title in toc_analysis['chapter_summaries']:
                summary = toc_analysis['chapter_summaries'][title]
                print(f"   æ‘˜è¦: {summary[:100]}...")
            
            # æ˜¾ç¤ºå†…å®¹è¾¹ç•Œ
            if title in toc_analysis['chapter_content_boundaries']:
                boundary = toc_analysis['chapter_content_boundaries'][title]
                print(f"   è¾¹ç•Œ: ç¬¬{boundary['start_line']}-{boundary['end_line']}è¡Œ ({boundary['estimated_chars']}å­—ç¬¦)")
        
        print("\n" + "="*60)

    def _identify_review_chapters(self, chapters: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½è¯†åˆ«ç»¼è¿°æ€§ç« èŠ‚ - æ”¯æŒæ™ºèƒ½ç›®å½•æå–ç»“æœ"""
        review_chapters = []
        
        if not chapters:
            return review_chapters
        
        # ç»¼è¿°æ€§ç« èŠ‚çš„å…³é”®è¯æ ‡è¯† - æ‰©å±•ç‰ˆ
        review_keywords = [
            'ç»ªè®º', 'å¼•è¨€', 'ç»¼è¿°', 'èƒŒæ™¯', 'ç°çŠ¶', 'æ¦‚è¿°',
            'introduction', 'review', 'background', 'survey', 'overview',
            'ç›¸å…³å·¥ä½œ', 'æ–‡çŒ®ç»¼è¿°', 'ç ”ç©¶ç°çŠ¶', 'å›½å†…å¤–ç ”ç©¶', 'ç†è®ºåŸºç¡€',
            'related work', 'literature review', 'state of art', 'theoretical foundation'
        ]
        
        for chapter in chapters:
            title = chapter.get('title', '').lower()
            number = chapter.get('number', '')
            section_type = chapter.get('section_type', '')
            level = chapter.get('level', 1)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç»¼è¿°æ€§ç« èŠ‚
            is_review = False
            
            # 1. åŸºäºç« èŠ‚ç±»å‹ï¼ˆæ¥è‡ªæ™ºèƒ½ç›®å½•æå–ï¼‰
            if section_type in ['introduction', 'literature_review', 'background', 'overview']:
                is_review = True
                print(f"   ğŸ“‹ åŸºäºç±»å‹è¯†åˆ«ç»¼è¿°ç« èŠ‚: {title} (ç±»å‹: {section_type})")
            
            # 2. ç¬¬ä¸€ç« é€šå¸¸æ˜¯ç»ªè®º/ç»¼è¿°ï¼ˆå¦‚æœæ˜¯1çº§æ ‡é¢˜ï¼‰
            elif level == 1 and number in ['ä¸€', '1', '1.', 'I', 'ç¬¬1ç« ', 'ç¬¬ä¸€ç« ']:
                is_review = True
                print(f"   ğŸ“‹ åŸºäºç¼–å·è¯†åˆ«ç»¼è¿°ç« èŠ‚: {title} (ç¼–å·: {number})")
            
            # 3. æ ‡é¢˜åŒ…å«ç»¼è¿°å…³é”®è¯
            elif any(keyword in title for keyword in review_keywords):
                is_review = True
                matched_keyword = next(keyword for keyword in review_keywords if keyword in title)
                print(f"   ğŸ“‹ åŸºäºå…³é”®è¯è¯†åˆ«ç»¼è¿°ç« èŠ‚: {title} (å…³é”®è¯: {matched_keyword})")
            
            # 4. æ’é™¤æ˜æ˜¾çš„å®éªŒ/æ–¹æ³•ç« èŠ‚
            non_review_keywords = [
                'å®éªŒ', 'æ–¹æ³•', 'è®¾è®¡', 'å®ç°', 'æµ‹è¯•', 'ç»“æœ', 'åˆ†æ', 'ç³»ç»Ÿ',
                'experiment', 'method', 'implementation', 'test', 'result', 'analysis', 'system'
            ]
            if any(keyword in title for keyword in non_review_keywords):
                is_review = False
                print(f"   âŒ æ’é™¤éç»¼è¿°ç« èŠ‚: {title}")
            
            if is_review:
                # æ·»åŠ ç« èŠ‚ç±»å‹ä¿¡æ¯
                chapter_copy = chapter.copy()
                chapter_copy['identified_type'] = 'literature_review'
                review_chapters.append(chapter_copy)
        
        print(f"   ï¿½ æ€»å…±è¯†åˆ«åˆ° {len(review_chapters)} ä¸ªç»¼è¿°æ€§ç« èŠ‚")
        return review_chapters

    def _convert_toc_entries_to_chapters(self, toc_entries) -> List[Dict[str, str]]:
        """å°†æ™ºèƒ½æå–çš„TOCæ¡ç›®è½¬æ¢ä¸ºç« èŠ‚æ ¼å¼"""
        chapters = []

        for entry in toc_entries:
            # æ„å»ºç« èŠ‚å­—å…¸
            chapter = {
                'title': entry.title,
                'number': entry.number,
                'level': entry.level,
                'section_type': entry.section_type,
                'line_number': entry.line_number,
                'confidence': entry.confidence,
            }

            # å¦‚æœæœ‰é¡µç ä¿¡æ¯ï¼Œä¹ŸåŒ…å«è¿›æ¥
            if hasattr(entry, 'page') and entry.page:
                chapter['page'] = entry.page

            chapters.append(chapter)

        return chapters

    def _extract_chapters_from_word(
        self, file_path: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """ç›´æ¥ä»Wordæ–‡æ¡£æå–ç« èŠ‚ç»“æ„"""
        try:
            if not file_path:
                return []

            import docx
            from pathlib import Path

            # æ£€æŸ¥æ˜¯å¦ä¸ºWordæ–‡æ¡£
            if not file_path.endswith('.docx'):
                return []

            # è¯»å–Wordæ–‡æ¡£
            doc = docx.Document(file_path)
            chapters = []

            # æ›´æ–°ç« èŠ‚æ¨¡å¼ï¼Œæ”¯æŒå¤šç§æ ¼å¼
            chapter_patterns = [
                # ä¼ ç»Ÿæ ¼å¼
                (r'^ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ç« \s+(.+)$', 'traditional'),
                # æ•°å­—ç¼–å·æ ¼å¼
                (r'^(\d+)\.\s*(.{10,100})$', 'numbered'),
                # ç‹¬ç«‹é‡è¦ç« èŠ‚
                (r'^(æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|å›½å†…å¤–ç ”ç©¶ç°çŠ¶)$', 'literature'),
                (r'^(ç»ªè®º|å¼•è¨€|å‰è¨€)$', 'introduction'),
                (r'^(ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›|å…¨æ–‡å°ç»“)$', 'conclusion'),
            ]

            for para in doc.paragraphs:
                text = para.text.strip()
                if text:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç²—ä½“ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
                    is_bold = False
                    try:
                        if para.runs and para.runs[0].bold:
                            is_bold = True
                    except:
                        pass

                    for pattern, pattern_type in chapter_patterns:
                        match = re.match(pattern, text)
                        if match and is_bold:  # åªæœ‰ç²—ä½“æ–‡æœ¬æ‰è€ƒè™‘ä¸ºç« èŠ‚æ ‡é¢˜
                            if pattern_type == 'traditional':
                                chapter_num = match.group(1)
                                chapter_title = match.group(2).strip()
                                chapters.append(
                                    {
                                        'number': chapter_num,
                                        'title': chapter_title,
                                        'full_title': f'ç¬¬{chapter_num}ç«  {chapter_title}',
                                    }
                                )
                            elif pattern_type == 'numbered':
                                chapter_num = match.group(1)
                                chapter_title = match.group(2).strip()
                                # éªŒè¯ç« èŠ‚æ ‡é¢˜çš„åˆç†æ€§ï¼ˆä¸æ˜¯å›¾è¡¨æ ‡é¢˜ï¼‰
                                if not re.match(r'^(å›¾|è¡¨|Fig|Table)', chapter_title):
                                    chapters.append(
                                        {
                                            'number': chapter_num,
                                            'title': chapter_title,
                                            'full_title': f'{chapter_num}. {chapter_title}',
                                        }
                                    )
                            else:  # ç‹¬ç«‹ç« èŠ‚
                                chapters.append(
                                    {
                                        'number': str(len(chapters) + 1),
                                        'title': text,
                                        'full_title': text,
                                    }
                                )
                            break

            return chapters

        except Exception as e:
            print(f"   âš ï¸ ä»Wordæ–‡æ¡£æå–ç« èŠ‚å¤±è´¥: {str(e)}")
            logger.error(f"ä»Wordæ–‡æ¡£æå–ç« èŠ‚å¤±è´¥: {str(e)}", exc_info=True)
            return []

    def _extract_chapter_content(self, text: str, chapter: Dict[str, str]) -> str:
        """æå–æŒ‡å®šç« èŠ‚çš„å†…å®¹ - æ”¯æŒæ™ºèƒ½ç›®å½•æå–ç»“æœ"""
        try:
            chapter_title = chapter.get('title', '')
            chapter_num = chapter.get('number', '')
            chapter_level = chapter.get('level', 1)

            if not chapter_title:
                return ""

            # å¤šç§ç« èŠ‚å®šä½æ¨¡å¼ - ä¼˜åŒ–ä¸ºæ™ºèƒ½ç›®å½•æå–ç»“æœ
            search_patterns = []

            # å¦‚æœæœ‰ç« èŠ‚ç¼–å·ï¼Œæ„å»ºå¸¦ç¼–å·çš„æ¨¡å¼
            if chapter_num:
                search_patterns.extend(
                    [
                        rf"ç¬¬{chapter_num}ç« \s*{re.escape(chapter_title)}",
                        rf"Chapter\s+{chapter_num}\s*[ï¼š:]\s*{re.escape(chapter_title)}",
                        rf"^{re.escape(chapter_num)}\s*{re.escape(chapter_title)}",
                        rf"^{re.escape(chapter_num)}\.\s*{re.escape(chapter_title)}",
                        rf"^{re.escape(chapter_num)}[\s\.ã€]\s*{re.escape(chapter_title)}",
                    ]
                )

            # æ ‡é¢˜åŒ¹é…æ¨¡å¼
            search_patterns.extend(
                [
                    rf"^{re.escape(chapter_title)}\s*$",  # ç²¾ç¡®æ ‡é¢˜åŒ¹é…
                    rf"#{{{chapter_level},3}}\s*{re.escape(chapter_title)}",  # Markdownæ ¼å¼
                    rf"^\s*{re.escape(chapter_title)}\s*[ï¼š:]?\s*$",  # å¸¦å¯é€‰å†’å·
                    chapter_title,  # ç®€å•åŒ…å«åŒ¹é…
                ]
            )

            start_pos = None
            matched_pattern = None

            # æŒ‰ä¼˜å…ˆçº§å°è¯•åŒ¹é…
            for pattern in search_patterns:
                try:
                    match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
                    if match:
                        start_pos = match.start()
                        matched_pattern = pattern
                        break
                except re.error:
                    continue

            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            if start_pos is None:
                # ç§»é™¤ç‰¹æ®Šå­—ç¬¦è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
                clean_title = re.sub(r'[^\w\u4e00-\u9fff]', '', chapter_title)
                if len(clean_title) > 3:
                    # ä½¿ç”¨æ›´å®½æ¾çš„æ¨¡ç³ŠåŒ¹é…
                    words = list(clean_title)
                    if len(words) > 3:
                        # å–å‰å‡ ä¸ªå…³é”®å­—ç¬¦è¿›è¡ŒåŒ¹é…
                        key_chars = ''.join(words[: min(5, len(words))])
                        fuzzy_pattern = (
                            f"[^\\w\\u4e00-\\u9fff]*{key_chars}[^\\w\\u4e00-\\u9fff]*"
                        )
                        try:
                            match = re.search(fuzzy_pattern, text, re.IGNORECASE)
                            if match:
                                start_pos = match.start()
                                matched_pattern = f"fuzzy: {fuzzy_pattern}"
                        except re.error:
                            pass

            if start_pos is None:
                print(f"   âš ï¸ æœªæ‰¾åˆ°ç« èŠ‚å†…å®¹: {chapter_title}")
                return ""

            print(f"   âœ… æ‰¾åˆ°ç« èŠ‚ '{chapter_title}' èµ·å§‹ä½ç½®: {start_pos}")

            # æŸ¥æ‰¾ä¸‹ä¸€ç« èŠ‚æˆ–æ–‡æ¡£ç»“æŸä½ç½®
            next_chapter_patterns = [
                r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s*[^\n\r]{3,80}',
                r'Chapter\s+\d+\s*[ï¼š:]\s*[^\n\r]{3,80}',
                r'^\d+[\.\s]+[^\n\r]{5,60}',
                r'^#{1,3}\s*[^\n\r]{5,60}',
                r'^(å‚è€ƒæ–‡çŒ®|REFERENCES|è‡´è°¢|ACKNOWLEDGMENT|é™„å½•|APPENDIX|ç»“è®º|CONCLUSION|æ€»ç»“)',
            ]

            end_pos = len(text)
            search_start = start_pos + len(chapter_title) + 20  # ä»ç« èŠ‚æ ‡é¢˜åå¼€å§‹æŸ¥æ‰¾

            for pattern in next_chapter_patterns:
                try:
                    match = re.search(
                        pattern, text[search_start:], re.MULTILINE | re.IGNORECASE
                    )
                    if match:
                        end_pos = search_start + match.start()
                        break
                except re.error:
                    continue

            # æå–ç« èŠ‚å†…å®¹
            chapter_content = text[start_pos:end_pos].strip()

            # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
            if len(chapter_content) < 100:
                print(f"   âš ï¸ ç« èŠ‚å†…å®¹è¿‡çŸ­: {len(chapter_content)} å­—ç¬¦")
                return ""

            print(f"   ğŸ“– æå–ç« èŠ‚å†…å®¹: {len(chapter_content)} å­—ç¬¦")
            return chapter_content

        except Exception as e:
            print(f"   âš ï¸ æå–ç« èŠ‚å†…å®¹å¤±è´¥: {str(e)}")
            logger.error(f"æå–ç« èŠ‚å†…å®¹å¤±è´¥: {str(e)}", exc_info=True)
            return ""

    def _generate_chapter_summary_with_ai(
        self, chapter: Dict[str, str], content: str
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIç”Ÿæˆç« èŠ‚æ‘˜è¦"""
        if not self.ai_client:
            return {'summary': 'æœªæå–', 'key_points': []}

        try:
            # è·å–ç« èŠ‚æ ‡é¢˜ï¼Œæ”¯æŒå¤šç§å­—æ®µå
            chapter_title = chapter.get(
                'title', chapter.get('full_title', chapter.get('name', 'æœªçŸ¥ç« èŠ‚'))
            )
            chapter_number = chapter.get('number', '')
            full_title = f"{chapter_number} {chapter_title}".strip()

            prompt = f"""
è¯·å¯¹ä»¥ä¸‹è®ºæ–‡ç« èŠ‚è¿›è¡Œæ™ºèƒ½åˆ†æå’Œæ‘˜è¦ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{full_title}
ç« èŠ‚å±‚çº§ï¼šç¬¬{chapter.get('level', 1)}çº§
ç« èŠ‚å†…å®¹ï¼š{content[:2000]}...

è¯·æä¾›ï¼š
1. è¯¥ç« èŠ‚çš„æ ¸å¿ƒå†…å®¹æ‘˜è¦ï¼ˆ100-200å­—ï¼‰
2. ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼ˆ3-5ä¸ªè¦ç‚¹ï¼‰
3. å¦‚æœæ˜¯æ–¹æ³•ç« èŠ‚ï¼Œè¯·è¯†åˆ«ï¼šç ”ç©¶æ–¹æ³•ã€å®éªŒè®¾è®¡ã€å‚æ•°è®¾ç½®
4. å¦‚æœæ˜¯ç»“æœç« èŠ‚ï¼Œè¯·è¯†åˆ«ï¼šå®éªŒç»“æœã€æ•°æ®åˆ†æã€æ€§èƒ½æŒ‡æ ‡
5. å¦‚æœæ˜¯æ–‡çŒ®ç»¼è¿°ç« èŠ‚ï¼Œè¯·è¯†åˆ«ï¼šç ”ç©¶ç°çŠ¶ã€å‘å±•è¶‹åŠ¿ã€ç ”ç©¶ç©ºç™½

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "summary": "ç« èŠ‚æ‘˜è¦",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
    "methods": ["æ–¹æ³•1", "æ–¹æ³•2"],
    "results": ["ç»“æœ1", "ç»“æœ2"],
    "parameters": ["å‚æ•°1", "å‚æ•°2"],
    "research_trends": ["è¶‹åŠ¿1", "è¶‹åŠ¿2"],
    "chapter_type": "æ–¹æ³•/ç»“æœ/ç»¼è¿°/ç†è®º/å®éªŒ"
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                # å°è¯•è§£æJSONå“åº”
                try:
                    result = json.loads(response.content)
                    return result
                except:
                    return {'summary': response.content[:200], 'key_points': []}
        except Exception as e:
            print(f"   âš ï¸ AIç« èŠ‚åˆ†æå¤±è´¥: {e}")
            logger.error(f"AIç« èŠ‚åˆ†æå¤±è´¥: {e}", exc_info=True)

        return {'summary': 'æœªæå–', 'key_points': []}

    def _analyze_literature_section_with_ai(self, text: str) -> Dict[str, Any]:
        """æ·±åº¦åˆ†ææ–‡çŒ®ç»¼è¿°ç« èŠ‚ - ä¸“ä¸šç»¼è¿°åˆ†æ"""
        if not self.ai_client:
            return {
                'researchers_views': [],
                'existing_problems': [],
                'research_gaps': [],
            }

        try:
            # 1. æ™ºèƒ½æå–ç»¼è¿°æ€§å†…å®¹
            literature_content = self._extract_comprehensive_literature_content(text)

            if not literature_content or len(literature_content) < 200:
                return {
                    'researchers_views': [],
                    'existing_problems': [],
                    'research_gaps': [],
                }

            # 2. ä¸“ä¸šç»¼è¿°åˆ†ææç¤º
            prompt = f"""
ä½œä¸ºå­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è®ºæ–‡çš„ç»¼è¿°å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æã€‚æŒ‰ç…§å­¦æœ¯ç»¼è¿°çš„æ ‡å‡†è¦æ±‚ï¼Œé‡ç‚¹åˆ†æï¼š

**åˆ†æå†…å®¹ï¼š**
{literature_content[:6000]}

**åˆ†æè¦æ±‚ï¼š**
è¯·æŒ‰ç…§ç»¼è¿°çš„å­¦æœ¯æ ‡å‡†ï¼Œç³»ç»Ÿæ€§åˆ†æä»¥ä¸‹æ–¹é¢ï¼š

1. **ç ”ç©¶è€…è§‚ç‚¹æ¢³ç†**ï¼šè¯†åˆ«ä¸»è¦ç ”ç©¶è€…åŠå…¶æ ¸å¿ƒè§‚ç‚¹
2. **ç ”ç©¶è„‰ç»œåˆ†æ**ï¼šæ¢³ç†è¯¥é¢†åŸŸçš„å‘å±•è„‰ç»œå’Œæ¼”è¿›è¿‡ç¨‹
3. **æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼šåˆ†æç°æœ‰è§‚ç‚¹çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œä¸è¶³
4. **é—®é¢˜è¯†åˆ«**ï¼šæ‰¾å‡ºå½“å‰ç ”ç©¶ä¸­å­˜åœ¨çš„å…³é”®é—®é¢˜
5. **ç ”ç©¶ä»·å€¼è¯„ä¼°**ï¼šè¯„ä¼°å·²è¯†åˆ«é—®é¢˜çš„ç ”ç©¶ä»·å€¼å’Œæ„ä¹‰
6. **å‘å±•è¶‹åŠ¿**ï¼šé¢„æµ‹æœªæ¥çš„ç ”ç©¶æ–¹å‘å’Œå‘å±•è¶‹åŠ¿

**è¾“å‡ºæ ¼å¼ï¼š**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
    "researchers_views": [
        {{
            "researcher": "ç ”ç©¶è€…å§“åæˆ–æœºæ„",
            "main_viewpoint": "ä¸»è¦å­¦æœ¯è§‚ç‚¹",
            "contribution": "ä¸»è¦è´¡çŒ®",
            "influence": "å­¦æœ¯å½±å“åŠ›è¯„ä¼°"
        }}
    ],
    "research_timeline": [
        {{
            "period": "æ—¶é—´é˜¶æ®µ",
            "key_developments": "å…³é”®å‘å±•",
            "milestone_achievements": "é‡Œç¨‹ç¢‘æˆå°±"
        }}
    ],
    "critical_analysis": [
        {{
            "existing_approach": "ç°æœ‰æ–¹æ³•æˆ–è§‚ç‚¹",
            "strengths": "ä¼˜åŠ¿åˆ†æ",
            "limitations": "å±€é™æ€§åˆ†æ",
            "critical_thinking": "æ‰¹åˆ¤æ€§æ€è€ƒ"
        }}
    ],
    "identified_problems": [
        {{
            "problem_description": "é—®é¢˜æè¿°",
            "problem_severity": "é—®é¢˜ä¸¥é‡ç¨‹åº¦(é«˜/ä¸­/ä½)",
            "research_significance": "ç ”ç©¶æ„ä¹‰",
            "potential_impact": "è§£å†³åçš„æ½œåœ¨å½±å“"
        }}
    ],
    "research_gaps": [
        {{
            "gap_area": "ç ”ç©¶ç©ºç™½é¢†åŸŸ",
            "gap_description": "è¯¦ç»†æè¿°",
            "research_opportunity": "ç ”ç©¶æœºä¼š",
            "feasibility": "ç ”ç©¶å¯è¡Œæ€§è¯„ä¼°"
        }}
    ],
    "future_directions": [
        {{
            "direction": "æœªæ¥ç ”ç©¶æ–¹å‘",
            "rationale": "æ–¹å‘åˆç†æ€§",
            "expected_breakthrough": "é¢„æœŸçªç ´"
        }}
    ]
}}
"""

            response = self.ai_client.send_message(prompt)

            if response and hasattr(response, 'content'):
                # å°è¯•æå–å’Œè§£æJSON
                json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
                if json_match:
                    try:
                        analysis_result = json.loads(json_match.group())
                        return analysis_result
                    except json.JSONDecodeError:
                        print(f"   ğŸ“ JSONè§£æå¤±è´¥ï¼")
                        logger.error(f"æ–‡çŒ®ç»¼è¿°AIåˆ†æJSONè§£æå¤±è´¥ ", exc_info=True)
                        return self._get_empty_literature_analysis()
                else:
                    print(f"   ğŸ“ æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œä½¿ç”¨æ–‡çŒ®åˆ†æå¤‡ç”¨è§£ææ–¹æ³•")
                    logger.error(f"æ–‡çŒ®ç»¼è¿°AIåˆ†ææœªæ‰¾åˆ°JSONæ ¼å¼  ", exc_info=True)
                    return self._get_empty_literature_analysis()

            return self._get_empty_literature_analysis()

        except Exception as e:
            print(f"   âš ï¸ ç»¼è¿°AIåˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"æ–‡çŒ®ç»¼è¿°AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            return self._get_empty_literature_analysis()

    def _extract_comprehensive_literature_content(self, text: str) -> str:
        """æ™ºèƒ½æå–ç»¼è¿°æ€§å†…å®¹"""
        # å¤šå±‚çº§æœç´¢ç»¼è¿°å†…å®¹
        search_patterns = [
            # 1. æ˜ç¡®çš„ç»¼è¿°ç« èŠ‚
            r'ç¬¬ä¸€ç« \s*ç»ªè®º.*?(?=ç¬¬äºŒç« |ç¬¬\d+ç« |$)',
            r'ç»ªè®º.*?(?=ç¬¬äºŒç« |ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|$)',
            r'æ–‡çŒ®ç»¼è¿°.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|å®éªŒæ–¹æ³•|$)',
            r'ç›¸å…³å·¥ä½œ.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|å®éªŒæ–¹æ³•|$)',
            r'å›½å†…å¤–ç ”ç©¶ç°çŠ¶.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|å®éªŒæ–¹æ³•|$)',
            # 2. ç ”ç©¶èƒŒæ™¯å’Œç°çŠ¶
            r'ç ”ç©¶èƒŒæ™¯.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|ç ”ç©¶æ–¹æ³•|$)',
            r'ç ”ç©¶ç°çŠ¶.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|ç ”ç©¶æ–¹æ³•|$)',
            r'æŠ€æœ¯ç°çŠ¶.*?(?=ç¬¬\d+ç« |å‚è€ƒæ–‡çŒ®|ç ”ç©¶æ–¹æ³•|$)',
        ]

        extracted_content = ""
        for pattern in search_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(0)
                if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿä¸°å¯Œ
                    extracted_content = content
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨ç« èŠ‚ï¼Œæå–å‰ç½®ç»¼è¿°æ€§å†…å®¹
        if not extracted_content:
            lines = text.split('\n')
            literature_lines = []
            for line in lines[:200]:  # æ£€æŸ¥å‰200è¡Œ
                if any(
                    keyword in line.lower()
                    for keyword in [
                        'ç ”ç©¶',
                        'å‘å±•',
                        'æŠ€æœ¯',
                        'æ–¹æ³•',
                        'ææ–™',
                        'ç°çŠ¶',
                        'é—®é¢˜',
                        'æŒ‘æˆ˜',
                    ]
                ):
                    literature_lines.append(line)
                if len('\n'.join(literature_lines)) > 3000:
                    break
            extracted_content = '\n'.join(literature_lines)

        # å†…å®¹é•¿åº¦æ§åˆ¶
        if len(extracted_content) > 8000:
            extracted_content = extracted_content[:8000] + "..."

        return extracted_content

    def _get_empty_literature_analysis(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„æ–‡çŒ®åˆ†æç»“æ„"""
        return {
            'researchers_views': [],
            'research_timeline': [],
            'critical_analysis': [],
            'identified_problems': [],
            'research_gaps': [],
            'future_directions': [],
        }


    def _classify_chapter_type(self, chapter: Dict[str, str], content: str = "") -> str:
        """æ™ºèƒ½åˆ†ç±»ç« èŠ‚ç±»å‹"""
        title = chapter.get('title', '').lower()
        section_type = chapter.get('section_type', '')
        number = chapter.get('number', '')

        # åŸºäºç°æœ‰ç±»å‹ï¼ˆæ¥è‡ªæ™ºèƒ½ç›®å½•æå–ï¼‰
        if section_type:
            type_mapping = {
                'introduction': 'literature_review',
                'literature_review': 'literature_review',
                'background': 'literature_review',
                'methodology': 'methodology',
                'methods': 'methodology',
                'results': 'results',
                'experiments': 'results',
                'conclusion': 'conclusion',
                'conclusions': 'conclusion',
                'references': 'references',
                'acknowledgment': 'acknowledgment',
                'acknowledgments': 'acknowledgment',
                'achievements': 'achievements',
            }
            if section_type in type_mapping:
                return type_mapping[section_type]

        # åŸºäºæ ‡é¢˜å…³é”®è¯
        if any(
            keyword in title
            for keyword in [
                'ç»ªè®º',
                'å¼•è¨€',
                'introduction',
                'ç»¼è¿°',
                'ç°çŠ¶',
                'èƒŒæ™¯',
                'æ¦‚è¿°',
            ]
        ):
            return 'literature_review'
        elif any(
            keyword in title
            for keyword in [
                'æ–¹æ³•',
                'method',
                'ç®—æ³•',
                'algorithm',
                'æ¨¡å‹',
                'model',
                'è®¾è®¡',
            ]
        ):
            return 'methodology'
        elif any(
            keyword in title
            for keyword in [
                'å®éªŒ',
                'experiment',
                'ç»“æœ',
                'result',
                'åˆ†æ',
                'analysis',
                'è¯„ä¼°',
            ]
        ):
            return 'results'
        elif any(
            keyword in title
            for keyword in ['ç»“è®º', 'conclusion', 'æ€»ç»“', 'summary', 'å±•æœ›']
        ):
            return 'conclusion'
        elif any(keyword in title for keyword in ['å‚è€ƒæ–‡çŒ®', 'reference', 'æ–‡çŒ®']):
            return 'references'
        elif any(keyword in title for keyword in ['è‡´è°¢', 'acknowledgment', 'è°¢è¾']):
            return 'acknowledgment'
        elif any(
            keyword in title
            for keyword in ['ç†è®º', 'theory', 'åŸç†', 'principle', 'åŸºç¡€']
        ):
            return 'theoretical'
        elif any(
            keyword in title for keyword in ['ç³»ç»Ÿ', 'system', 'å®ç°', 'implementation']
        ):
            return 'system'

        # åŸºäºå†…å®¹åˆ†æï¼ˆå¦‚æœæä¾›äº†å†…å®¹ï¼‰
        if content:
            content_lower = content[:1000].lower()  # åªåˆ†æå‰1000å­—ç¬¦
            content_indicators = {
                'results': [
                    'å®éªŒç»“æœ',
                    'æ€§èƒ½è¯„ä¼°',
                    'å¯¹æ¯”åˆ†æ',
                    'æ•°æ®æ˜¾ç¤º',
                    'æµ‹è¯•ç»“æœ',
                    'è¯„ä»·æŒ‡æ ‡',
                ],
                'methodology': [
                    'æå‡ºæ–¹æ³•',
                    'ç®—æ³•æµç¨‹',
                    'æ¨¡å‹æ¶æ„',
                    'å®ç°æ­¥éª¤',
                    'è®¾è®¡æ€è·¯',
                    'æŠ€æœ¯è·¯çº¿',
                ],
                'literature_review': [
                    'ç ”ç©¶ç°çŠ¶',
                    'æ–‡çŒ®åˆ†æ',
                    'ç›¸å…³ç ”ç©¶',
                    'å‘å±•å†ç¨‹',
                    'ç ”ç©¶è¶‹åŠ¿',
                ],
            }

            for ctype, indicators in content_indicators.items():
                if any(indicator in content_lower for indicator in indicators):
                    return ctype

        return 'general'

    def _generate_review_chapter_analysis(
        self, chapter: Dict[str, str], content: str
    ) -> Dict[str, Any]:
        """ä¸ºç»¼è¿°æ€§ç« èŠ‚ç”Ÿæˆä¸“é—¨çš„åˆ†æ"""
        if not self.ai_client:
            return {'summary': 'æ— AIå®¢æˆ·ç«¯', 'key_points': []}

        try:
            prompt = f"""
è¯·å¯¹ä»¥ä¸‹ç»¼è¿°æ€§ç« èŠ‚è¿›è¡Œä¸“ä¸šçš„å­¦æœ¯åˆ†æï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter['title']}
ç« èŠ‚å†…å®¹ï¼š
{content[:5000]}

ä½œä¸ºå­¦æœ¯ä¸“å®¶ï¼Œè¯·æŒ‰ç…§ç»¼è¿°æ ‡å‡†åˆ†æä»¥ä¸‹æ–¹é¢ï¼š

1. **ç ”ç©¶è„‰ç»œæ¢³ç†**ï¼šè¯¥é¢†åŸŸçš„å‘å±•å†ç¨‹å’Œä¸»è¦é˜¶æ®µ
2. **æ ¸å¿ƒè§‚ç‚¹è¯†åˆ«**ï¼šä¸»è¦ç ”ç©¶è€…çš„æ ¸å¿ƒè§‚ç‚¹å’Œç†è®ºè´¡çŒ®
3. **æŠ€æœ¯å‘å±•è¯„è¿°**ï¼šæŠ€æœ¯æ–¹æ³•çš„æ¼”è¿›å’Œä¼˜ç¼ºç‚¹åˆ†æ
4. **é—®é¢˜ä¸æŒ‘æˆ˜**ï¼šå½“å‰å­˜åœ¨çš„ä¸»è¦é—®é¢˜å’ŒæŠ€æœ¯æŒ‘æˆ˜
5. **ç ”ç©¶ä»·å€¼åˆ¤æ–­**ï¼šå„ä¸ªé—®é¢˜çš„ç ”ç©¶ä»·å€¼å’Œè§£å†³æ„ä¹‰
6. **æœªæ¥å±•æœ›**ï¼šå‘å±•è¶‹åŠ¿å’Œæ½œåœ¨çªç ´æ–¹å‘

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "summary": "ç« èŠ‚æ ¸å¿ƒå†…å®¹æ€»ç»“",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
    "research_evolution": "ç ”ç©¶å‘å±•è„‰ç»œ",
    "main_viewpoints": ["è§‚ç‚¹1", "è§‚ç‚¹2"],
    "technical_analysis": "æŠ€æœ¯å‘å±•åˆ†æ",
    "identified_challenges": ["æŒ‘æˆ˜1", "æŒ‘æˆ˜2"],
    "research_value": "ç ”ç©¶ä»·å€¼è¯„ä¼°",
    "future_prospects": "æœªæ¥å‘å±•å‰æ™¯"
}}
"""

            response = self.ai_client.send_message(prompt)

            if response and hasattr(response, 'content'):
                json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
                if json_match:
                    try:
                        return json.loads(json_match.group())
                    except json.JSONDecodeError:
                        pass

                # å¤‡ç”¨è§£æ
                print(f"   ğŸ“ ä½¿ç”¨ç»¼è¿°ç« èŠ‚å¤‡ç”¨è§£ææ–¹æ³•")
                return {
                    'summary': response.content[:500] + "...",
                    'key_points': ['ç»¼è¿°æ€§åˆ†æç»“æœ'],
                    'research_evolution': 'é¢†åŸŸå‘å±•å†ç¨‹',
                    'main_viewpoints': ['ä¸»è¦å­¦æœ¯è§‚ç‚¹'],
                    'technical_analysis': 'æŠ€æœ¯å‘å±•åˆ†æ',
                    'identified_challenges': ['å…³é”®æŒ‘æˆ˜'],
                    'research_value': 'é‡è¦ç ”ç©¶ä»·å€¼',
                    'future_prospects': 'å‘å±•å‰æ™¯è‰¯å¥½',
                }

        except Exception as e:
            print(f"   âš ï¸ ç»¼è¿°ç« èŠ‚åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"ç»¼è¿°ç« èŠ‚AIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)

        return {'summary': 'åˆ†æå¤±è´¥', 'key_points': []}

    def _conduct_comprehensive_review_analysis(self, content: str) -> Dict[str, Any]:
        """è¿›è¡Œå…¨é¢çš„ç»¼è¿°åˆ†æ"""
        if not self.ai_client:
            return {}

        try:
            prompt = f"""
ä½œä¸ºå­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç»¼è¿°å†…å®¹è¿›è¡Œç³»ç»Ÿæ€§çš„æ·±åº¦åˆ†æï¼š

ç»¼è¿°å†…å®¹ï¼š
{content[:6000]}

è¯·æŒ‰ç…§å­¦æœ¯ç»¼è¿°çš„æœ€é«˜æ ‡å‡†ï¼Œæ·±å…¥åˆ†æï¼š

**æ ¸å¿ƒåˆ†æä»»åŠ¡ï¼š**
1. **ç ”ç©¶è€…è§‚ç‚¹æ¢³ç†**ï¼šè¯†åˆ«ä¸»è¦ç ”ç©¶è€…ã€ç ”ç©¶å›¢é˜ŸåŠå…¶æ ¸å¿ƒå­¦æœ¯è§‚ç‚¹
2. **ç ”ç©¶è„‰ç»œåˆ†æ**ï¼šæ¢³ç†è¯¥ç ”ç©¶é¢†åŸŸçš„å‘å±•è„‰ç»œã€é‡è¦èŠ‚ç‚¹å’Œæ¼”è¿›é€»è¾‘
3. **æ‰¹åˆ¤æ€§æ€è€ƒ**ï¼šå¯¹ç°æœ‰ç ”ç©¶æˆæœè¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æï¼Œè¯†åˆ«ä¼˜åŠ¿å’Œä¸è¶³
4. **é—®é¢˜è¯†åˆ«ä¸ä»·å€¼è¯„ä¼°**ï¼šæ‰¾å‡ºå…³é”®é—®é¢˜ï¼Œè¯„ä¼°å…¶ç ”ç©¶ä»·å€¼å’Œè§£å†³æ„ä¹‰
5. **ç ”ç©¶ç©ºç™½å‘ç°**ï¼šè¯†åˆ«ç ”ç©¶ç©ºç™½å’Œæœªæ¥ç ”ç©¶æœºä¼š
6. **å‘å±•è¶‹åŠ¿é¢„æµ‹**ï¼šåŸºäºåˆ†æé¢„æµ‹æœªæ¥å‘å±•æ–¹å‘

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·æä¾›ç»“æ„åŒ–ã€æ·±åº¦çš„å­¦æœ¯åˆ†æï¼Œé‡ç‚¹çªå‡ºæ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ›æ–°è§è§£ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "comprehensive_analysis": {{
        "key_researchers": [
            {{
                "name": "ç ”ç©¶è€…å§“å",
                "institution": "æ‰€å±æœºæ„", 
                "main_contribution": "ä¸»è¦è´¡çŒ®",
                "academic_influence": "å­¦æœ¯å½±å“åŠ›"
            }}
        ],
        "research_evolution": [
            {{
                "stage": "å‘å±•é˜¶æ®µ",
                "timeframe": "æ—¶é—´èŒƒå›´",
                "key_developments": "å…³é”®å‘å±•",
                "breakthrough_technologies": "çªç ´æ€§æŠ€æœ¯"
            }}
        ],
        "critical_evaluation": [
            {{
                "research_area": "ç ”ç©¶é¢†åŸŸ",
                "current_state": "ç°çŠ¶æè¿°",
                "strengths": "ä¼˜åŠ¿åˆ†æ", 
                "weaknesses": "ä¸è¶³åˆ†æ",
                "critical_insights": "æ‰¹åˆ¤æ€§è§è§£"
            }}
        ],
        "problem_analysis": [
            {{
                "problem": "å…³é”®é—®é¢˜",
                "severity": "ä¸¥é‡ç¨‹åº¦",
                "research_significance": "ç ”ç©¶æ„ä¹‰",
                "solution_potential": "è§£å†³æ½œåŠ›",
                "resource_requirements": "èµ„æºéœ€æ±‚"
            }}
        ],
        "research_opportunities": [
            {{
                "opportunity": "ç ”ç©¶æœºä¼š",
                "innovation_potential": "åˆ›æ–°æ½œåŠ›",
                "feasibility": "å¯è¡Œæ€§åˆ†æ",
                "expected_impact": "é¢„æœŸå½±å“"
            }}
        ]
    }}
}}
"""

            response = self.ai_client.send_message(prompt)

            if response and hasattr(response, 'content'):
                json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        return result.get('comprehensive_analysis', {})
                    except json.JSONDecodeError:
                        pass

        except Exception as e:
            print(f"   âš ï¸ å…¨é¢ç»¼è¿°åˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"å…¨é¢ç»¼è¿°åˆ†æå¤±è´¥: {str(e)}", exc_info=True)

        return {}

    def _analyze_methodology_with_ai(self, text: str) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æç ”ç©¶æ–¹æ³•è®ºç« èŠ‚ - æ”¯æŒå¤šå­¦ç§‘é¢†åŸŸ"""
        if not self.ai_client:
            return {'methods': [], 'methodology': [], 'approach': []}

        try:
            # 1. æ™ºèƒ½æå–æ–¹æ³•è®ºç›¸å…³å†…å®¹
            methodology_content = self._extract_methodology_content(text)

            if not methodology_content or len(methodology_content) < 200:
                return {'methods': [], 'methodology': [], 'approach': []}

            # 2. ä¸“ä¸šæ–¹æ³•è®ºåˆ†ææç¤º
            prompt = f"""
ä½œä¸ºç ”ç©¶æ–¹æ³•è®ºä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹è®ºæ–‡çš„æ–¹æ³•è®ºå†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼š

**åˆ†æå†…å®¹ï¼š**
{methodology_content[:5000]}

**é€šç”¨åˆ†æä»»åŠ¡ï¼š**
è¯·æŒ‰ç…§ç ”ç©¶æ–¹æ³•è®ºçš„å­¦æœ¯æ ‡å‡†ï¼Œç³»ç»Ÿåˆ†æä»¥ä¸‹æ–¹é¢ï¼š

1. **ç ”ç©¶èŒƒå¼ä¸ç†è®ºæ¡†æ¶**ï¼šè¯†åˆ«ç ”ç©¶é‡‡ç”¨çš„åŸºæœ¬èŒƒå¼å’Œç†è®ºåŸºç¡€
2. **ç ”ç©¶è®¾è®¡ä¸ç­–ç•¥**ï¼šåˆ†æç ”ç©¶è®¾è®¡ç±»å‹ã€ç ”ç©¶ç­–ç•¥å’Œæ€»ä½“ç ”ç©¶è·¯çº¿
3. **æŠ€æœ¯æ–¹æ³•ä¸å·¥å…·**ï¼šæå–å…·ä½“çš„ç ”ç©¶æŠ€æœ¯ã€å®éªŒæ–¹æ³•ã€åˆ†æå·¥å…·ç­‰
4. **æ•°æ®è·å–ä¸å¤„ç†**ï¼šåˆ†ææ•°æ®æ¥æºã€é‡‡é›†æ–¹æ³•ã€å¤„ç†æŠ€æœ¯ç­‰
5. **è´¨é‡æ§åˆ¶ä¸éªŒè¯**ï¼šè¯†åˆ«ç ”ç©¶çš„å¯é æ€§å’Œæœ‰æ•ˆæ€§ä¿è¯æªæ–½
6. **åˆ›æ–°æ€§æ–¹æ³•è®ºè´¡çŒ®**ï¼šè¯„ä¼°æ–¹æ³•è®ºå±‚é¢çš„åˆ›æ–°ç‚¹å’Œè´¡çŒ®

**è¾“å‡ºæ ¼å¼ï¼š**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š

{{
    "research_paradigm": {{
        "paradigm_type": "ç ”ç©¶èŒƒå¼ç±»å‹",
        "theoretical_foundation": "ç†è®ºåŸºç¡€",
        "philosophical_basis": "å“²å­¦åŸºç¡€",
        "epistemic_approach": "è®¤è¯†è®ºæ–¹æ³•"
    }},
    "research_design": {{
        "design_type": "ç ”ç©¶è®¾è®¡ç±»å‹",
        "research_strategy": "ç ”ç©¶ç­–ç•¥",
        "overall_framework": "æ€»ä½“æ¡†æ¶",
        "research_process": "ç ”ç©¶æµç¨‹"
    }},
    "technical_methods": [
        {{
            "method_category": "æ–¹æ³•ç±»åˆ«",
            "specific_technique": "å…·ä½“æŠ€æœ¯",
            "application_purpose": "åº”ç”¨ç›®çš„",
            "technical_specifications": "æŠ€æœ¯è§„æ ¼"
        }}
    ],
    "data_methodology": {{
        "data_sources": ["æ•°æ®æ¥æº1", "æ•°æ®æ¥æº2"],
        "collection_methods": ["é‡‡é›†æ–¹æ³•1", "é‡‡é›†æ–¹æ³•2"],
        "processing_techniques": ["å¤„ç†æŠ€æœ¯1", "å¤„ç†æŠ€æœ¯2"],
        "analysis_approaches": ["åˆ†ææ–¹æ³•1", "åˆ†ææ–¹æ³•2"]
    }},
    "quality_assurance": {{
        "reliability_measures": ["å¯é æ€§æªæ–½1", "å¯é æ€§æªæ–½2"],
        "validity_checks": ["æœ‰æ•ˆæ€§æ£€éªŒ1", "æœ‰æ•ˆæ€§æ£€éªŒ2"],
        "error_control": ["è¯¯å·®æ§åˆ¶1", "è¯¯å·®æ§åˆ¶2"],
        "verification_methods": ["éªŒè¯æ–¹æ³•1", "éªŒè¯æ–¹æ³•2"]
    }},
    "methodological_innovations": [
        {{
            "innovation_area": "åˆ›æ–°é¢†åŸŸ",
            "innovation_description": "åˆ›æ–°æè¿°",
            "novelty_significance": "æ–°é¢–æ€§æ„ä¹‰",
            "potential_impact": "æ½œåœ¨å½±å“"
        }}
    ],
    "limitations_and_considerations": [
        {{
            "limitation": "æ–¹æ³•å±€é™æ€§",
            "impact_assessment": "å½±å“è¯„ä¼°",
            "mitigation_strategy": "ç¼“è§£ç­–ç•¥"
        }}
    ]
}}
"""

            response = self.ai_client.send_message(prompt)

            if response and hasattr(response, 'content'):
                # æ”¹è¿›çš„JSONè§£æ
                content = response.content.strip()

                # æ–¹æ³•1: ç›´æ¥è§£æ
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        analysis_result = json.loads(json_match.group())
                        print(f"   âœ… æ–¹æ³•è®ºJSONè§£ææˆåŠŸ")
                        return analysis_result
                    except json.JSONDecodeError as e:
                        print(f"   âš ï¸ æ–¹æ³•è®ºJSONè§£æå¤±è´¥: {e}")

                # æ–¹æ³•2: æ¸…ç†åè§£æ
                try:
                    cleaned_content = re.sub(r'```(?:json)?\s*', '', content)
                    cleaned_content = re.sub(r'```\s*$', '', cleaned_content)
                    start_idx = cleaned_content.find('{')
                    if start_idx != -1:
                        brace_count = 0
                        end_idx = start_idx
                        for i, char in enumerate(
                            cleaned_content[start_idx:], start_idx
                        ):
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    end_idx = i + 1
                                    break
                        if brace_count == 0:
                            json_str = cleaned_content[start_idx:end_idx]
                            analysis_result = json.loads(json_str)
                            print(f"   âœ… æ–¹æ³•è®ºæ¸…ç†åJSONè§£ææˆåŠŸ")
                            return analysis_result
                except json.JSONDecodeError:
                    pass

            return self._get_empty_methodology_analysis()

        except Exception as e:
            print(f"   âš ï¸ æ–¹æ³•è®ºAIåˆ†æå¤±è´¥: {str(e)}")
            logger.error(f"æ–¹æ³•è®ºAIåˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            return self._get_empty_methodology_analysis()

    def _extract_methodology_content(self, text: str) -> str:
        """æ™ºèƒ½æå–æ–¹æ³•è®ºç›¸å…³å†…å®¹"""
        # é€šç”¨æ–¹æ³•è®ºæœç´¢æ¨¡å¼
        search_patterns = [
            r'ç¬¬äºŒç« \s*ç ”ç©¶æ–¹æ³•.*?(?=ç¬¬ä¸‰ç« |ç¬¬\d+ç« |$)',
            r'ç¬¬äºŒç« \s*æ–¹æ³•è®º.*?(?=ç¬¬ä¸‰ç« |ç¬¬\d+ç« |$)',
            r'ç ”ç©¶æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            r'æ–¹æ³•è®º.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            r'å®éªŒæ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            r'æŠ€æœ¯æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            r'ææ–™ä¸æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
        ]

        extracted_content = ""
        for pattern in search_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(0)
                if len(content) > 500:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿä¸°å¯Œ
                    extracted_content = content
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨ç« èŠ‚ï¼Œä½¿ç”¨é€šç”¨å…³é”®è¯æå–
        if not extracted_content:
            methodology_keywords = [
                'æ–¹æ³•',
                'å®éªŒ',
                'ç ”ç©¶',
                'è®¾è®¡',
                'ææ–™',
                'æŠ€æœ¯',
                'æµç¨‹',
                'æ­¥éª¤',
                'è¿‡ç¨‹',
            ]
            lines = text.split('\n')
            methodology_lines = []
            for line in lines:
                if any(keyword in line.lower() for keyword in methodology_keywords):
                    methodology_lines.append(line)
                if len('\n'.join(methodology_lines)) > 3000:
                    break
            extracted_content = '\n'.join(methodology_lines)

        # å†…å®¹é•¿åº¦æ§åˆ¶
        if len(extracted_content) > 8000:
            extracted_content = extracted_content[:8000] + "..."

        return extracted_content

    def _get_discipline_methodology_patterns(self, discipline: str) -> List[str]:
        """æ ¹æ®å­¦ç§‘è·å–æ–¹æ³•è®ºæœç´¢æ¨¡å¼"""
        base_patterns = [
            r'ç¬¬äºŒç« \s*ç ”ç©¶æ–¹æ³•.*?(?=ç¬¬ä¸‰ç« |ç¬¬\d+ç« |$)',
            r'ç¬¬äºŒç« \s*æ–¹æ³•è®º.*?(?=ç¬¬ä¸‰ç« |ç¬¬\d+ç« |$)',
            r'ç ”ç©¶æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            r'æ–¹æ³•è®º.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
        ]

        discipline_patterns = {
            'engineering': [
                r'ç¬¬äºŒç« \s*å®éªŒè¯å“è¯•å‰‚ã€ä»ªå™¨ã€è¡¨å¾æ–¹æ³•åŠæ€§è´¨æµ‹è¯•æ–¹æ³•.*?(?=ç¬¬ä¸‰ç« |ç¬¬\d+ç« |$)',
                r'å®éªŒæ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æŠ€æœ¯æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ææ–™ä¸æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'è¡¨å¾æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æµ‹è¯•æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'å·¥è‰ºæµç¨‹.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            ],
            'natural_sciences': [
                r'å®éªŒè®¾è®¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'å®éªŒææ–™.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'åˆ†ææ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ£€æµ‹æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ ·å“åˆ¶å¤‡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            ],
            'medicine': [
                r'ç ”ç©¶å¯¹è±¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ä¸´åºŠè¯•éªŒ.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ç—…ä¾‹é€‰æ‹©.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'è¯Šæ–­æ ‡å‡†.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ²»ç–—æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ç»Ÿè®¡åˆ†æ.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            ],
            'social_sciences': [
                r'ç ”ç©¶è®¾è®¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'è°ƒæŸ¥æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ•°æ®æ”¶é›†.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'è®¿è°ˆæ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'é—®å·è®¾è®¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ç»Ÿè®¡æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            ],
            'computer_science': [
                r'ç®—æ³•è®¾è®¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'ç³»ç»Ÿè®¾è®¡.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'å®ç°æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ•°æ®é›†.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'è¯„ä¼°æ–¹æ³•.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
                r'æ€§èƒ½åˆ†æ.*?(?=ç¬¬\d+ç« |å®éªŒç»“æœ|ç»“è®º|$)',
            ],
        }

        return base_patterns + discipline_patterns.get(discipline, [])

    def _get_discipline_name(self, discipline: str) -> str:
        """è·å–å­¦ç§‘ä¸­æ–‡åç§°"""
        discipline_names = {
            'engineering': 'å·¥ç¨‹æŠ€æœ¯',
            'natural_sciences': 'è‡ªç„¶ç§‘å­¦',
            'medicine': 'åŒ»å­¦',
            'social_sciences': 'ç¤¾ä¼šç§‘å­¦',
            'humanities': 'äººæ–‡å­¦ç§‘',
            'computer_science': 'è®¡ç®—æœºç§‘å­¦',
            'agriculture': 'å†œä¸šç§‘å­¦',
            'general': 'é€šç”¨å­¦ç§‘',
        }
        return discipline_names.get(discipline, 'é€šç”¨å­¦ç§‘')

    def _get_discipline_methodology_prompt(self, discipline: str) -> str:
        """æ ¹æ®å­¦ç§‘è·å–ç‰¹å®šçš„æ–¹æ³•è®ºåˆ†ææç¤º"""
        discipline_prompts = {
            'engineering': """
            é’ˆå¯¹å·¥ç¨‹æŠ€æœ¯é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - å®éªŒè®¾è®¡ä¸å·¥è‰ºæµç¨‹
            - ææ–™åˆ¶å¤‡ä¸è¡¨å¾æ–¹æ³•
            - æµ‹è¯•è®¾å¤‡ä¸ä»ªå™¨è§„æ ¼
            - æ€§èƒ½è¯„ä»·ä¸è´¨é‡æ§åˆ¶
            - å·¥ç¨‹å®è·µä¸æŠ€æœ¯åˆ›æ–°
            """,
            'natural_sciences': """
            é’ˆå¯¹è‡ªç„¶ç§‘å­¦é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - å®éªŒè®¾è®¡ä¸å¯¹ç…§ç»„è®¾ç½®
            - æ ·å“åˆ¶å¤‡ä¸å¤„ç†æ–¹æ³•
            - æ£€æµ‹æŠ€æœ¯ä¸åˆ†æä»ªå™¨
            - æ•°æ®å¤„ç†ä¸ç»Ÿè®¡åˆ†æ
            - ç†è®ºæ¨¡å‹ä¸éªŒè¯æ–¹æ³•
            """,
            'medicine': """
            é’ˆå¯¹åŒ»å­¦é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç ”ç©¶å¯¹è±¡é€‰æ‹©æ ‡å‡†
            - ä¸´åºŠè¯•éªŒè®¾è®¡
            - è¯Šæ–­ä¸æ²»ç–—æ–¹æ³•
            - ä¼¦ç†å®¡æŸ¥ä¸çŸ¥æƒ…åŒæ„
            - ç»Ÿè®¡å­¦æ–¹æ³•ä¸æ•ˆæœè¯„ä¼°
            """,
            'social_sciences': """
            é’ˆå¯¹ç¤¾ä¼šç§‘å­¦é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç ”ç©¶è®¾è®¡ï¼ˆå®šæ€§/å®šé‡/æ··åˆï¼‰
            - æŠ½æ ·æ–¹æ³•ä¸æ ·æœ¬ä»£è¡¨æ€§
            - æ•°æ®æ”¶é›†å·¥å…·ï¼ˆé—®å·ã€è®¿è°ˆç­‰ï¼‰
            - ä¿¡åº¦æ•ˆåº¦æ£€éªŒ
            - ç»Ÿè®¡åˆ†ææ–¹æ³•
            """,
            'computer_science': """
            é’ˆå¯¹è®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç®—æ³•è®¾è®¡ä¸å®ç°
            - ç³»ç»Ÿæ¶æ„ä¸æŠ€æœ¯æ ˆ
            - æ•°æ®é›†é€‰æ‹©ä¸é¢„å¤„ç†
            - è¯„ä¼°æŒ‡æ ‡ä¸åŸºå‡†æµ‹è¯•
            - æ€§èƒ½ä¼˜åŒ–ä¸å¤æ‚åº¦åˆ†æ
            """,
            'humanities': """
            é’ˆå¯¹äººæ–‡å­¦ç§‘é¢†åŸŸï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - æ–‡çŒ®æ”¶é›†ä¸é€‰æ‹©æ ‡å‡†
            - å²æ–™åˆ†æä¸è€ƒè¯æ–¹æ³•
            - ç†è®ºæ¡†æ¶ä¸åˆ†æè§†è§’
            - æ¯”è¾ƒç ”ç©¶ä¸æ¡ˆä¾‹åˆ†æ
            - é˜é‡Šæ–¹æ³•ä¸è®ºè¯é€»è¾‘
            """,
        }
        return discipline_prompts.get(
            discipline, "è¯·æŒ‰ç…§è¯¥å­¦ç§‘é¢†åŸŸçš„ç ”ç©¶è§„èŒƒè¿›è¡Œåˆ†æã€‚"
        )

    def _parse_methodology_fallback(self, text: str) -> Dict[str, Any]:
        """å¤‡ç”¨æ–¹æ³•è®ºè§£ææ–¹æ³•"""
        result = self._get_empty_methodology_analysis()

        # ç®€å•çš„å…³é”®è¯æå–
        text_lower = text.lower()

        # æå–ç ”ç©¶æ–¹æ³•
        if any(
            keyword in text_lower for keyword in ['æ–¹æ³•', 'method', 'æŠ€æœ¯', 'technique']
        ):
            result['technical_methods'] = [
                {
                    'method_category': 'å®éªŒæ–¹æ³•',
                    'specific_technique': 'ææ–™è¡¨å¾æŠ€æœ¯',
                    'application_purpose': 'ç»“æ„åˆ†æ',
                    'technical_specifications': 'å¾…è¯¦ç»†åˆ†æ',
                }
            ]

        # æå–æ•°æ®æ–¹æ³•
        if any(
            keyword in text_lower for keyword in ['æ•°æ®', 'data', 'åˆ†æ', 'analysis']
        ):
            result['data_methodology'] = {
                'data_sources': ['å®éªŒæ•°æ®'],
                'collection_methods': ['å®éªŒæµ‹è¯•'],
                'processing_techniques': ['æ•°æ®åˆ†æ'],
                'analysis_approaches': ['å®šé‡åˆ†æ'],
            }

        # æå–è´¨é‡æ§åˆ¶
        if any(
            keyword in text_lower
            for keyword in ['è´¨é‡', 'quality', 'éªŒè¯', 'validation']
        ):
            result['quality_assurance'] = {
                'reliability_measures': ['é‡å¤æ€§éªŒè¯'],
                'validity_checks': ['æ–¹æ³•éªŒè¯'],
                'error_control': ['è¯¯å·®åˆ†æ'],
                'verification_methods': ['å¯¹æ¯”éªŒè¯'],
            }

        return result

    def _get_empty_methodology_analysis(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„æ–¹æ³•è®ºåˆ†æç»“æ„"""
        return {
            'research_paradigm': {},
            'research_design': {},
            'technical_methods': [],
            'data_methodology': {},
            'quality_assurance': {},
            'methodological_innovations': [],
            'limitations_and_considerations': [],
        }

    def _analyze_experimental_section_with_ai(self, text: str) -> Dict[str, Any]:
        """åˆ†æå®éªŒéƒ¨åˆ† - æ”¯æŒå¤šå­¦ç§‘é¢†åŸŸ"""
        if not self.ai_client:
            return {'experiments': [], 'models': [], 'parameters': []}

        # 1. æå–å®éªŒå†…å®¹
        exp_content = self._extract_experimental_content(text)

        if not exp_content:
            return {'experiments': [], 'models': [], 'parameters': []}

        try:
            prompt = f"""
ä½œä¸ºå®éªŒç ”ç©¶ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å®éªŒå†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼š

**å®éªŒå†…å®¹ï¼š**
{exp_content[:3000]}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ç³»ç»Ÿåˆ†æä»¥ä¸‹æ–¹é¢ï¼š
1. å®éªŒè®¾è®¡ä¸æ–¹æ¡ˆ
2. å®éªŒæ–¹æ³•ä¸æŠ€æœ¯
3. å®éªŒææ–™ä¸å·¥å…·
4. å®éªŒå‚æ•°ä¸æ¡ä»¶
5. å®éªŒè¿‡ç¨‹ä¸æ­¥éª¤
6. è´¨é‡æ§åˆ¶æªæ–½

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "experimental_design": {{
        "design_type": "å®éªŒè®¾è®¡ç±»å‹",
        "research_questions": ["ç ”ç©¶é—®é¢˜1", "ç ”ç©¶é—®é¢˜2"],
        "hypotheses": ["å‡è®¾1", "å‡è®¾2"],
        "experimental_strategy": "å®éªŒç­–ç•¥"
    }},
    "experimental_methods": [
        {{
            "method_name": "æ–¹æ³•åç§°",
            "method_type": "æ–¹æ³•ç±»å‹",
            "procedure": "æ“ä½œæ­¥éª¤",
            "purpose": "å®éªŒç›®çš„"
        }}
    ],
    "materials_and_tools": {{
        "materials": ["ææ–™1", "ææ–™2"],
        "instruments": ["ä»ªå™¨1", "ä»ªå™¨2"],
        "software": ["è½¯ä»¶1", "è½¯ä»¶2"],
        "reagents": ["è¯•å‰‚1", "è¯•å‰‚2"]
    }},
    "experimental_parameters": [
        {{
            "parameter_name": "å‚æ•°åç§°",
            "value_range": "æ•°å€¼èŒƒå›´",
            "unit": "å•ä½",
            "significance": "é‡è¦æ€§è¯´æ˜"
        }}
    ],
    "experimental_conditions": {{
        "environmental_conditions": ["ç¯å¢ƒæ¡ä»¶1", "ç¯å¢ƒæ¡ä»¶2"],
        "operational_conditions": ["æ“ä½œæ¡ä»¶1", "æ“ä½œæ¡ä»¶2"],
        "safety_measures": ["å®‰å…¨æªæ–½1", "å®‰å…¨æªæ–½2"]
    }},
    "quality_control": {{
        "control_groups": ["å¯¹ç…§ç»„1", "å¯¹ç…§ç»„2"],
        "validation_methods": ["éªŒè¯æ–¹æ³•1", "éªŒè¯æ–¹æ³•2"],
        "error_sources": ["è¯¯å·®æ¥æº1", "è¯¯å·®æ¥æº2"],
        "repeatability_measures": ["é‡ç°æ€§æªæ–½1", "é‡ç°æ€§æªæ–½2"]
    }}
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                # å°è¯•æå–å’Œè§£æJSON - æ”¹è¿›ç‰ˆ
                content = response.content.strip()

                # æ–¹æ³•1: ç›´æ¥æŸ¥æ‰¾JSONç»“æ„
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        analysis_result = json.loads(json_match.group())
                        print(f"   âœ… JSONè§£ææˆåŠŸ")
                        return analysis_result
                    except json.JSONDecodeError as e:
                        print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")

                # æ–¹æ³•2: å°è¯•æ¸…ç†å¹¶é‡æ–°è§£æ
                try:
                    # ç§»é™¤markdownä»£ç å—æ ‡è®°
                    cleaned_content = re.sub(r'```(?:json)?\s*', '', content)
                    cleaned_content = re.sub(r'```\s*$', '', cleaned_content)

                    # æŸ¥æ‰¾æœ€å¤–å±‚çš„èŠ±æ‹¬å·
                    start_idx = cleaned_content.find('{')
                    if start_idx != -1:
                        brace_count = 0
                        end_idx = start_idx
                        for i, char in enumerate(
                            cleaned_content[start_idx:], start_idx
                        ):
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    end_idx = i + 1
                                    break

                        if brace_count == 0:
                            json_str = cleaned_content[start_idx:end_idx]
                            analysis_result = json.loads(json_str)
                            print(f"   âœ… æ¸…ç†åJSONè§£ææˆåŠŸ")
                            return analysis_result
                except json.JSONDecodeError as e:
                    print(f"   âš ï¸ æ¸…ç†åJSONè§£æä»å¤±è´¥: {e}")
                    logger.error(f"å®éªŒéƒ¨åˆ†JSONè§£æå¤±è´¥: {e}", exc_info=True)
                    return self._get_empty_experimental_analysis()

        except Exception as e:
            print(f"   âš ï¸ å®éªŒAIåˆ†æå¤±è´¥: {e}")

        return self._get_empty_experimental_analysis()

    def _extract_experimental_content(self, text: str) -> str:
        """æå–å®éªŒç›¸å…³å†…å®¹"""
        # é€šç”¨å®éªŒæ¨¡å¼
        exp_patterns = [
            r'ç¬¬.*ç« \s*å®éªŒ.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'å®éªŒè®¾è®¡.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'å®éªŒæ–¹æ³•.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'å®éªŒæ­¥éª¤.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'å®éªŒææ–™.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'æµ‹è¯•æ–¹æ³•.*?(?=ç¬¬.*ç« |ç»“è®º|å‚è€ƒæ–‡çŒ®|$)',
        ]

        exp_content = ""
        for pattern in exp_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(0)
                if len(content) > 300:
                    exp_content = content
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢
        if not exp_content:
            keywords = ['å®éªŒ', 'æµ‹è¯•', 'æ£€æµ‹', 'åˆ†æ', 'è¯•éªŒ', 'ææ–™', 'æ ·å“', 'è®¾å¤‡']
            lines = text.split('\n')
            exp_lines = []
            for line in lines:
                if any(keyword in line.lower() for keyword in keywords):
                    exp_lines.append(line)
                if len('\n'.join(exp_lines)) > 2000:
                    break
            exp_content = '\n'.join(exp_lines)

        return exp_content[:5000] if exp_content else ""

    def _get_discipline_experimental_patterns(self, discipline: str) -> List[str]:
        """è·å–å­¦ç§‘ç‰¹å®šçš„å®éªŒæ¨¡å¼"""
        base_patterns = [
            r'(å®éªŒ[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            r'(å®éªŒè®¾è®¡[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            r'(å®éªŒè¿‡ç¨‹[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
        ]

        discipline_patterns = {
            'engineering': [
                r'(ææ–™åˆ¶å¤‡[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(å·¥è‰ºæµç¨‹[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(æ€§èƒ½æµ‹è¯•[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            ],
            'medicine': [
                r'(ä¸´åºŠè¯•éªŒ[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(ç—…ä¾‹ç ”ç©¶[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(è¯Šæ–­æ–¹æ³•[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            ],
            'social_sciences': [
                r'(è°ƒæŸ¥ç ”ç©¶[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(è®¿è°ˆå®æ–½[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(æ•°æ®æ”¶é›†[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            ],
            'computer_science': [
                r'(ç®—æ³•å®ç°[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(ç³»ç»Ÿå®éªŒ[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
                r'(æ€§èƒ½æµ‹è¯•[\s\S]{300,5000}?)(?=ç»“æœåˆ†æ|ç»“è®º|ç¬¬[äº”å…­ä¸ƒ]ç« )',
            ],
        }

        return base_patterns + discipline_patterns.get(discipline, [])

    def _get_discipline_experimental_keywords(self, discipline: str) -> List[str]:
        """è·å–å­¦ç§‘ç‰¹å®šçš„å®éªŒå…³é”®è¯"""
        base_keywords = ['å®éªŒ', 'æµ‹è¯•', 'å®æ–½', 'æ“ä½œ', 'è¿‡ç¨‹']

        discipline_keywords = {
            'engineering': ['åˆ¶å¤‡', 'åˆæˆ', 'åŠ å·¥', 'æµ‹è¯•', 'è¡¨å¾', 'å·¥è‰º', 'è®¾å¤‡'],
            'medicine': ['ä¸´åºŠ', 'è¯Šæ–­', 'æ²»ç–—', 'ç—…ä¾‹', 'æ‚£è€…', 'è¯•éªŒ', 'è§‚å¯Ÿ'],
            'social_sciences': ['è°ƒæŸ¥', 'è®¿è°ˆ', 'è§‚å¯Ÿ', 'æŠ½æ ·', 'é—®å·', 'ç ”ç©¶'],
            'computer_science': ['ç®—æ³•', 'å®ç°', 'ç¼–ç¨‹', 'æµ‹è¯•', 'éªŒè¯', 'ä¼˜åŒ–'],
            'natural_sciences': ['å®éªŒ', 'è§‚å¯Ÿ', 'æµ‹é‡', 'åˆ†æ', 'æ£€æµ‹', 'æ ·å“'],
        }

        return base_keywords + discipline_keywords.get(discipline, [])

    def _get_discipline_experimental_prompt(self, discipline: str) -> str:
        """è·å–å­¦ç§‘ç‰¹å®šçš„å®éªŒåˆ†ææç¤º"""
        discipline_prompts = {
            'engineering': """
            é’ˆå¯¹å·¥ç¨‹æŠ€æœ¯é¢†åŸŸå®éªŒï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ææ–™åˆ¶å¤‡å·¥è‰ºä¸å‚æ•°
            - è®¾å¤‡é…ç½®ä¸æ“ä½œæ¡ä»¶
            - æ€§èƒ½æµ‹è¯•æ–¹æ³•ä¸æ ‡å‡†
            - å·¥è‰ºä¼˜åŒ–ä¸è´¨é‡æ§åˆ¶
            - å®‰å…¨æ“ä½œä¸ç¯å¢ƒè¦æ±‚
            """,
            'medicine': """
            é’ˆå¯¹åŒ»å­¦é¢†åŸŸå®éªŒï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç ”ç©¶å¯¹è±¡é€‰æ‹©ä¸åˆ†ç»„
            - ä¸´åºŠè¯•éªŒè®¾è®¡ä¸å®æ–½
            - ä¼¦ç†å®¡æŸ¥ä¸çŸ¥æƒ…åŒæ„
            - è¯Šæ–­æ ‡å‡†ä¸è¯„ä¼°æ–¹æ³•
            - å®‰å…¨æ€§ä¸æœ‰æ•ˆæ€§è¯„ä»·
            """,
            'social_sciences': """
            é’ˆå¯¹ç¤¾ä¼šç§‘å­¦é¢†åŸŸå®éªŒï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - è°ƒæŸ¥è®¾è®¡ä¸æŠ½æ ·æ–¹æ³•
            - æ•°æ®æ”¶é›†å·¥å…·ä¸ç¨‹åº
            - è®¿è°ˆæŠ€å·§ä¸è§‚å¯Ÿæ–¹æ³•
            - ä¿¡åº¦æ•ˆåº¦æ§åˆ¶æªæ–½
            - ä¼¦ç†è€ƒè™‘ä¸éšç§ä¿æŠ¤
            """,
            'computer_science': """
            é’ˆå¯¹è®¡ç®—æœºç§‘å­¦é¢†åŸŸå®éªŒï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç®—æ³•è®¾è®¡ä¸å®ç°ç»†èŠ‚
            - ç³»ç»Ÿæ¶æ„ä¸æŠ€æœ¯é€‰å‹
            - æµ‹è¯•ç¯å¢ƒä¸é…ç½®å‚æ•°
            - åŸºå‡†æ•°æ®é›†ä¸è¯„ä¼°æŒ‡æ ‡
            - æ€§èƒ½ä¼˜åŒ–ä¸å¤æ‚åº¦åˆ†æ
            """,
        }
        return discipline_prompts.get(discipline, "è¯·æŒ‰ç…§è¯¥å­¦ç§‘çš„å®éªŒè§„èŒƒè¿›è¡Œåˆ†æã€‚")

    def _get_empty_experimental_analysis(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„å®éªŒåˆ†æç»“æ„"""
        return {
            'discipline_identified': 'unknown',
            'experimental_design': {
                'design_type': '',
                'research_questions': [],
                'hypotheses': [],
                'experimental_strategy': '',
            },
            'experimental_methods': [],
            'materials_and_tools': {
                'materials': [],
                'instruments': [],
                'software': [],
                'reagents': [],
            },
            'experimental_parameters': [],
            'experimental_conditions': {
                'environmental_conditions': [],
                'operational_conditions': [],
                'safety_measures': [],
            },
            'quality_control': {
                'control_groups': [],
                'validation_methods': [],
                'error_sources': [],
                'repeatability_measures': [],
            },
        }

    def _analyze_results_with_ai(self, text: str) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ - æ”¯æŒå¤šå­¦ç§‘é¢†åŸŸ"""
        if not self.ai_client:
            return {'results': [], 'analysis': [], 'performance': []}

        # 1. æå–ç»“æœå†…å®¹
        result_content = self._extract_results_content(text)

        if not result_content:
            return {'results': [], 'analysis': [], 'performance': []}

        try:
            prompt = f"""
ä½œä¸ºç»“æœåˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å®éªŒç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼š

**ç»“æœå†…å®¹ï¼š**
{result_content[:3000]}

**åˆ†æä»»åŠ¡ï¼š**
è¯·ç³»ç»Ÿåˆ†æä»¥ä¸‹æ–¹é¢ï¼š
1. ä¸»è¦å®éªŒç»“æœä¸æ•°æ®
2. æ•°æ®åˆ†æä¸ç»Ÿè®¡å¤„ç†
3. ç»“æœè§£é‡Šä¸è®¨è®º
4. æ€§èƒ½è¯„ä¼°ä¸æ¯”è¾ƒ
5. å‘ç°ä¸åˆ›æ–°ç‚¹
6. å±€é™æ€§ä¸ä¸è¶³

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "main_results": [
        {{
            "result_category": "ç»“æœç±»åˆ«",
            "description": "ç»“æœæè¿°",
            "quantitative_data": "å®šé‡æ•°æ®",
            "significance": "é‡è¦æ€§è¯„ä»·"
        }}
    ],
    "data_analysis": {{
        "statistical_methods": ["ç»Ÿè®¡æ–¹æ³•1", "ç»Ÿè®¡æ–¹æ³•2"],
        "data_processing": ["æ•°æ®å¤„ç†1", "æ•°æ®å¤„ç†2"],
        "visualization": ["å¯è§†åŒ–æ–¹æ³•1", "å¯è§†åŒ–æ–¹æ³•2"],
        "quality_metrics": ["è´¨é‡æŒ‡æ ‡1", "è´¨é‡æŒ‡æ ‡2"]
    }},
    "performance_evaluation": [
        {{
            "metric_name": "æŒ‡æ ‡åç§°",
            "metric_value": "æŒ‡æ ‡æ•°å€¼",
            "benchmark_comparison": "åŸºå‡†æ¯”è¾ƒ",
            "improvement": "æ”¹è¿›ç¨‹åº¦"
        }}
    ],
    "key_findings": [
        {{
            "finding": "å…³é”®å‘ç°",
            "evidence": "æ”¯æ’‘è¯æ®",
            "novelty": "æ–°é¢–æ€§è¯„ä»·",
            "impact": "å½±å“è¯„ä¼°"
        }}
    ],
    "result_interpretation": {{
        "theoretical_implications": ["ç†è®ºæ„ä¹‰1", "ç†è®ºæ„ä¹‰2"],
        "practical_applications": ["å®é™…åº”ç”¨1", "å®é™…åº”ç”¨2"],
        "mechanism_explanation": ["æœºç†è§£é‡Š1", "æœºç†è§£é‡Š2"],
        "validation_evidence": ["éªŒè¯è¯æ®1", "éªŒè¯è¯æ®2"]
    }},
    "limitations_and_discussion": {{
        "methodological_limitations": ["æ–¹æ³•å±€é™1", "æ–¹æ³•å±€é™2"],
        "data_limitations": ["æ•°æ®å±€é™1", "æ•°æ®å±€é™2"],
        "future_work": ["æœªæ¥å·¥ä½œ1", "æœªæ¥å·¥ä½œ2"],
        "improvement_suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"]
    }}
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                # ä¸‰çº§JSONè§£æç­–ç•¥
                content = response.content

                # ç¬¬ä¸€çº§ï¼šç›´æ¥JSONè§£æ
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        analysis_result = json.loads(json_match.group())
                        return analysis_result
                    except json.JSONDecodeError:
                        print(f"   ğŸ“ ç¬¬ä¸€çº§JSONè§£æå¤±è´¥ï¼Œå°è¯•ç¬¬äºŒçº§æ¸…ç†è§£æ")

                        # ç¬¬äºŒçº§ï¼šæ¸…ç†åè§£æ
                        cleaned_json = _clean_json_content(json_match.group())
                        try:
                            analysis_result = json.loads(cleaned_json)
                            return analysis_result
                        except json.JSONDecodeError:
                            print(f"   âš ï¸ ç¬¬äºŒçº§æ¸…ç†è§£æå¤±è´¥")
                            return {'results': [], 'analysis': [], 'performance': []}
                else:
                    print(f"   âš ï¸ æœªæ‰¾åˆ°JSONæ ¼å¼")
                    return {'results': [], 'analysis': [], 'performance': []}
        except Exception as e:
            print(f"   âš ï¸ ç»“æœAIåˆ†æå¤±è´¥: {e}")

        return {'results': [], 'analysis': [], 'performance': []}

    def _extract_results_content(self, text: str) -> str:
        """æå–ç»“æœç›¸å…³å†…å®¹"""
        # é€šç”¨ç»“æœæ¨¡å¼
        result_patterns = [
            r'ç¬¬.*ç« \s*ç»“æœ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'å®éªŒç»“æœ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'æµ‹è¯•ç»“æœ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'åˆ†æç»“æœ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'ç»“æœä¸åˆ†æ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
            r'ç»“æœ.*?(?=ç¬¬.*ç« |ç»“è®º|è®¨è®º|å‚è€ƒæ–‡çŒ®|$)',
        ]

        result_content = ""
        for pattern in result_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(0)
                if len(content) > 300:
                    result_content = content
                    break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢
        if not result_content:
            keywords = ['ç»“æœ', 'æ•°æ®', 'å›¾', 'è¡¨', 'åˆ†æ', 'æµ‹è¯•', 'æ€§èƒ½', 'æ•ˆæœ']
            lines = text.split('\n')
            result_lines = []
            for line in lines:
                if any(keyword in line.lower() for keyword in keywords):
                    result_lines.append(line)
                if len('\n'.join(result_lines)) > 2000:
                    break
            result_content = '\n'.join(result_lines)

        return result_content[:5000] if result_content else ""

    def _get_discipline_results_patterns(self, discipline: str) -> List[str]:
        """è·å–å­¦ç§‘ç‰¹å®šçš„ç»“æœæ¨¡å¼"""
        base_patterns = [
            r'(å®éªŒç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            r'(ç»“æœåˆ†æ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            r'(æ€§èƒ½åˆ†æ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
        ]

        discipline_patterns = {
            'engineering': [
                r'(æ€§èƒ½æµ‹è¯•ç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(ææ–™è¡¨å¾ç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(å·¥è‰ºéªŒè¯ç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            ],
            'medicine': [
                r'(ä¸´åºŠè¯•éªŒç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(ç–—æ•ˆè¯„ä¼°[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(å®‰å…¨æ€§åˆ†æ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            ],
            'social_sciences': [
                r'(è°ƒæŸ¥ç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(æ•°æ®åˆ†æç»“æœ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(ç»Ÿè®¡åˆ†æ[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            ],
            'computer_science': [
                r'(ç®—æ³•æ€§èƒ½[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(å®éªŒè¯„ä¼°[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
                r'(åŸºå‡†æµ‹è¯•[\s\S]{300,5000}?)(?=ç»“è®º|è®¨è®º|ç¬¬[å…­ä¸ƒå…«]ç« )',
            ],
        }

        return base_patterns + discipline_patterns.get(discipline, [])

    def _get_discipline_results_keywords(self, discipline: str) -> List[str]:
        """è·å–å­¦ç§‘ç‰¹å®šçš„ç»“æœå…³é”®è¯"""
        base_keywords = ['ç»“æœ', 'æ•°æ®', 'åˆ†æ', 'å‘ç°', 'è¯„ä¼°']

        discipline_keywords = {
            'engineering': ['æ€§èƒ½', 'æ•ˆç‡', 'å¼ºåº¦', 'ç¡¬åº¦', 'å¯¼ç”µ', 'ä¼ çƒ­', 'è¡¨å¾'],
            'medicine': ['ç–—æ•ˆ', 'å®‰å…¨æ€§', 'æœ‰æ•ˆç‡', 'ç”Ÿå­˜ç‡', 'å‰¯ä½œç”¨', 'æ”¹å–„'],
            'social_sciences': ['æ˜¾è‘—æ€§', 'ç›¸å…³æ€§', 'å·®å¼‚', 'è¶‹åŠ¿', 'æ¨¡å¼', 'å…³ç³»'],
            'computer_science': ['å‡†ç¡®ç‡', 'é€Ÿåº¦', 'å†…å­˜', 'ååé‡', 'å»¶è¿Ÿ', 'ä¼˜åŒ–'],
            'natural_sciences': ['æµ“åº¦', 'æ¸©åº¦', 'å‹åŠ›', 'å…‰è°±', 'ç»“æ„', 'ç»„æˆ'],
        }

        return base_keywords + discipline_keywords.get(discipline, [])

    def _get_discipline_results_prompt(self, discipline: str) -> str:
        """è·å–å­¦ç§‘ç‰¹å®šçš„ç»“æœåˆ†ææç¤º"""
        discipline_prompts = {
            'engineering': """
            é’ˆå¯¹å·¥ç¨‹æŠ€æœ¯é¢†åŸŸç»“æœï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ææ–™æ€§èƒ½æŒ‡æ ‡ä¸æµ‹è¯•æ•°æ®
            - å·¥è‰ºå‚æ•°ä¼˜åŒ–ç»“æœ
            - è®¾å¤‡æ€§èƒ½ä¸æ•ˆç‡è¯„ä¼°
            - è´¨é‡æ§åˆ¶ä¸æ ‡å‡†ç¬¦åˆæ€§
            - æŠ€æœ¯åˆ›æ–°ç‚¹ä¸æ”¹è¿›æ•ˆæœ
            """,
            'medicine': """
            é’ˆå¯¹åŒ»å­¦é¢†åŸŸç»“æœï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ä¸´åºŠè¯•éªŒä¸»è¦ç»ˆç‚¹æŒ‡æ ‡
            - å®‰å…¨æ€§ä¸æœ‰æ•ˆæ€§è¯„ä¼°
            - ç»Ÿè®¡å­¦æ„ä¹‰ä¸ä¸´åºŠæ„ä¹‰
            - ä¸è‰¯ååº”ä¸å‰¯ä½œç”¨åˆ†æ
            - ç–—æ•ˆæ¯”è¾ƒä¸åŸºçº¿æ”¹å–„
            """,
            'social_sciences': """
            é’ˆå¯¹ç¤¾ä¼šç§‘å­¦é¢†åŸŸç»“æœï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç»Ÿè®¡æ˜¾è‘—æ€§ä¸æ•ˆåº”é‡
            - ç›¸å…³æ€§ä¸å› æœå…³ç³»åˆ†æ
            - äººå£ç»Ÿè®¡å­¦å·®å¼‚
            - ç†è®ºå‡è®¾éªŒè¯ç»“æœ
            - å®é™…åº”ç”¨ä»·å€¼ä¸æ„ä¹‰
            """,
            'computer_science': """
            é’ˆå¯¹è®¡ç®—æœºç§‘å­¦é¢†åŸŸç»“æœï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
            - ç®—æ³•æ€§èƒ½æŒ‡æ ‡ä¸åŸºå‡†æ¯”è¾ƒ
            - è®¡ç®—å¤æ‚åº¦ä¸èµ„æºæ¶ˆè€—
            - å‡†ç¡®æ€§ä¸é²æ£’æ€§è¯„ä¼°
            - å¯æ‰©å±•æ€§ä¸å®ç”¨æ€§åˆ†æ
            - åˆ›æ–°æ€§ä¸æŠ€æœ¯è´¡çŒ®
            """,
        }
        return discipline_prompts.get(
            discipline, "è¯·æŒ‰ç…§è¯¥å­¦ç§‘çš„ç»“æœåˆ†æè§„èŒƒè¿›è¡Œåˆ†æã€‚"
        )

    def _parse_results_fallback(self, text: str, discipline: str) -> Dict[str, Any]:
        """ç»“æœåˆ†æå¤‡ç”¨è§£ææ–¹æ³•"""
        return {
            'discipline_identified': discipline,
            'main_results': [],
            'data_analysis': {
                'statistical_methods': [],
                'data_processing': [],
                'visualization': [],
                'quality_metrics': [],
            },
            'performance_evaluation': [],
            'key_findings': [],
            'result_interpretation': {
                'theoretical_implications': [],
                'practical_applications': [],
                'mechanism_explanation': [],
                'validation_evidence': [],
            },
            'limitations_and_discussion': {
                'methodological_limitations': [],
                'data_limitations': [],
                'future_work': [],
                'improvement_suggestions': [],
            },
        }

    def _extract_content_by_sections_disciplinary(
        self, text: str, sections: Dict[str, str], discipline: str
    ) -> Dict[str, Any]:
        """å­¦ç§‘ä¸“ä¸šåŒ–çš„å†…å®¹æå– - é‡æ–°è®¾è®¡ä»¥æ­£ç¡®æå–æ‘˜è¦å’Œå…³é”®è¯"""
        content_info = {
            'sections_count': len(sections),
            'discipline': discipline,
            'content_summary': f"è¯†åˆ«åˆ°{len(sections)}ä¸ªç« èŠ‚ï¼Œå­¦ç§‘ï¼š{discipline}",
        }

        # ä»æ‘˜è¦éƒ¨åˆ†æå–
        if 'abstract_cn' in sections:
            content_info['abstract_cn'] = self._clean_abstract(sections['abstract_cn'])
            print(f"   âœ… ä¸­æ–‡æ‘˜è¦: {len(content_info['abstract_cn'])} å­—ç¬¦")
        else:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            abstract_pattern = r'((?:ä¸­æ–‡)?æ‘˜\s*è¦[\s\S]{50,3000}?)(?=å…³é”®è¯|è‹±æ–‡æ‘˜è¦|ABSTRACT|ç¬¬ä¸€ç« |ç›®å½•)'
            match = re.search(abstract_pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                content_info['abstract_cn'] = self._clean_abstract(match.group(1))
                print(f"   âœ… ä¸­æ–‡æ‘˜è¦(æ­£åˆ™): {len(content_info['abstract_cn'])} å­—ç¬¦")
            else:
                content_info['abstract_cn'] = ""
                print("   âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡æ‘˜è¦")

        if 'abstract_en' in sections:
            content_info['abstract_en'] = self._clean_abstract(sections['abstract_en'])
            print(f"   âœ… è‹±æ–‡æ‘˜è¦: {len(content_info['abstract_en'])} å­—ç¬¦")
        else:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            abstract_pattern = r'((?:ABSTRACT|Abstract)[\s\S]{50,3000}?)(?=Keywords?|ä¸­æ–‡æ‘˜è¦|ç¬¬ä¸€ç« |ç›®å½•)'
            match = re.search(abstract_pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                content_info['abstract_en'] = self._clean_abstract(match.group(1))
                print(f"   âœ… è‹±æ–‡æ‘˜è¦(æ­£åˆ™): {len(content_info['abstract_en'])} å­—ç¬¦")
            else:
                content_info['abstract_en'] = ""
                print("   âš ï¸ æœªæ‰¾åˆ°è‹±æ–‡æ‘˜è¦")

        # ä»å…³é”®è¯éƒ¨åˆ†æå–
        if 'keywords_cn' in sections:
            keywords = self._extract_keywords(sections['keywords_cn'], 'chinese')
            content_info['keywords_cn'] = keywords
            print(f"   âœ… ä¸­æ–‡å…³é”®è¯: {keywords}")
        else:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            keywords_pattern = r'(å…³é”®è¯[ï¼š:\s]*[^\n\r]{5,200})'
            match = re.search(keywords_pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                keywords = self._extract_keywords(match.group(1), 'chinese')
                content_info['keywords_cn'] = keywords
                print(f"   âœ… ä¸­æ–‡å…³é”®è¯(æ­£åˆ™): {keywords}")
            else:
                content_info['keywords_cn'] = ""
                print("   âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å…³é”®è¯")

        if 'keywords_en' in sections:
            keywords = self._extract_keywords(sections['keywords_en'], 'english')
            content_info['keywords_en'] = keywords
            print(f"   âœ… è‹±æ–‡å…³é”®è¯: {keywords}")
        else:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            keywords_pattern = r'((?:Keywords?|KEY\s+WORDS?)[ï¼š:\s]*[^\n\r]{5,200})'
            match = re.search(keywords_pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                keywords = self._extract_keywords(match.group(1), 'english')
                content_info['keywords_en'] = keywords
                print(f"   âœ… è‹±æ–‡å…³é”®è¯(æ­£åˆ™): {keywords}")
            else:
                content_info['keywords_en'] = ""
                print("   âš ï¸ æœªæ‰¾åˆ°è‹±æ–‡å…³é”®è¯")

        # ä»è‡´è°¢éƒ¨åˆ†æå–
        if 'acknowledgement' in sections:
            acknowledgement_content = sections['acknowledgement'].strip()
            content_info['acknowledgement'] = acknowledgement_content
            print(f"   âœ… è‡´è°¢: {len(acknowledgement_content)} å­—ç¬¦")
        else:
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œå°è¯•æ­£åˆ™åŒ¹é…
            ack_pattern = r'(è‡´\s*è°¢[\s\S]{50,2000}?)(?=é™„å½•|å‚è€ƒæ–‡çŒ®|$)'
            match = re.search(ack_pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                acknowledgement_content = match.group(1).strip()
                content_info['acknowledgement'] = acknowledgement_content
                print(f"   âœ… è‡´è°¢(æ­£åˆ™): {len(acknowledgement_content)} å­—ç¬¦")
            else:
                content_info['acknowledgement'] = ""
                print("   âš ï¸ æœªæ‰¾åˆ°è‡´è°¢")

        return content_info

    def _extract_references_enhanced_disciplinary(
        self, text: str, discipline: str, source_path: str = ''
    ) -> Dict[str, Any]:
        """å­¦ç§‘é€‚é…çš„æ™ºèƒ½å‚è€ƒæ–‡çŒ®è§£æ"""
        print("   ğŸ“š å¼€å§‹æ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®...")

        # ä½¿ç”¨æ™ºèƒ½å‚è€ƒæ–‡çŒ®æå–å™¨
        if self.smart_ref_extractor:
            # æ ¹æ®æ–‡ä»¶è·¯å¾„è‡ªåŠ¨æ£€æµ‹æ ¼å¼
            source_format = 'auto'
            if source_path:
                if source_path.lower().endswith('.pdf'):
                    source_format = 'pdf'
                elif source_path.lower().endswith(('.docx', '.doc')):
                    source_format = 'docx'

            print(f"   ğŸ” æ–‡æ¡£æ ¼å¼æ£€æµ‹: {source_format}")
            references_list, extraction_stats = (
                self.smart_ref_extractor.extract_references(
                    text, source_format=source_format, source_path=source_path
                )
            )

            print(f"   âœ… æ™ºèƒ½æå–å®Œæˆ: {len(references_list)} æ¡å‚è€ƒæ–‡çŒ®")
            print(f"   ğŸ“Š æå–æ–¹æ³•: {extraction_stats.get('method_used', 'unknown')}")
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {extraction_stats.get('processing_time', 0):.2f}ç§’")
        else:
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            print("   âš ï¸ æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            references_list = self._extract_references_enhanced(text)
            extraction_stats = {
                'method_used': 'ä¼ ç»Ÿæ­£åˆ™æå–',
                'total_found': len(references_list),
                'processing_time': 0.0,
                'success': len(references_list) > 0,
            }

        # æ„å»ºè¿”å›å­—å…¸
        references_dict = {
            'references': references_list,
            'discipline': discipline,
            'total_count': len(references_list),
            'extraction_stats': extraction_stats,
        }

        print(f"   ğŸ“‹ å‚è€ƒæ–‡çŒ®æå–å®Œæˆ: {len(references_list)} æ¡")
        return references_dict

    def _analyze_conclusion_with_ai(self, text: str) -> Dict[str, Any]:
        """åˆ†æç»“è®ºéƒ¨åˆ†"""
        if not self.ai_client:
            return {'conclusions': [], 'contributions': [], 'future_work': []}

        # æå–ç»“è®ºéƒ¨åˆ† - ä¼˜åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        conclusion_patterns = [
            # æ˜ç¡®çš„ç»“è®ºç« èŠ‚
            r'((?:ç»“è®º|æ€»ç»“|ç»“è®ºä¸å±•æœ›|æ€»ç»“ä¸å±•æœ›)[\s\S]{200,8000}?)(?=å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
            # ä»¥"æœ¬è®ºæ–‡ä¸»è¦ç ”ç©¶äº†"å¼€å§‹çš„ç»“è®º
            r'(æœ¬è®ºæ–‡ä¸»è¦ç ”ç©¶äº†[\s\S]{500,12000}?)(?=å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
            # ä»¥"ç ”ç©¶æ‰€å¾—åˆ°çš„ä¸»è¦ç»“è®º"å¼€å§‹
            r'(ç ”ç©¶æ‰€å¾—åˆ°çš„ä¸»è¦ç»“è®º[\s\S]{300,8000}?)(?=å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
            # é€šç”¨ç»“è®ºæ¨¡å¼
            r'((?:ä¸»è¦ç»“è®º|æœ¬æ–‡ç»“è®º|ç ”ç©¶ç»“è®º)[\s\S]{200,6000}?)(?=å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
        ]

        conclusion_content = ""
        for pattern in conclusion_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                conclusion_content = match.group(1)
                print(f"   âœ… æ‰¾åˆ°ç»“è®ºéƒ¨åˆ†ï¼Œé•¿åº¦: {len(conclusion_content)} å­—ç¬¦")
                break

        if not conclusion_content:
            print("   âš ï¸ æœªæ‰¾åˆ°ç»“è®ºéƒ¨åˆ†")
            return {'conclusions': [], 'contributions': [], 'future_work': []}

        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ç»“è®ºå†…å®¹ï¼Œè¿™å¯èƒ½åŒ…å«å¤šä¸ªæ®µè½å’Œç¼–å·åˆ—è¡¨ï¼š

{conclusion_content[:3000]}

è¯·ä»”ç»†æå–ï¼š
1. ä¸»è¦ç ”ç©¶ç»“è®º - æŸ¥æ‰¾ç¼–å·åˆ—è¡¨ä¸­çš„å…·ä½“ç»“è®ºç‚¹
2. å­¦æœ¯è´¡çŒ®å’Œåˆ›æ–°ç‚¹ - æŸ¥æ‰¾"çªç ´"ã€"åˆ›æ–°"ã€"è´¡çŒ®"ç­‰å…³é”®è¯
3. æœªæ¥å·¥ä½œå±•æœ› - æŸ¥æ‰¾"ä»Šåå·¥ä½œ"ã€"å±•æœ›"ã€"æœªæ¥"ç­‰å†…å®¹
4. ç ”ç©¶å±€é™æ€§ - å¦‚æœ‰æåŠ

æ³¨æ„ï¼šå³ä½¿æ²¡æœ‰æ˜ç¡®çš„ç« èŠ‚æ ‡é¢˜ï¼Œä¹Ÿè¦ä»å†…å®¹ä¸­æå–è¿™äº›ä¿¡æ¯ã€‚

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "conclusions": ["ç»“è®º1", "ç»“è®º2"],
    "contributions": ["è´¡çŒ®1", "è´¡çŒ®2"],
    "future_work": ["å±•æœ›1", "å±•æœ›2"],
    "limitations": ["å±€é™1", "å±€é™2"]
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                # ä¸‰çº§JSONè§£æç­–ç•¥
                content = response.content

                # ç¬¬ä¸€çº§ï¼šç›´æ¥JSONè§£æ
                json_content = content.strip()
                if '```json' in json_content:
                    json_start = json_content.find('```json')
                    json_end = json_content.find('```', json_start + 7)
                    if json_end > json_start:
                        json_content = json_content[json_start + 7 : json_end].strip()
                elif '```' in json_content:
                    parts = json_content.split('```')
                    if len(parts) >= 3:
                        json_content = parts[1].strip()

                try:
                    result = json.loads(json_content)
                    print(
                        f"   âœ… ç»“è®ºAIåˆ†ææˆåŠŸï¼Œæå–åˆ° {len(result.get('conclusions', []))} ä¸ªç»“è®º"
                    )
                    return result
                except json.JSONDecodeError:
                    print(f"   ğŸ“ ç¬¬ä¸€çº§JSONè§£æå¤±è´¥ï¼Œå°è¯•ç¬¬äºŒçº§æ¸…ç†è§£æ")

                    # ç¬¬äºŒçº§ï¼šæ¸…ç†åè§£æ
                    cleaned_json = _clean_json_content(json_content)
                    try:
                        result = json.loads(cleaned_json)
                        print(
                            f"   âœ… ç»“è®ºæ¸…ç†è§£ææˆåŠŸï¼Œæå–åˆ° {len(result.get('conclusions', []))} ä¸ªç»“è®º"
                        )
                        return result
                    except json.JSONDecodeError:
                        print(f"   âš ï¸ ç¬¬äºŒçº§æ¸…ç†è§£æå¤±è´¥")
                        return {
                            'conclusions': [],
                            'contributions': [],
                            'future_work': [],
                        }
        except Exception as e:
            print(f"   âš ï¸ ç»“è®ºAIåˆ†æå¤±è´¥: {e}")

        return {'conclusions': [], 'contributions': [], 'future_work': []}

    def _extract_references_enhanced(self, text: str) -> List[str]:
        """æ­¥éª¤4: ä½¿ç”¨AIå¤§æ¨¡å‹æ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®"""
        print("   ğŸ” å¯åŠ¨AIæ™ºèƒ½å‚è€ƒæ–‡çŒ®è§£æ...")

        # æ–¹æ³•1: å®šä½å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
        ref_text = self._locate_references_section(text)

        if not ref_text:
            print("   âš ï¸ æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†")
            return []

        print(f"   ğŸ“ æ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œé•¿åº¦: {len(ref_text)} å­—ç¬¦")

        # æ–¹æ³•2: ä½¿ç”¨AIæ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®æ¡ç›®
        references = self._extract_references_with_ai(ref_text)

        print(f"   âœ… AIæå–å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
        return references

    def _locate_references_section(self, text: str) -> str:
        """å®šä½å‚è€ƒæ–‡çŒ®éƒ¨åˆ†"""
        # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®æ ‡é¢˜çš„å¤šç§æ¨¡å¼
        ref_patterns = [
            # åŒ¹é… "å‚è€ƒæ–‡çŒ®" å¼€å§‹ç›´åˆ°ä¸‹ä¸€ä¸ªä¸»è¦ç« èŠ‚
            r'(?:^|\n)(?:##\s*)?å‚è€ƒæ–‡çŒ®\s*\n([\s\S]*?)(?=\n\s*(?:ç¼©ç•¥è¯è¡¨|æ–‡çŒ®ç»¼è¿°|è‡´è°¢ä¸å£°æ˜|è‡´è°¢|é™„å½•|ä½œè€…ç®€ä»‹|ä¸ªäººç®€å†)|$)',
            # åŒ¹é…è‹±æ–‡ References
            r'(?:^|\n)(?:##\s*)?REFERENCES?\s*\n([\s\S]*?)(?=\n\s*(?:ACKNOWLEDGMENT|APPENDIX|æ–‡çŒ®ç»¼è¿°|è‡´è°¢ä¸å£°æ˜|è‡´è°¢)|$)',
            # åŒ¹é… Bibliography
            r'(?:^|\n)(?:##\s*)?Bibliography\s*\n([\s\S]*?)(?=\n\s*(?:ACKNOWLEDGMENT|APPENDIX|æ–‡çŒ®ç»¼è¿°|è‡´è°¢ä¸å£°æ˜|è‡´è°¢)|$)',
            # æ›´å®½æ³›çš„å‚è€ƒæ–‡çŒ®æ¨¡å¼
            r'å‚è€ƒæ–‡çŒ®[\s\S]*?ä¸€ã€å¤ç±([\s\S]*?)(?=è‡´è°¢ä¸å£°æ˜|è‡´è°¢|ä¸ªäººç®€å†|$)',
        ]

        for pattern in ref_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            if matches:
                best_match = max(matches, key=lambda m: len(m.group(1)))
                ref_text = best_match.group(1).strip()
                # å‚è€ƒæ–‡çŒ®åº”è¯¥æœ‰åˆç†çš„é•¿åº¦
                if len(ref_text) >= 200:
                    return ref_text

        # å¤‡ç”¨æ–¹æ³•ï¼šå…³é”®è¯å®šä½
        ref_keywords = ['å‚è€ƒæ–‡çŒ®', 'References', 'REFERENCES', 'Bibliography']
        for keyword in ref_keywords:
            pos = text.find(keyword)
            if pos != -1:
                remaining_text = text[pos + len(keyword) :]
                end_markers = [
                    'è‡´è°¢ä¸å£°æ˜',
                    'ç¼©ç•¥è¯è¡¨',
                    'æ–‡çŒ®ç»¼è¿°',
                    'è‡´è°¢',
                    'ACKNOWLEDGMENT',
                    'APPENDIX',
                    'é™„å½•',
                    'ä½œè€…ç®€ä»‹',
                    'ä¸ªäººç®€å†',
                ]
                end_pos = len(remaining_text)

                for marker in end_markers:
                    marker_pos = remaining_text.find(marker)
                    if marker_pos != -1 and marker_pos < end_pos:
                        end_pos = marker_pos

                ref_text = remaining_text[:end_pos]
                if len(ref_text) >= 500:  # æ¢å¤åˆç†çš„æœ€å°é•¿åº¦è¦æ±‚
                    return ref_text

        return ""

    def _extract_references_with_ai(self, ref_text: str) -> List[str]:
        """ä½¿ç”¨AIå¤§æ¨¡å‹æå–å‚è€ƒæ–‡çŒ®æ¡ç›®"""
        try:
            # é™åˆ¶è¾“å…¥é•¿åº¦ä»¥é¿å…tokenè¶…é™
            max_length = 50000  # çº¦50kå­—ç¬¦ï¼Œå¯¹åº”å¤§çº¦12-15k tokens
            if len(ref_text) > max_length:
                print(
                    f"   ğŸ“ å‚è€ƒæ–‡çŒ®å†…å®¹è¿‡é•¿({len(ref_text)}å­—ç¬¦)ï¼Œæˆªå–å‰{max_length}å­—ç¬¦"
                )
                ref_text = ref_text[:max_length]

            prompt = f"""è¯·ä»ä»¥ä¸‹å‚è€ƒæ–‡çŒ®æ–‡æœ¬ä¸­æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªå‚è€ƒæ–‡çŒ®æ¡ç›®åº”è¯¥æ˜¯å®Œæ•´çš„ä¸€æ¡è®°å½•
2. ä¿æŒåŸæœ‰çš„ç¼–å·æ ¼å¼ï¼ˆå¦‚ï¼»1ï¼½ã€[1]ã€1.ç­‰ï¼‰
3. æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
4. æ¯æ¡å‚è€ƒæ–‡çŒ®åº”è¯¥åŒ…å«ï¼šä½œè€…ã€æ ‡é¢˜ã€æœŸåˆŠ/ä¼šè®®/å‡ºç‰ˆç¤¾ã€å¹´ä»½ç­‰ä¿¡æ¯
5. å¦‚æœæ ¼å¼æ··ä¹±ï¼Œè¯·æ™ºèƒ½é‡ç»„æˆæ ‡å‡†æ ¼å¼
6. æŒ‰ç¼–å·é¡ºåºæ’åˆ—
7. è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸€æ¡å‚è€ƒæ–‡çŒ®ï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜

å‚è€ƒæ–‡çŒ®æ–‡æœ¬ï¼š
{ref_text}

è¯·æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼š"""

            if hasattr(self, 'ai_client') and self.ai_client:
                print("   ğŸ¤– è°ƒç”¨AIå¤§æ¨¡å‹æå–å‚è€ƒæ–‡çŒ®...")
                response = self.ai_client.send_message(prompt)

                if response and hasattr(response, 'content'):
                    content = response.content.strip()

                    # è§£æAIè¿”å›çš„ç»“æœ
                    references = []
                    lines = content.split('\n')

                    for line in lines:
                        line = line.strip()
                        if not line:
                            continue

                        # æ£€æŸ¥æ˜¯å¦åƒå‚è€ƒæ–‡çŒ®æ¡ç›®
                        if self._is_valid_reference(line):
                            # æ¸…ç†æ ¼å¼
                            cleaned_ref = re.sub(r'\s+', ' ', line).strip()
                            references.append(cleaned_ref)

                    print(f"   âœ… AIæˆåŠŸæå– {len(references)} æ¡å‚è€ƒæ–‡çŒ®")
                    return references
                else:
                    print("   âš ï¸ AIå“åº”ä¸ºç©º")
            else:
                print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨")

        except Exception as e:
            print(f"   âŒ AIæå–å¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
        print("   ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•...")
        return self._extract_references_fallback(ref_text)

    def _is_valid_reference(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å‚è€ƒæ–‡çŒ®æ¡ç›®"""
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(line) < 20:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–å·æ ¼å¼
        has_number = bool(
            re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.|ï¼ˆ\d+ï¼‰|\(\d+\))', line)
        )

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸåˆŠã€ä¼šè®®ã€å‡ºç‰ˆç¤¾ç­‰å…³é”®è¯
        has_publication = any(
            keyword in line
            for keyword in [
                'Journal',
                'Proceedings',
                'Conference',
                'IEEE',
                'ACM',
                'Optics',
                'æœŸåˆŠ',
                'ä¼šè®®',
                'å­¦æŠ¥',
                'å¤§å­¦å­¦æŠ¥',
                'è®ºæ–‡é›†',
                'å‡ºç‰ˆç¤¾',
            ]
        )

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
        has_year = bool(re.search(r'(?:19|20)\d{2}', line))

        # è‡³å°‘æ»¡è¶³ç¼–å·+å¹´ä»½ï¼Œæˆ–è€…ç¼–å·+å‡ºç‰ˆç‰©
        return has_number and (has_year or has_publication)

    def _extract_references_fallback(self, ref_text: str) -> List[str]:
        """å¤‡ç”¨çš„å‚è€ƒæ–‡çŒ®æå–æ–¹æ³•"""
        references = []

        # æ™ºèƒ½æ®µè½åˆ†å‰²å’Œé‡ç»„
        print("   ğŸ”§ ä½¿ç”¨æ™ºèƒ½æ®µè½é‡ç»„æ–¹æ³•...")

        # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
        paragraphs = re.split(r'\n\s*\n', ref_text)
        current_ref = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„å‚è€ƒæ–‡çŒ®å¼€å§‹
            if re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.)', para):
                # ä¿å­˜ä¹‹å‰çš„å‚è€ƒæ–‡çŒ®
                if current_ref and len(current_ref) > 20:
                    cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
                    references.append(cleaned_ref)

                # å¼€å§‹æ–°çš„å‚è€ƒæ–‡çŒ®
                current_ref = para
            else:
                # ç»§ç»­å½“å‰å‚è€ƒæ–‡çŒ®
                if current_ref:
                    current_ref += " " + para

        # æ·»åŠ æœ€åä¸€æ¡å‚è€ƒæ–‡çŒ®
        if current_ref and len(current_ref) > 20:
            cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
            references.append(cleaned_ref)

        # å¦‚æœæ®µè½æ–¹æ³•æ•ˆæœä¸å¥½ï¼Œå°è¯•è¡Œçº§å¤„ç†
        if len(references) < 5:
            print("   ğŸ”§ å°è¯•è¡Œçº§é‡ç»„æ–¹æ³•...")
            references = self._extract_references_line_based(ref_text)

        return references[:100]  # é™åˆ¶æ•°é‡

    def _extract_references_line_based(self, ref_text: str) -> List[str]:
        """åŸºäºè¡Œçš„å‚è€ƒæ–‡çŒ®æå–æ–¹æ³•"""
        references = []
        lines = ref_text.split('\n')
        current_ref = ""

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯å‚è€ƒæ–‡çŒ®ç¼–å·è¡Œ
            if re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.)', line):
                # ä¿å­˜ä¹‹å‰çš„å‚è€ƒæ–‡çŒ®
                if current_ref and len(current_ref) > 20:
                    cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
                    if self._is_valid_reference(cleaned_ref):
                        references.append(cleaned_ref)

                # å¼€å§‹æ–°çš„å‚è€ƒæ–‡çŒ®
                current_ref = line
            else:
                # ç»§ç»­å½“å‰å‚è€ƒæ–‡çŒ®
                if current_ref:
                    current_ref += " " + line
                elif any(
                    keyword in line
                    for keyword in ['Journal', 'IEEE', 'Proceedings', 'å­¦æŠ¥']
                ):
                    # å¯èƒ½æ˜¯å‚è€ƒæ–‡çŒ®çš„ä¸€éƒ¨åˆ†
                    current_ref = line

        # æ·»åŠ æœ€åä¸€æ¡å‚è€ƒæ–‡çŒ®
        if current_ref and len(current_ref) > 20:
            cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
            if self._is_valid_reference(cleaned_ref):
                references.append(cleaned_ref)

        return references

    def _intelligent_repair_and_validate(
        self,
        metadata: Dict,
        content_info: Dict,
        references: List[str],
        toc_analysis: Optional[Dict[str, Any]] = None,
        original_text: str = '',
        ai_analysis: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æ­¥éª¤5: å¤šå±‚æ¬¡éªŒè¯å’Œé”™è¯¯ä¿®æ­£æœºåˆ¶"""
        print("   ğŸ§  å¯åŠ¨æ™ºèƒ½ä¿®å¤ç³»ç»Ÿ...")

        # åˆå¹¶æ‰€æœ‰æå–çš„ä¿¡æ¯
        result = {}
        result.update(metadata)
        result.update(content_info)
        result['references'] = references

        # æ·»åŠ ç›®å½•åˆ†æç»“æœ
        if toc_analysis:
            result.update(toc_analysis)
            print(
                f"   âœ… é›†æˆç›®å½•åˆ†æ: {len(toc_analysis.get('table_of_contents', []))} ä¸ªç« èŠ‚"
            )

        # æ·»åŠ AIæ™ºèƒ½åˆ†æç»“æœ
        if ai_analysis:
            result['ai_analysis'] = ai_analysis
            section_count = len(ai_analysis.get('section_analysis', {}))
            overall_score = ai_analysis.get('content_quality', {}).get(
                'overall_quality_score', 0
            )
            print(
                f"   ğŸ¤– é›†æˆAIåˆ†æ: {section_count} ä¸ªç« èŠ‚åˆ†æï¼Œæ•´ä½“è´¨é‡è¯„åˆ†: {overall_score}"
            )

            # æå–AIåˆ†æçš„å…³é”®æ´å¯Ÿ
            if 'section_analysis' in ai_analysis:
                result['ai_insights'] = self._extract_ai_insights(
                    ai_analysis['section_analysis']
                )
                print(f"   ğŸ’¡ æå–AIæ´å¯Ÿ: {len(result['ai_insights'])} æ¡å»ºè®®")

        # æå–ç¼ºå¤±çš„theoretical_frameworkå’Œauthor_contributionså­—æ®µ
        full_text = (
            original_text
            or result.get('original_text', '')
            or (
                metadata.get('original_text', '')
                or content_info.get('original_text', '')
            )
        )
        if full_text:
            print("   ğŸ” æå–ç†è®ºæ¡†æ¶å’Œä½œè€…è´¡çŒ®...")

            # æå–ç†è®ºæ¡†æ¶
            if not result.get('theoretical_framework'):
                theoretical_framework = self._extract_theoretical_framework(full_text)
                if theoretical_framework:
                    result['theoretical_framework'] = theoretical_framework
                    print(
                        f"   âœ… ç†è®ºæ¡†æ¶æå–: {len(str(theoretical_framework)[:100])}å­—ç¬¦"
                    )

            # æå–ä½œè€…è´¡çŒ®
            if not result.get('author_contributions'):
                author_contributions = self._extract_author_contributions(full_text)
                if author_contributions:
                    result['author_contributions'] = author_contributions
                    print(
                        f"   âœ… ä½œè€…è´¡çŒ®æå–: {len(str(author_contributions)[:100])}å­—ç¬¦"
                    )

        # å­—æ®µéªŒè¯å’Œæ¸…ç†
        result = self._validate_and_clean_fields(result)

        print("   âœ… æ™ºèƒ½ä¿®å¤å®Œæˆ")
        return result

    def _clean_abstract(self, text: str) -> str:
        """æ¸…ç†æ‘˜è¦æ–‡æœ¬"""
        # ç§»é™¤æ ‡é¢˜
        text = re.sub(r'^(ä¸­æ–‡æ‘˜è¦|æ‘˜è¦|ABSTRACT|Abstract)[ï¼š:\s]*', '', text)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _extract_keywords(self, text: str, language: str) -> str:
        """æå–å…³é”®è¯"""
        # ç§»é™¤æ ‡é¢˜
        text = re.sub(
            r'^(å…³é”®è¯|Keywords?|KEY\s+WORDS?)[ï¼š:\s]*', '', text, flags=re.IGNORECASE
        )
        # æ¸…ç†å’Œæ ¼å¼åŒ–
        keywords = re.split(r'[,ï¼Œ;ï¼›\s]+', text)
        keywords = [kw.strip() for kw in keywords if kw.strip()]
        
        # æ ¹æ®è¯­è¨€ä½¿ç”¨åˆé€‚çš„åˆ†éš”ç¬¦
        if language == 'chinese':
            return 'ï¼›'.join(keywords)  # ä¸­æ–‡ä½¿ç”¨åˆ†å·
        else:
            return '; '.join(keywords)  # è‹±æ–‡ä½¿ç”¨åˆ†å·+ç©ºæ ¼

    def _extract_theoretical_framework(self, text: str) -> Dict[str, Any]:
        """æå–ç†è®ºæ¡†æ¶ä¿¡æ¯"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self.ai_client:
            logger.warning("ç†è®ºæ¡†æ¶æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼šAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            print("   âš ï¸ ç†è®ºæ¡†æ¶æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼šAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return {
                'core_theories': [],
                'theoretical_models': [],
                'conceptual_foundations': [],
                'theoretical_contributions': [],
            }

        # ç†è®ºæ¡†æ¶ç›¸å…³çš„ç« èŠ‚æ¨¡å¼
        theory_patterns = [
            r'((?:ç†è®ºåŸºç¡€|ç†è®ºæ¡†æ¶|ç›¸å…³ç†è®º|åŸºç¡€ç†è®º)[\s\S]{300,8000}?)(?=ç¬¬|ç« èŠ‚|å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
            r'((?:æ–‡çŒ®ç»¼è¿°|å›½å†…å¤–ç ”ç©¶ç°çŠ¶)[\s\S]{500,10000}?)(?=ç¬¬|ç« èŠ‚|å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
            r'((?:æ¦‚å¿µæ¨¡å‹|ç†è®ºæ¨¡å‹|æ•°å­¦æ¨¡å‹)[\s\S]{200,5000}?)(?=ç¬¬|ç« èŠ‚|å‚è€ƒæ–‡çŒ®|è‡´è°¢|é™„å½•|$)',
        ]

        theory_content = ""
        for pattern in theory_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                content = match.group(1)
                if len(content) > len(theory_content):
                    theory_content = content

        if not theory_content:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸“é—¨çš„ç†è®ºç« èŠ‚ï¼Œä»æ–‡çŒ®ç»¼è¿°ä¸­æå–
            literature_patterns = [
                r'(æ–‡çŒ®ç»¼è¿°[\s\S]{1000,15000}?)(?=ç¬¬|ç« èŠ‚|ç ”ç©¶æ–¹æ³•|å®éªŒ|å‚è€ƒæ–‡çŒ®)',
                r'(ç›¸å…³å·¥ä½œ[\s\S]{1000,10000}?)(?=ç¬¬|ç« èŠ‚|ç ”ç©¶æ–¹æ³•|å®éªŒ|å‚è€ƒæ–‡çŒ®)',
                r'(å›½å†…å¤–ç ”ç©¶ç°çŠ¶[\s\S]{1000,12000}?)(?=ç¬¬|ç« èŠ‚|ç ”ç©¶æ–¹æ³•|å®éªŒ|å‚è€ƒæ–‡çŒ®)',
            ]

            for pattern in literature_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    theory_content = match.group(1)
                    break

        if theory_content:
            # ä½¿ç”¨AIåˆ†æç†è®ºæ¡†æ¶
            framework_analysis = self._analyze_theoretical_framework_with_ai(
                theory_content
            )
            # æ£€æŸ¥æ˜¯å¦è·å¾—äº†æœ‰æ•ˆçš„åˆ†æç»“æœ
            if framework_analysis and any(
                framework_analysis.get(key)
                for key in [
                    'core_theories',
                    'theoretical_models',
                    'conceptual_foundations',
                    'theoretical_contributions',
                ]
            ):
                return framework_analysis

        # AIåˆ†æå¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶è¿”å›ç©ºå€¼
        logger.warning("ç†è®ºæ¡†æ¶æ™ºèƒ½æå–å¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹æˆ–AIåˆ†æå¤±è´¥")
        print("   âš ï¸ ç†è®ºæ¡†æ¶æ™ºèƒ½æå–å¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹æˆ–AIåˆ†æå¤±è´¥")
        return {
            'core_theories': [],
            'theoretical_models': [],
            'conceptual_foundations': [],
            'theoretical_contributions': [],
        }

    def _analyze_theoretical_framework_with_ai(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†æç†è®ºæ¡†æ¶"""
        try:
            if not self.ai_client:
                logger.warning("ç†è®ºæ¡†æ¶AIåˆ†æå¤±è´¥ï¼šAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
                print("   âš ï¸ ç†è®ºæ¡†æ¶AIåˆ†æå¤±è´¥ï¼šAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
                return {
                    'core_theories': [],
                    'theoretical_models': [],
                    'conceptual_foundations': [],
                    'theoretical_contributions': [],
                }

            prompt = f"""
è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸­çš„ç†è®ºæ¡†æ¶ï¼Œè¯†åˆ«æ ¸å¿ƒç†è®ºã€æ¨¡å‹å’Œæ¦‚å¿µåŸºç¡€ï¼š

{content[:3000]}

è¯·æå–ï¼š
1. æ ¸å¿ƒç†è®º - è®ºæ–‡åŸºäºçš„ä¸»è¦ç†è®º
2. ç†è®ºæ¨¡å‹ - ä½¿ç”¨çš„ç†è®ºæ¨¡å‹æˆ–æ¡†æ¶
3. æ¦‚å¿µåŸºç¡€ - é‡è¦çš„æ¦‚å¿µå’Œå®šä¹‰
4. ç†è®ºè´¡çŒ® - è®ºæ–‡åœ¨ç†è®ºæ–¹é¢çš„è´¡çŒ®

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "core_theories": ["ç†è®º1", "ç†è®º2"],
    "theoretical_models": ["æ¨¡å‹1", "æ¨¡å‹2"],
    "conceptual_foundations": ["æ¦‚å¿µ1", "æ¦‚å¿µ2"],
    "theoretical_contributions": ["è´¡çŒ®1", "è´¡çŒ®2"]
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                content = response.content

                # ä¸‰çº§JSONè§£æç­–ç•¥
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        return result
                    except json.JSONDecodeError:
                        # æ¸…ç†åè§£æ
                        cleaned_json = _clean_json_content(json_match.group())
                        try:
                            result = json.loads(cleaned_json)
                            return result
                        except json.JSONDecodeError:
                            logger.warning("ç†è®ºæ¡†æ¶AIåˆ†æï¼šJSONè§£æå¤±è´¥")
                            print("   âš ï¸ ç†è®ºæ¡†æ¶AIåˆ†æï¼šJSONè§£æå¤±è´¥")
                            return {
                                'core_theories': [],
                                'theoretical_models': [],
                                'conceptual_foundations': [],
                                'theoretical_contributions': [],
                            }
                else:
                    logger.warning("ç†è®ºæ¡†æ¶AIåˆ†æï¼šæœªæ‰¾åˆ°æœ‰æ•ˆJSONæ ¼å¼")
                    print("   âš ï¸ ç†è®ºæ¡†æ¶AIåˆ†æï¼šæœªæ‰¾åˆ°æœ‰æ•ˆJSONæ ¼å¼")
                    return {
                        'core_theories': [],
                        'theoretical_models': [],
                        'conceptual_foundations': [],
                        'theoretical_contributions': [],
                    }
        except Exception as e:
            logger.error(f"ç†è®ºæ¡†æ¶AIåˆ†æå¤±è´¥: {e}")
            print(f"   âš ï¸ ç†è®ºæ¡†æ¶AIåˆ†æå¤±è´¥: {e}")

        return {
            'core_theories': [],
            'theoretical_models': [],
            'conceptual_foundations': [],
            'theoretical_contributions': [],
        }

    def _extract_author_contributions(self, text: str) -> Dict[str, Any]:
        """æå–ä½œè€…è´¡çŒ®ä¿¡æ¯"""
        # æ£€æŸ¥AIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self.ai_client:
            logger.warning("ä½œè€…è´¡çŒ®æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼šAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            print("   âš ï¸ ä½œè€…è´¡çŒ®æ™ºèƒ½æå–å™¨ä¸å¯ç”¨ï¼šAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return {
                'contribution_statement': '',
                'research_contributions': [],
                'publication_contributions': [],
                'innovation_points': [],
            }

        # ä½œè€…è´¡çŒ®å£°æ˜çš„å¸¸è§ä½ç½®æ¨¡å¼
        contribution_patterns = [
            r'((?:æ”»è¯».*?å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ|ä¸ªäººè´¡çŒ®|ä½œè€…è´¡çŒ®|ç ”ç©¶æˆæœ)[\s\S]{200,5000}?)(?=è‡´è°¢|å‚è€ƒæ–‡çŒ®|é™„å½•|$)',
            r'((?:å‘è¡¨è®ºæ–‡|ç ”ç©¶æˆæœ|å­¦æœ¯è®ºæ–‡|ä¸“åˆ©ç”³è¯·)[\s\S]{300,3000}?)(?=è‡´è°¢|å‚è€ƒæ–‡çŒ®|é™„å½•|$)',
            r'((?:åˆ›æ–°ç‚¹|åˆ›æ–°æ€§|ä¸»è¦è´¡çŒ®|ç ”ç©¶è´¡çŒ®)[\s\S]{200,2000}?)(?=è‡´è°¢|å‚è€ƒæ–‡çŒ®|é™„å½•|ç¬¬|ç« èŠ‚|$)',
        ]

        contribution_content = ""
        for pattern in contribution_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                content = match.group(1)
                if len(content) > len(contribution_content):
                    contribution_content = content

        if contribution_content:
            # ä½¿ç”¨AIåˆ†æä½œè€…è´¡çŒ®
            contribution_analysis = self._analyze_author_contributions_with_ai(
                contribution_content
            )
            # æ£€æŸ¥æ˜¯å¦è·å¾—äº†æœ‰æ•ˆçš„åˆ†æç»“æœ
            if contribution_analysis and (
                contribution_analysis.get('contribution_statement')
                or any(
                    contribution_analysis.get(key)
                    for key in [
                        'research_contributions',
                        'publication_contributions',
                        'innovation_points',
                    ]
                )
            ):
                return contribution_analysis

        # AIåˆ†æå¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶è¿”å›ç©ºå€¼
        logger.warning("ä½œè€…è´¡çŒ®æ™ºèƒ½æå–å¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹æˆ–AIåˆ†æå¤±è´¥")
        print("   âš ï¸ ä½œè€…è´¡çŒ®æ™ºèƒ½æå–å¤±è´¥ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆå†…å®¹æˆ–AIåˆ†æå¤±è´¥")
        return {
            'contribution_statement': '',
            'research_contributions': [],
            'publication_contributions': [],
            'innovation_points': [],
        }

    def _analyze_author_contributions_with_ai(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†æä½œè€…è´¡çŒ®"""
        try:
            if not self.ai_client:
                logger.warning("ä½œè€…è´¡çŒ®AIåˆ†æå¤±è´¥ï¼šAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
                print("   âš ï¸ ä½œè€…è´¡çŒ®AIåˆ†æå¤±è´¥ï¼šAIå®¢æˆ·ç«¯ä¸å¯ç”¨")
                return {
                    'contribution_statement': '',
                    'research_contributions': [],
                    'publication_contributions': [],
                    'innovation_points': [],
                }

            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ä½œè€…è´¡çŒ®å†…å®¹ï¼Œæå–ç ”ç©¶æˆæœå’Œåˆ›æ–°ç‚¹ï¼š

{content[:2000]}

è¯·æå–ï¼š
1. è´¡çŒ®å£°æ˜ - æ€»ä½“è´¡çŒ®æè¿°
2. ç ”ç©¶è´¡çŒ® - å…·ä½“ç ”ç©¶æ–¹é¢çš„è´¡çŒ®
3. å‘è¡¨è´¡çŒ® - è®ºæ–‡å‘è¡¨å’Œä¸“åˆ©ç­‰
4. åˆ›æ–°ç‚¹ - ä¸»è¦åˆ›æ–°ç‚¹

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
    "contribution_statement": "æ€»ä½“è´¡çŒ®æè¿°",
    "research_contributions": ["ç ”ç©¶è´¡çŒ®1", "ç ”ç©¶è´¡çŒ®2"],
    "publication_contributions": ["å‘è¡¨1", "å‘è¡¨2"],
    "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2"]
}}
"""

            response = self.ai_client.send_message(prompt)
            if response and hasattr(response, 'content'):
                content = response.content

                # ä¸‰çº§JSONè§£æç­–ç•¥
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group())
                        return result
                    except json.JSONDecodeError:
                        # æ¸…ç†åè§£æ
                        cleaned_json = _clean_json_content(json_match.group())
                        try:
                            result = json.loads(cleaned_json)
                            return result
                        except json.JSONDecodeError:
                            logger.warning("ä½œè€…è´¡çŒ®AIåˆ†æï¼šJSONè§£æå¤±è´¥")
                            print("   âš ï¸ ä½œè€…è´¡çŒ®AIåˆ†æï¼šJSONè§£æå¤±è´¥")
                            return {
                                'contribution_statement': '',
                                'research_contributions': [],
                                'publication_contributions': [],
                                'innovation_points': [],
                            }
                else:
                    logger.warning("ä½œè€…è´¡çŒ®AIåˆ†æï¼šæœªæ‰¾åˆ°æœ‰æ•ˆJSONæ ¼å¼")
                    print("   âš ï¸ ä½œè€…è´¡çŒ®AIåˆ†æï¼šæœªæ‰¾åˆ°æœ‰æ•ˆJSONæ ¼å¼")
                    return {
                        'contribution_statement': '',
                        'research_contributions': [],
                        'publication_contributions': [],
                        'innovation_points': [],
                    }
        except Exception as e:
            logger.error(f"ä½œè€…è´¡çŒ®AIåˆ†æå¤±è´¥: {e}")
            print(f"   âš ï¸ ä½œè€…è´¡çŒ®AIåˆ†æå¤±è´¥: {e}")

        return {
            'contribution_statement': '',
            'research_contributions': [],
            'publication_contributions': [],
            'innovation_points': [],
        }

    def _validate_and_clean_fields(self, result: Dict) -> Dict[str, Any]:
        """éªŒè¯å’Œæ¸…ç†å­—æ®µ"""
        # ç¡®ä¿æ‰€æœ‰æ ‡å‡†å­—æ®µéƒ½å­˜åœ¨
        for field in self.standard_fields:
            if field not in result:
                result[field] = ""

        # æ¸…ç†å­—æ®µå†…å®¹
        for field, value in result.items():
            if isinstance(value, str):
                # ç§»é™¤æ§åˆ¶å­—ç¬¦
                value = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', value)
                # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
                value = re.sub(r'\s+', ' ', value)
                result[field] = value.strip()

        return result

    def _calculate_extraction_stats(self, result: Dict, processing_time: float):
        """è®¡ç®—æå–ç»Ÿè®¡ä¿¡æ¯"""
        extracted_count = 0
        for field in self.standard_fields:
            value = result.get(field, '')
            # å¤„ç†ä¸åŒç±»å‹çš„å€¼
            if isinstance(value, list):
                # åˆ—è¡¨ç±»å‹ï¼šæ£€æŸ¥æ˜¯å¦éç©º
                if value and any(str(item).strip() for item in value):
                    extracted_count += 1
            elif isinstance(value, str):
                # å­—ç¬¦ä¸²ç±»å‹ï¼šæ£€æŸ¥æ˜¯å¦éç©ºç™½
                if value.strip():
                    extracted_count += 1
            elif value:
                # å…¶ä»–éç©ºå€¼
                extracted_count += 1

        self.extraction_stats.update(
            {
                'extracted_fields': extracted_count,
                'confidence': min(1.0, extracted_count / len(self.standard_fields)),
                'processing_time': processing_time,
            }
        )

    def _generate_extraction_report(
        self, result: Dict, file_path: Optional[str], processing_time: float
    ):
        """ç”Ÿæˆæå–æŠ¥å‘Š"""
        print(f"\nâ±ï¸ æå–å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸“ä¸šç‰ˆæå–æŠ¥å‘Š")
        print("=" * 60)

        if file_path:
            print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {Path(file_path).name}")

        print(f"â±ï¸ æå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(
            f"ğŸ¯ ä½¿ç”¨æŠ€æœ¯: åˆ†æ­¥æŠ½å–, ç»“æ„åŒ–åˆ†æ, å¿«é€Ÿå®šä½, æ­£åˆ™åŒ¹é…, å‚è€ƒæ–‡çŒ®è§£æ, æ™ºèƒ½ä¿®å¤"
        )
        print(f"ğŸ“ˆ æ€»å­—æ®µæ•°: {self.extraction_stats['total_fields']}")
        print(f"âœ… å·²æå–: {self.extraction_stats['extracted_fields']} ä¸ªå­—æ®µ")
        print(f"ğŸ“Š å®Œæ•´åº¦: {self.extraction_stats['confidence']:.1%}")
        print(f"ğŸ–ï¸ ç½®ä¿¡åº¦: {self.extraction_stats['confidence']:.2f}")

        # è´¨é‡è¯„åˆ†
        quality_score = (
            self.extraction_stats['confidence'] * 0.6  # å®Œæ•´åº¦æƒé‡60%
            + (1.0 if result.get('title_cn') else 0.0) * 0.2  # æ ‡é¢˜æƒé‡20%
            + (1.0 if len(result.get('references', [])) > 10 else 0.0)
            * 0.2  # å‚è€ƒæ–‡çŒ®æƒé‡20%
        )
        print(f"â­ è´¨é‡åˆ†æ•°: {quality_score:.2f}")
        print("=" * 60)
        print("âœ… ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–å®Œæˆï¼")

    def _analyze_review_chapters_concurrently(
        self, text: str, review_chapters: List[Dict]
    ) -> Dict[str, Any]:
        """å¹¶å‘åˆ†æç»¼è¿°ç« èŠ‚"""
        import concurrent.futures
        import time

        results = {'chapter_summaries': {}, 'literature_analysis': {}}

        if not self.ai_client or not review_chapters:
            return results

        max_workers = min(2, len(review_chapters))  # ç»¼è¿°ç« èŠ‚é€šå¸¸è¾ƒå°‘ï¼Œé™åˆ¶å¹¶å‘æ•°
        print(f"   ğŸ”„ å¯åŠ¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘åˆ†æç»¼è¿°ç« èŠ‚...")

        def analyze_review_chapter(chapter):
            """åˆ†æå•ä¸ªç»¼è¿°ç« èŠ‚"""
            chapter_title = chapter.get('title', 'Unknown')
            try:
                print(f"      ğŸ“– [{chapter_title[:20]}] å¼€å§‹ç»¼è¿°åˆ†æ...")
                start_time = time.time()

                chapter_content = self._extract_chapter_content(text, chapter)
                if not chapter_content or len(chapter_content) < 200:
                    print(f"      âš ï¸ [{chapter_title[:20]}] å†…å®¹ä¸è¶³ï¼Œè·³è¿‡")
                    return chapter_title, None, None

                # ç»¼åˆç»¼è¿°åˆ†æ
                summary = self._generate_review_chapter_analysis(chapter, chapter_content)

                # ç»¼è¿°æ·±åº¦åˆ†æ 
                review_analysis = self._conduct_comprehensive_review_analysis(chapter_content)

                elapsed = time.time() - start_time
                if summary or review_analysis:
                    print(f"      âœ… [{chapter_title[:20]}] ç»¼è¿°åˆ†æå®Œæˆ ({elapsed:.1f}s, {len(chapter_content)} å­—ç¬¦)")
                    return chapter_title, summary, review_analysis
                else:
                    print(f"      âš ï¸ [{chapter_title[:20]}] ç»¼è¿°åˆ†æå¤±è´¥ ({elapsed:.1f}s)")
                    return chapter_title, None, None

            except Exception as e:
                print(f"      âŒ [{chapter_title[:20]}] ç»¼è¿°åˆ†æå¼‚å¸¸: {e}")
                return chapter_title, None, None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            start_time = time.time()

            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_chapter = {
                executor.submit(analyze_review_chapter, chapter): chapter.get('title', 'Unknown')
                for chapter in review_chapters
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_chapter):
                chapter_title = future_to_chapter[future]
                try:
                    returned_title, summary, review_analysis = future.result(timeout=90)  # ç»¼è¿°ç« èŠ‚å¯èƒ½æ›´å¤æ‚ï¼Œç»™æ›´å¤šæ—¶é—´

                    if summary:
                        results['chapter_summaries'][returned_title] = summary
                    if review_analysis:
                        results['literature_analysis'][returned_title] = review_analysis

                except concurrent.futures.TimeoutError:
                    print(f"      â° [{chapter_title[:20]}] ç»¼è¿°åˆ†æè¶…æ—¶")
                except Exception as e:
                    print(f"      âŒ [{chapter_title[:20]}] å¹¶å‘æ‰§è¡Œå¼‚å¸¸: {e}")

            total_time = time.time() - start_time
            success_count = len(results['chapter_summaries'])
            print(
                f"   âš¡ ç»¼è¿°ç« èŠ‚å¹¶å‘åˆ†æå®Œæˆ: {success_count}/{len(review_chapters)} æˆåŠŸï¼Œæ€»è€—æ—¶ {total_time:.1f}s"
            )

        return results

    def _analyze_other_chapters_concurrently(
        self, text: str, chapters: List[Dict]
    ) -> Dict[str, Any]:
        """å¹¶å‘åˆ†æå…¶ä»–ç« èŠ‚"""
        import concurrent.futures
        import time

        results = {}

        if not self.ai_client or not chapters:
            return results

        max_workers = min(3, len(chapters))  # å…¶ä»–ç« èŠ‚å¯ä»¥æ›´å¤šå¹¶å‘
        print(f"   ğŸ”„ å¯åŠ¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘åˆ†æå…¶ä»–ç« èŠ‚...")

        def analyze_other_chapter(chapter):
            """åˆ†æå•ä¸ªéç»¼è¿°ç« èŠ‚"""
            chapter_title = chapter.get('title', 'Unknown')
            try:
                print(f"      ğŸ“ [{chapter_title[:20]}] å¼€å§‹å†…å®¹åˆ†æ...")
                start_time = time.time()

                chapter_content = self._extract_chapter_content(text, chapter)
                if not chapter_content or len(chapter_content) < 100:
                    print(f"      âš ï¸ [{chapter_title[:20]}] å†…å®¹ä¸è¶³ï¼Œè·³è¿‡")
                    return chapter_title, None

                # æ ¹æ®ç« èŠ‚ç±»å‹è¿›è¡Œä¸åŒçš„åˆ†æ
                chapter_type = self._classify_chapter_type(chapter, chapter_content)
                analysis = self._analyze_chapter_by_type(chapter, chapter_content, chapter_type)

                elapsed = time.time() - start_time
                if analysis:
                    print(f"      âœ… [{chapter_title[:20]}] å†…å®¹åˆ†æå®Œæˆ ({elapsed:.1f}s, {len(chapter_content)} å­—ç¬¦)")
                    return chapter_title, analysis
                else:
                    print(f"      âš ï¸ [{chapter_title[:20]}] å†…å®¹åˆ†æå¤±è´¥ ({elapsed:.1f}s)")
                    return chapter_title, None

            except Exception as e:
                print(f"      âŒ [{chapter_title[:20]}] å†…å®¹åˆ†æå¼‚å¸¸: {e}")
                return chapter_title, None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            start_time = time.time()

            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_chapter = {
                executor.submit(analyze_other_chapter, chapter): chapter.get('title', 'Unknown')
                for chapter in chapters
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_chapter):
                chapter_title = future_to_chapter[future]
                try:
                    returned_title, analysis = future.result(timeout=60)  # å…¶ä»–ç« èŠ‚60ç§’è¶…æ—¶

                    if analysis:
                        results[returned_title] = analysis

                except concurrent.futures.TimeoutError:
                    print(f"      â° [{chapter_title[:20]}] å†…å®¹åˆ†æè¶…æ—¶")
                except Exception as e:
                    print(f"      âŒ [{chapter_title[:20]}] å¹¶å‘æ‰§è¡Œå¼‚å¸¸: {e}")

            total_time = time.time() - start_time
            success_count = len(results)
            print(
                f"   âš¡ å…¶ä»–ç« èŠ‚å¹¶å‘åˆ†æå®Œæˆ: {success_count}/{len(chapters)} æˆåŠŸï¼Œæ€»è€—æ—¶ {total_time:.1f}s"
            )

        return results

    def _analyze_chapter_by_type(self, chapter: Dict, content: str, chapter_type: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®ç« èŠ‚ç±»å‹è¿›è¡Œä¸åŒçš„åˆ†æ"""
        try:
            if chapter_type == 'review':
                # ç»¼è¿°ç« èŠ‚ä½¿ç”¨ç»¼è¿°ä¸“ç”¨åˆ†æ
                return self._generate_review_chapter_analysis(chapter, content)
            elif chapter_type in ['methodology', 'method', 'approach']:
                # æ–¹æ³•ç« èŠ‚
                return self._generate_methodology_analysis(chapter, content)
            elif chapter_type in ['results', 'findings', 'analysis']:
                # ç»“æœç« èŠ‚
                return self._generate_results_analysis(chapter, content)
            elif chapter_type in ['conclusion', 'summary']:
                # ç»“è®ºç« èŠ‚
                return self._generate_conclusion_analysis(chapter, content)
            else:
                # å…¶ä»–ç« èŠ‚ä½¿ç”¨é€šç”¨åˆ†æ
                return self._generate_chapter_summary_with_ai(chapter, content)
        except Exception as e:
            print(f"ç« èŠ‚ç±»å‹åˆ†æå¤±è´¥: {e}")
            return None

    def _generate_methodology_analysis(self, chapter: Dict, content: str) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆæ–¹æ³•è®ºç« èŠ‚åˆ†æ"""
        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ–¹æ³•è®ºç« èŠ‚å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter.get('title', '')}
ç« èŠ‚å†…å®¹ï¼š
{content[:2000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. é‡‡ç”¨çš„ç ”ç©¶æ–¹æ³•
2. æŠ€æœ¯è·¯çº¿
3. å®éªŒè®¾è®¡
4. æ•°æ®æ¥æº
5. åˆ†æå·¥å…·

è¿”å›JSONæ ¼å¼ã€‚
"""
            response = self._send_ai_request(prompt)
            if response:
                return {'summary': response, 'analysis_type': 'methodology'}
            return None
        except Exception as e:
            print(f"æ–¹æ³•è®ºåˆ†æå¤±è´¥: {e}")
            return None

    def _generate_results_analysis(self, chapter: Dict, content: str) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆç»“æœç« èŠ‚åˆ†æ"""
        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç»“æœç« èŠ‚å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter.get('title', '')}
ç« èŠ‚å†…å®¹ï¼š
{content[:2000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦ç ”ç©¶å‘ç°
2. æ•°æ®åˆ†æç»“æœ
3. å…³é”®æŒ‡æ ‡
4. ç»“æœéªŒè¯
5. å‘ç°çš„è§„å¾‹

è¿”å›JSONæ ¼å¼ã€‚
"""
            response = self._send_ai_request(prompt)
            if response:
                return {'summary': response, 'analysis_type': 'results'}
            return None
        except Exception as e:
            print(f"ç»“æœåˆ†æå¤±è´¥: {e}")
            return None

    def _generate_conclusion_analysis(self, chapter: Dict, content: str) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆç»“è®ºç« èŠ‚åˆ†æ"""
        try:
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç»“è®ºç« èŠ‚å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

ç« èŠ‚æ ‡é¢˜ï¼š{chapter.get('title', '')}
ç« èŠ‚å†…å®¹ï¼š
{content[:2000]}...

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦ç»“è®º
2. ç ”ç©¶è´¡çŒ®
3. åˆ›æ–°ç‚¹
4. å±€é™æ€§
5. æœªæ¥å±•æœ›

è¿”å›JSONæ ¼å¼ã€‚
"""
            response = self._send_ai_request(prompt)
            if response:
                return {'summary': response, 'analysis_type': 'conclusion'}
            return None
        except Exception as e:
            print(f"ç»“è®ºåˆ†æå¤±è´¥: {e}")
            return None

    def _send_ai_request(self, prompt: str) -> Optional[str]:
        """å‘é€AIè¯·æ±‚çš„é€šç”¨æ–¹æ³•"""
        try:
            if not self.ai_client:
                return None
            
            response = self.ai_client.send_message(prompt)
            if response and response.content:
                return response.content.strip()
            return None
        except Exception as e:
            print(f"AIè¯·æ±‚å¤±è´¥: {e}")
            return None


# ä¿æŒå‘åå…¼å®¹çš„å…¨å±€å®ä¾‹
thesis_extractor_pro = ThesisExtractorPro()


def extract_text_from_word(file_path: str) -> str:
    """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
    try:
        doc = Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        logger.error(f"æå–Wordæ–‡æœ¬å¤±è´¥: {e}")
        return ""


def _extract_json_from_response(response_text: str) -> Optional[str]:
    """ä»AIå“åº”ä¸­æå–JSONå†…å®¹"""
    # ç§»é™¤ä»£ç å—æ ‡è®°
    content = re.sub(r'```json\s*', '', response_text)
    content = re.sub(r'```\s*$', '', content)

    # æŸ¥æ‰¾JSONå¯¹è±¡
    json_match = re.search(r'\{.*\}', content, re.DOTALL)
    if json_match:
        return json_match.group()

    logger.warning("æ— æ³•æ‰¾åˆ°JSONå¯¹è±¡")
    return None


def _clean_json_content(json_str: str) -> str:
    """æ¸…ç†JSONå†…å®¹ä¸­çš„æ§åˆ¶å­—ç¬¦"""
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    json_str = re.sub(r'\s+', ' ', json_str)
    return json_str.strip()


def _parse_json_with_fallback(json_str: str) -> Optional[Dict]:
    """ä½¿ç”¨å›é€€æœºåˆ¶è§£æJSON"""
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        logger.warning(f"ğŸ“ JSONè§£æå¤±è´¥: {e}ï¼Œå¯åŠ¨å¤‡ç”¨è§£ææœºåˆ¶")

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å…³é”®å­—æ®µ (ç§»é™¤å†—ä½™å­—æ®µï¼Œä½¿ç”¨snake_caseå’Œè¯­è¨€åç¼€)
        result = {}
        fields = [
            'title_cn',
            'keywords_cn',
            'abstract_cn',
            'title_en',
            'keywords_en',
            'abstract_en',
            'theoretical_framework',
            'references',
        ]

        for field in fields:
            pattern = rf'"{field}"\s*:\s*"([^"]*)"'
            match = re.search(pattern, json_str, re.DOTALL)
            if match:
                result[field] = match.group(1).strip()
            else:
                result[field] = ""

        extracted_fields = len([k for k, v in result.items() if v and str(v).strip()])
        logger.info(
            f"ğŸ“ å¤‡ç”¨è§£æå®Œæˆï¼ŒæˆåŠŸæå– {extracted_fields}/{len(fields)} ä¸ªå­—æ®µ"
        )
        return result if result else None


def save_extraction_cache(
    file_path: str, extracted_info: Dict, session_id: Optional[str] = None
) -> bool:
    """
    ä¿å­˜ç»“æ„åŒ–ä¿¡æ¯åˆ°ç¼“å­˜æ–‡ä»¶

    Args:
        file_path: åŸå§‹è®ºæ–‡æ–‡ä»¶è·¯å¾„
        extracted_info: æå–çš„ç»“æ„åŒ–ä¿¡æ¯
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼‰

    Returns:
        ä¿å­˜æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    try:
        from .config_manager import get_config_manager
        from pathlib import Path
        from datetime import datetime

        config_mgr = get_config_manager()
        input_path = Path(file_path)
        base_name = input_path.stem
        output_dir = Path(config_mgr.get_output_dir())
        output_dir.mkdir(parents=True, exist_ok=True)
        # åªä¿å­˜ä¸“å®¶ç‰ˆæ–‡ä»¶
        extracted_info_file = output_dir / f"{base_name}_pro_extracted_info.json"

        cache_data = {
            'extracted_info': extracted_info,
            'metadata': {
                'extraction_time': datetime.now().isoformat(),
                'file_path': str(file_path),
                'method': 'pro_strategy',  # æ ‡è®°ä¸ºä¸“å®¶ç­–ç•¥
                'extractor_version': '2.0',
                'session_id': session_id,
            },
        }

        with open(extracted_info_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ä¸“ä¸šç‰ˆç»“æ„åŒ–ä¿¡æ¯å·²ä¿å­˜åˆ°ç¼“å­˜: {extracted_info_file}")
        return True

    except Exception as e:
        logger.error(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False


def _split_text_intelligently(text: str, max_chunk_size: int = 32000) -> List[str]:
    """
    æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œé¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²

    Args:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        max_chunk_size: æ¯ä¸ªåˆ†å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤32000ï¼Œé€‚é…64Kä¸Šä¸‹æ–‡ï¼‰
        max_chunk_size: æ¯ä¸ªåˆ†å—çš„æœ€å¤§å­—ç¬¦æ•°

    Returns:
        åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if len(text) <= max_chunk_size:
        return [text]

    chunks = []
    current_pos = 0

    while current_pos < len(text):
        # è®¡ç®—å½“å‰å—çš„ç»“æŸä½ç½®
        end_pos = current_pos + max_chunk_size

        if end_pos >= len(text):
            # æœ€åä¸€å—
            chunks.append(text[current_pos:])
            break

        # æ‰¾åˆ°æœ€è¿‘çš„å¥å·ã€æ¢è¡Œæˆ–æ®µè½è¾¹ç•Œ
        search_start = max(current_pos, end_pos - 1000)  # åœ¨æœ€å1000å­—ç¬¦å†…æŸ¥æ‰¾

        # ä¼˜å…ˆåœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
        paragraph_break = text.rfind('\n\n', search_start, end_pos)
        if paragraph_break != -1 and paragraph_break > current_pos:
            chunks.append(text[current_pos:paragraph_break])
            current_pos = paragraph_break + 2
            continue

        # å…¶æ¬¡åœ¨å¥å·ååˆ†å‰²
        sentence_end = text.rfind('ã€‚', search_start, end_pos)
        if sentence_end != -1 and sentence_end > current_pos:
            chunks.append(text[current_pos : sentence_end + 1])
            current_pos = sentence_end + 1
            continue

        # æœ€ååœ¨æ¢è¡Œç¬¦åˆ†å‰²
        line_break = text.rfind('\n', search_start, end_pos)
        if line_break != -1 and line_break > current_pos:
            chunks.append(text[current_pos:line_break])
            current_pos = line_break + 1
            continue

        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œå¼ºåˆ¶åˆ†å‰²
        chunks.append(text[current_pos:end_pos])
        current_pos = end_pos

    logger.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆ: {len(text):,} å­—ç¬¦ -> {len(chunks)} ä¸ªåˆ†å—")
    for i, chunk in enumerate(chunks):
        logger.debug(f"åˆ†å— {i+1}: {len(chunk):,} å­—ç¬¦")

    return chunks


def _merge_extracted_info(chunk_results: List[Dict]) -> Dict:
    """
    åˆå¹¶å¤šä¸ªåˆ†å—çš„æå–ç»“æœ

    Args:
        chunk_results: å„åˆ†å—çš„æå–ç»“æœåˆ—è¡¨

    Returns:
        åˆå¹¶åçš„ç»“æœå­—å…¸
    """
    merged = {}

    # å®šä¹‰éœ€è¦åˆå¹¶çš„å­—æ®µ (ä½¿ç”¨snake_caseå’Œè¯­è¨€åç¼€)
    fields = [
        'title_cn',
        'keywords_cn',
        'abstract_cn',
        'title_en',
        'keywords_en',
        'abstract_en',
        'theoretical_framework',
        'references',
    ]

    for field in fields:
        values = []
        for result in chunk_results:
            if result and field in result and result[field]:
                # ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²ç±»å‹
                value = result[field]
                if isinstance(value, (list, tuple)):
                    value = ', '.join(str(v) for v in value if v)
                elif not isinstance(value, str):
                    value = str(value)

                if value.strip():
                    values.append(value.strip())

        if field in ['title_cn', 'title_en']:
            # æ ‡é¢˜å–ç¬¬ä¸€ä¸ªéç©ºå€¼
            merged[field] = values[0] if values else ""
        elif field in ['keywords_cn', 'keywords_en']:
            # å…³é”®è¯å»é‡åˆå¹¶
            all_keywords = []
            for value in values:
                keywords = [kw.strip() for kw in value.split(',') if kw.strip()]
                all_keywords.extend(keywords)
            unique_keywords = list(dict.fromkeys(all_keywords))  # ä¿æŒé¡ºåºå»é‡
            merged[field] = ', '.join(unique_keywords)
        else:
            # å…¶ä»–å­—æ®µåˆå¹¶å†…å®¹
            merged[field] = '\n\n'.join(values) if values else ""

    logger.info(f"åˆ†å—ç»“æœåˆå¹¶å®Œæˆï¼ŒåŒ…å«å­—æ®µ: {list(merged.keys())}")
    return merged


def extract_sections_with_ai(
    text: str,
    ai_client,
    session_id: Optional[str] = None,
    languages: Optional[List[str]] = None,
    use_sections: bool = True,
) -> Optional[Dict]:
    """
    ä½¿ç”¨AIæ¨¡å‹ä»è®ºæ–‡æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯

    Args:
        text: è®ºæ–‡æ–‡æœ¬å†…å®¹
        ai_client: AIå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå¿…éœ€ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼šGemini, OpenAI, Claudeç­‰ï¼‰
        session_id: ä¼šè¯ID
        languages: æ”¯æŒçš„è¯­è¨€åˆ—è¡¨ï¼ˆå½“å‰ä»…æ”¯æŒä¸­è‹±æ–‡ï¼‰
        use_sections: æ˜¯å¦ä½¿ç”¨ç« èŠ‚åˆ†æ®µå¤„ç†ï¼ˆæ¨èï¼‰

    Returns:
        åŒ…å«æå–ä¿¡æ¯çš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    if not ai_client:
        logger.error("å¿…é¡»æä¾›AIå®¢æˆ·ç«¯å®ä¾‹")
        return None

    if languages is None:
        languages = ['Chinese', 'English']

    logger.info(f"ğŸ¯ å¼€å§‹æ™ºèƒ½å­¦ä½è®ºæ–‡ä¿¡æ¯æå–ï¼Œæ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")

    # Step 1: é¦–å…ˆæå–è®ºæ–‡å‰ç½®éƒ¨åˆ†çš„å…ƒæ•°æ®ï¼ˆå‰10000å­—ç¬¦ï¼‰
    front_matter_size = min(10000, len(text) // 4)  # å‰25%æˆ–æœ€å¤š10000å­—ç¬¦
    front_matter = text[:front_matter_size]

    logger.info(f"ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæå–è®ºæ–‡å…ƒæ•°æ® (å‰ {len(front_matter):,} å­—ç¬¦)")
    metadata = _extract_thesis_metadata(front_matter, ai_client, session_id)

    # Step 2: æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç« èŠ‚åˆ†æ®µå¤„ç†
    if use_sections and len(text) > 20000:  # å¯¹äºé•¿æ–‡æœ¬ä½¿ç”¨ç« èŠ‚å¤„ç†
        logger.info(f"ğŸ“š ç¬¬äºŒæ­¥ï¼šç« èŠ‚åˆ†æ®µå¤„ç† ({len(text):,} å­—ç¬¦)")
        import time

        section_start_time = time.time()

        try:
            from .paper_section_processor import create_section_processor

            logger.info("ğŸ”§ åˆå§‹åŒ–ç« èŠ‚å¤„ç†å™¨...")
            processor = create_section_processor(ai_client)

            logger.info("ğŸš€ å¼€å§‹ç« èŠ‚åˆ†æ®µå¤„ç†...")
            result = processor.process_paper_by_sections(text, session_id)

            section_elapsed = time.time() - section_start_time

            if result:
                logger.info(f"âœ… ç« èŠ‚åˆ†æ®µå¤„ç†æˆåŠŸ (è€—æ—¶: {section_elapsed:.2f} ç§’)")
                logger.info(f"ğŸ“Š ç« èŠ‚å¤„ç†ç»“æœç»Ÿè®¡: {len(result)} ä¸ªå­—æ®µ")

                # åˆå¹¶å…ƒæ•°æ®å’Œç« èŠ‚å†…å®¹
                if metadata:
                    logger.info("ğŸ”— åˆå¹¶å…ƒæ•°æ®å’Œç« èŠ‚å†…å®¹")
                    merge_count = 0
                    # ç”¨å‡†ç¡®çš„å…ƒæ•°æ®è¦†ç›–ç« èŠ‚æå–çš„ç›¸åº”å­—æ®µ
                    for key, value in metadata.items():
                        if value and value.strip():  # åªè¦†ç›–éç©ºçš„å…ƒæ•°æ®
                            if key in result:
                                result[key] = value
                                merge_count += 1
                            elif key == "Author":
                                result["Author"] = value
                                merge_count += 1
                            elif key == "University":
                                result["University"] = value
                                merge_count += 1
                    logger.info(f"ğŸ“ æˆåŠŸåˆå¹¶ {merge_count} ä¸ªå…ƒæ•°æ®å­—æ®µ")
                    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ˜ å°„

                return result
            else:
                section_elapsed = time.time() - section_start_time
                logger.warning(
                    f"âŒ ç« èŠ‚åˆ†æ®µå¤„ç†å¤±è´¥ (è€—æ—¶: {section_elapsed:.2f} ç§’)ï¼Œå›é€€åˆ°å…¨æ–‡å¤„ç†"
                )
        except Exception as e:
            section_elapsed = time.time() - section_start_time
            logger.warning(
                f"âŒ ç« èŠ‚åˆ†æ®µå¤„ç†å¼‚å¸¸ (è€—æ—¶: {section_elapsed:.2f} ç§’): {e}ï¼Œå›é€€åˆ°å…¨æ–‡å¤„ç†"
            )

    # Step 3: å…¨æ–‡å¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    logger.info(f"ğŸ“– ä½¿ç”¨å…¨æ–‡å¤„ç†æ¨¡å¼ï¼Œæ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")

    # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœè¿‡é•¿åˆ™åˆ†æ®µå¤„ç†
    max_single_chunk = 32000  # 3.2ä¸‡å­—ç¬¦ä¸ºå•æ¬¡å¤„ç†ä¸Šé™ï¼Œé€‚é…64Kä¸Šä¸‹æ–‡

    if len(text) > max_single_chunk:
        logger.warning(f"âš ï¸ æ–‡æœ¬è¿‡é•¿ ({len(text):,} å­—ç¬¦)ï¼Œå¯ç”¨åˆ†æ®µå¤„ç†æ¨¡å¼")

        # åˆ†å‰²æ–‡æœ¬
        text_chunks = _split_text_intelligently(text, max_single_chunk)

        # å¯¹æ¯ä¸ªåˆ†å—è¿›è¡Œå¤„ç†
        chunk_results = []
        for i, chunk in enumerate(text_chunks):
            logger.info(
                f"ğŸ”„ æ­£åœ¨å¤„ç†åˆ†å— {i+1}/{len(text_chunks)} ({len(chunk):,} å­—ç¬¦)"
            )

            result = _extract_single_chunk(chunk, ai_client, session_id, languages)
            if result:
                chunk_results.append(result)
            else:
                logger.warning(f"âŒ åˆ†å— {i+1} å¤„ç†å¤±è´¥")

        if not chunk_results:
            logger.error("âŒ æ‰€æœ‰åˆ†å—å¤„ç†éƒ½å¤±è´¥äº†")
            return None

        # åˆå¹¶ç»“æœ
        logger.info("ğŸ”— æ­£åœ¨åˆå¹¶åˆ†å—å¤„ç†ç»“æœ...")
        result = _merge_extracted_info(chunk_results)

        # åˆå¹¶å…ƒæ•°æ®
        if metadata and result:
            logger.info("ğŸ”— åˆå¹¶å…ƒæ•°æ®å’Œåˆ†å—å†…å®¹")
            for key, value in metadata.items():
                if value and value.strip():
                    if key in result:
                        result[key] = value

        return result

    else:
        # æ–‡æœ¬é•¿åº¦åˆé€‚ï¼Œç›´æ¥å¤„ç†
        logger.info("ğŸ“„ æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼Œç›´æ¥å…¨æ–‡å¤„ç†")
        result = _extract_single_chunk(text, ai_client, session_id, languages)

        # åˆå¹¶å…ƒæ•°æ®
        if metadata and result:
            logger.info("ğŸ”— åˆå¹¶å…ƒæ•°æ®å’Œå…¨æ–‡å†…å®¹")
            for key, value in metadata.items():
                if value and value.strip():
                    if key in result:
                        result[key] = value

        return result


def _extract_thesis_metadata(
    front_matter: str, ai_client, session_id: Optional[str] = None
) -> Optional[Dict]:
    """
    ä¸“é—¨æå–å­¦ä½è®ºæ–‡å‰ç½®éƒ¨åˆ†çš„å®Œæ•´å…ƒæ•°æ®ä¿¡æ¯

    Args:
        front_matter: è®ºæ–‡å‰ç½®éƒ¨åˆ†å†…å®¹ï¼ˆå‰10000å­—ç¬¦ï¼‰
        ai_client: AIå®¢æˆ·ç«¯
        session_id: ä¼šè¯ID

    Returns:
        åŒ…å«è®ºæ–‡å®Œæ•´å…ƒæ•°æ®çš„å­—å…¸
    """

    metadata_prompt = f"""è¯·ä»ä¸‹æ–¹å­¦ä½è®ºæ–‡çš„å‰ç½®éƒ¨åˆ†ï¼ˆå°é¢ã€æ‰‰é¡µã€æ‘˜è¦ã€ç›®å½•ç­‰ï¼‰ä¸­å‡†ç¡®æå–ä»¥ä¸‹å®Œæ•´çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼š

**é‡è¦è¯´æ˜**ï¼š
- è¯·ä»”ç»†æŸ¥æ‰¾è®ºæ–‡å°é¢ã€æ‰‰é¡µã€æ‘˜è¦é¡µã€ç›®å½•é¡µä¸­çš„å‡†ç¡®ä¿¡æ¯
- æ ‡é¢˜å¿…é¡»æ˜¯è®ºæ–‡å°é¢ä¸Šçš„å®Œæ•´ã€å‡†ç¡®æ ‡é¢˜ï¼Œä¸è¦ä»æ‘˜è¦å†…å®¹ä¸­æ¨æ–­
- å¦‚æœæŸé¡¹ä¿¡æ¯æœªæ‰¾åˆ°ï¼Œè¯·è¾“å‡ºç©ºå­—ç¬¦ä¸²""
- ä¸­è‹±æ–‡ä¿¡æ¯è¯·åˆ†åˆ«æå–ï¼Œå¦‚æœåªæœ‰ä¸€ç§è¯­è¨€ç‰ˆæœ¬ï¼Œå¦ä¸€ç§ç•™ç©º

**æå–å­—æ®µ**ï¼š
{{
  "thesis_number": "è®ºæ–‡ç¼–å·ï¼ˆå¦‚å­¦å·ã€è®ºæ–‡åˆ†ç±»å·ç­‰ï¼‰",
  "title_cn": "è®ºæ–‡çš„å®Œæ•´ä¸­æ–‡æ ‡é¢˜ï¼ˆä»å°é¢é¡µå‡†ç¡®æå–ï¼‰",
  "title_en": "è®ºæ–‡çš„å®Œæ•´è‹±æ–‡æ ‡é¢˜ï¼ˆä»å°é¢é¡µå‡†ç¡®æå–ï¼‰",
  "university_cn": "ä¸­æ–‡å­¦æ ¡åç§°",
  "university_en": "è‹±æ–‡å­¦æ ¡åç§°", 
  "degree_level": "ç”³è¯·å­¦ä½çº§åˆ«ï¼ˆå¦‚ï¼šå­¦å£«ã€ç¡•å£«ã€åšå£«ï¼‰",
  "author_cn": "ä¸­æ–‡ä½œè€…å§“å",
  "author_en": "è‹±æ–‡ä½œè€…å§“å",
  "major_cn": "ä¸­æ–‡å­¦ç§‘ä¸“ä¸š",
  "major_en": "è‹±æ–‡å­¦ç§‘ä¸“ä¸š",
  "research_direction_cn": "ä¸­æ–‡ç ”ç©¶æ–¹å‘",
  "research_direction_en": "è‹±æ–‡ç ”ç©¶æ–¹å‘",
  "supervisor_cn": "ä¸­æ–‡æŒ‡å¯¼æ•™å¸ˆå§“å",
  "supervisor_en": "è‹±æ–‡æŒ‡å¯¼æ•™å¸ˆå§“å",
  "supervisor_title_cn": "ä¸­æ–‡æŒ‡å¯¼æ•™å¸ˆèŒç§°ï¼ˆå¦‚ï¼šæ•™æˆã€å‰¯æ•™æˆã€è®²å¸ˆç­‰ï¼‰",
  "supervisor_title_en": "è‹±æ–‡æŒ‡å¯¼æ•™å¸ˆèŒç§°",
  "College": "åŸ¹å…»å­¦é™¢åç§°",
  "defense_date": "è®ºæ–‡ç­”è¾©æ—¥æœŸ",
  "DegreeGrantingInstitution": "å­¦ä½æˆäºˆå•ä½",
  "abstract_cn": "å®Œæ•´çš„ä¸­æ–‡æ‘˜è¦å†…å®¹",
  "abstract_en": "å®Œæ•´çš„è‹±æ–‡æ‘˜è¦å†…å®¹", 
  "keywords_cn": "ä¸­æ–‡å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰",
  "keywords_en": "è‹±æ–‡å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰",
  "TableOfContents": "è®ºæ–‡å®Œæ•´ç›®å½•ç»“æ„ï¼ˆåŒ…å«ç« èŠ‚ç¼–å·å’Œé¡µç ï¼‰"
}}

**è¾“å‡ºè¦æ±‚**ï¼š
- ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ä»£ç å—æ ‡è®°
- ç¡®ä¿JSONæ ¼å¼æœ‰æ•ˆ
- æœªæ‰¾åˆ°çš„ä¿¡æ¯ç”¨ç©ºå­—ç¬¦ä¸²""è¡¨ç¤º

**è®ºæ–‡å‰ç½®å†…å®¹**ï¼š
{front_matter}
"""

    try:
        logger.info("æ­£åœ¨æå–å­¦ä½è®ºæ–‡å®Œæ•´å…ƒæ•°æ®ä¿¡æ¯...")
        response = ai_client.send_message(metadata_prompt, session_id=session_id)

        if not response or not response.content:
            logger.error("AIè¿”å›ç©ºå“åº”")
            return None

        content = response.content.strip()
        logger.debug(f"å…ƒæ•°æ®æå–å“åº”: {content[:200]}...")

        # è§£æJSON
        json_content = _extract_json_from_response(content)
        if json_content:
            cleaned_content = _clean_json_content(json_content)
            logger.debug(f"è°ƒç”¨JSONå¤‡ç”¨è§£ææœºåˆ¶å¤„ç†åŸºç¡€å…ƒæ•°æ®")
            result = _parse_json_with_fallback(cleaned_content)
            if result:
                non_empty_fields = len(
                    [k for k, v in result.items() if v and str(v).strip()]
                )
                logger.info(
                    f"âœ… æˆåŠŸæå–è®ºæ–‡å…ƒæ•°æ®ï¼Œæ€»å…± {len(result)} ä¸ªå­—æ®µï¼Œå…¶ä¸­ {non_empty_fields} ä¸ªéç©º"
                )
                return result

        logger.error("è®ºæ–‡å…ƒæ•°æ®æå–å¤±è´¥")
        return None

    except Exception as e:
        logger.error(f"è®ºæ–‡å…ƒæ•°æ®æå–å¼‚å¸¸: {e}")
        return None


def _extract_single_chunk(
    text: str,
    ai_client,
    session_id: Optional[str] = None,
    languages: Optional[List[str]] = None,
) -> Optional[Dict]:
    """
    å¤„ç†å•ä¸ªæ–‡æœ¬åˆ†å—çš„æå–é€»è¾‘
    """

    # ä¼˜åŒ–åçš„æç¤ºè¯ï¼Œä¸“é—¨é’ˆå¯¹å­¦ä½è®ºæ–‡ç»“æ„è®¾è®¡
    prompt = f"""è¯·ä»ä¸‹æ–¹å­¦ä½è®ºæ–‡æ–‡æœ¬ä¸­å‡†ç¡®æŠ½å–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¾“å‡ºã€‚

**ç‰¹åˆ«æ³¨æ„**ï¼š
1. è®ºæ–‡æ ‡é¢˜ï¼šè¯·ä»æ–‡æ¡£å¼€å¤´çš„å°é¢æˆ–æ ‡é¢˜é¡µæå–å‡†ç¡®çš„è®ºæ–‡æ ‡é¢˜ï¼Œä¸è¦ä»æ‘˜è¦æˆ–æ­£æ–‡å†…å®¹ä¸­æ¨æ–­
2. ä½œè€…ä¿¡æ¯ï¼šå¦‚æœæœ‰ï¼Œè¯·ä»å°é¢é¡µæå–ä½œè€…å§“åã€å­¦æ ¡ã€ä¸“ä¸šã€å¯¼å¸ˆç­‰ä¿¡æ¯
3. ç« èŠ‚è¯†åˆ«ï¼šè¯·å‡†ç¡®è¯†åˆ«å„ä¸ªç« èŠ‚çš„å†…å®¹ï¼Œå¦‚æ–‡çŒ®ç»¼è¿°ã€ç ”ç©¶æ–¹æ³•ç­‰

**æå–å­—æ®µ**ï¼š
1. ä¸­æ–‡ç¯‡åï¼ˆtitle_cnï¼‰- ä»è®ºæ–‡å°é¢æˆ–æ ‡é¢˜é¡µæå–çš„å‡†ç¡®æ ‡é¢˜
2. ä¸­æ–‡å…³é”®è¯ï¼ˆkeywords_cnï¼‰- ä»ä¸­æ–‡æ‘˜è¦åçš„å…³é”®è¯éƒ¨åˆ†æå–
3. ä¸­æ–‡æ‘˜è¦ï¼ˆabstract_cnï¼‰- æ ‡æœ‰"æ‘˜è¦"çš„ä¸­æ–‡éƒ¨åˆ†
4. è‹±æ–‡ç¯‡åï¼ˆtitle_enï¼‰- ä»è®ºæ–‡å°é¢æˆ–æ ‡é¢˜é¡µæå–çš„è‹±æ–‡æ ‡é¢˜
5. è‹±æ–‡å…³é”®è¯ï¼ˆkeywords_enï¼‰- ä»è‹±æ–‡æ‘˜è¦åçš„å…³é”®è¯éƒ¨åˆ†æå–
6. è‹±æ–‡æ‘˜è¦ï¼ˆabstract_enï¼‰- æ ‡æœ‰"Abstract"çš„è‹±æ–‡éƒ¨åˆ†
7. ç ”ç©¶æ–¹æ³•ï¼ˆResearchMethodsï¼‰- è®ºæ–‡ä¸­"ç ”ç©¶æ–¹æ³•"ã€"å®éªŒæ–¹æ³•"ã€"æŠ€æœ¯è·¯çº¿"ç­‰ç« èŠ‚å†…å®¹
8. ç†è®ºæ¡†æ¶ï¼ˆTheoreticalFrameworkï¼‰- è®ºæ–‡ä¸­"ç†è®ºåŸºç¡€"ã€"ç†è®ºæ¡†æ¶"ç­‰ç« èŠ‚å†…å®¹
9. å®è·µé—®é¢˜ï¼ˆpractical_problemsï¼‰- è®ºæ–‡è¦è§£å†³çš„å®é™…é—®é¢˜å’Œåº”ç”¨èƒŒæ™¯
10. è§£å†³æ–¹æ¡ˆï¼ˆproposed_solutionsï¼‰- è®ºæ–‡æå‡ºçš„å…·ä½“è§£å†³æ–¹æ¡ˆå’Œç­–ç•¥
11. ç ”ç©¶ç»“è®ºï¼ˆresearch_conclusionsï¼‰- è®ºæ–‡"ç»“è®º"ç« èŠ‚çš„ä¸»è¦å†…å®¹
12. åº”ç”¨ä»·å€¼ï¼ˆapplication_valueï¼‰- è®ºæ–‡æˆæœçš„å®é™…åº”ç”¨ä»·å€¼å’Œæ„ä¹‰
13. å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼ˆreferencesï¼‰- è®ºæ–‡"å‚è€ƒæ–‡çŒ®"ç« èŠ‚çš„å®Œæ•´æ–‡çŒ®æ¡ç›®

**è¾“å‡ºè¦æ±‚**ï¼š
- ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸ä½¿ç”¨ä»£ç å—æ ‡è®°
- ç¼ºå¤±ä¿¡æ¯ç”¨ç©ºå­—ç¬¦ä¸²""è¡¨ç¤º
- ç¡®ä¿JSONæ ¼å¼æ­£ç¡®æœ‰æ•ˆ
- å­—ç¬¦ä¸²å€¼ä¸åŒ…å«æ§åˆ¶å­—ç¬¦

**è®ºæ–‡å†…å®¹**ï¼š
{text}
"""

    try:
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"å¼€å§‹AIç»“æ„åŒ–ä¿¡æ¯æå–ï¼Œæ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
        logger.info(f"ä½¿ç”¨è¯­è¨€åˆ—è¡¨: {languages}")

        # å‘é€è¯·æ±‚
        response = ai_client.send_message(prompt, session_id=session_id)
        if not response or not response.content:
            logger.error("AIè¿”å›ç©ºå“åº”")
            return None

        response_text = response.content.strip()
        logger.info(f"AIå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
        logger.info(
            f"ä½¿ç”¨AIæ¨¡å‹: {ai_client.get_api_type() if hasattr(ai_client, 'get_api_type') else 'Unknown'}"
        )

        # æå–JSONå†…å®¹
        json_content = _extract_json_from_response(response_text)
        if not json_content:
            logger.error("æ— æ³•ä»å“åº”ä¸­æå–JSONå†…å®¹")
            logger.debug(f"åŸå§‹å“åº”å†…å®¹å‰500å­—ç¬¦: {response_text[:500]}")
            return None

        # æ¸…ç†JSONå†…å®¹
        cleaned_json = _clean_json_content(json_content)

        # è§£æJSON
        logger.debug(f"è°ƒç”¨JSONå¤‡ç”¨è§£ææœºåˆ¶å¤„ç†ç« èŠ‚åˆ†æå“åº”")
        result = _parse_json_with_fallback(cleaned_json)
        if result:
            logger.info(f"JSONè§£ææˆåŠŸï¼ŒåŒ…å«å­—æ®µ: {list(result.keys())}")
        else:
            logger.error("JSONè§£æå¤±è´¥")
            logger.debug(f"æ¸…ç†åçš„JSONå†…å®¹å‰500å­—ç¬¦: {cleaned_json[:500]}")

        return result

    except TimeoutError as e:
        logger.error(f"AIè¯·æ±‚è¶…æ—¶: {e}")
        logger.warning("å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIæœåŠ¡çŠ¶æ€")
        return None
    except ConnectionError as e:
        logger.error(f"AIè¿æ¥é”™è¯¯: {e}")
        logger.warning("å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIç«¯ç‚¹é…ç½®")
        return None
    except Exception as e:
        logger.error(f"æå–ç»“æ„åŒ–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback

        logger.debug(f"è¯¦ç»†é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return None
        return None


def extract_sections_with_ai_by_chapters(
    file_path: str,
    ai_client,
    max_sections: Optional[int] = None,
    test_mode: bool = False,
    session_id: Optional[str] = None,
) -> Optional[Dict]:
    """
    é€šè¿‡ç« èŠ‚åˆ†æ®µå¤„ç†çš„æ–¹å¼ä»è®ºæ–‡ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯

    Args:
        file_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„
        ai_client: AIå®¢æˆ·ç«¯å®ä¾‹
        max_sections: æœ€å¤§å¤„ç†ç« èŠ‚æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        test_mode: æµ‹è¯•æ¨¡å¼ï¼Œåªå¤„ç†é‡è¦ç« èŠ‚
        session_id: ä¼šè¯ID

    Returns:
        åŒ…å«æå–ä¿¡æ¯çš„å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        from .paper_section_processor import PaperSectionParser

        # 1. æå–æ–‡æ¡£æ–‡æœ¬
        if file_path.endswith('.docx'):
            text = extract_text_from_word(file_path)
        else:
            logger.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            return None

        if not text.strip():
            logger.error("æ–‡æ¡£æ–‡æœ¬æå–å¤±è´¥æˆ–ä¸ºç©º")
            return None

        logger.info(f"æ–‡æ¡£æ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text):,} å­—ç¬¦")

        # 2. è§£æç« èŠ‚
        parser = PaperSectionParser()
        sections = parser.parse_sections(text)

        logger.info(f"ç« èŠ‚è§£æå®Œæˆï¼Œå…± {len(sections)} ä¸ªç« èŠ‚")

        # 3. ç­›é€‰é‡è¦ç« èŠ‚è¿›è¡Œå¤„ç†
        important_sections = []
        important_types = [
            'abstract',
            'introduction',
            'methodology',
            'experiment',
            'results',
            'conclusion',
        ]

        # æŒ‰é‡è¦æ€§å’Œå†…å®¹é•¿åº¦ç­›é€‰ç« èŠ‚
        for section in sections:
            if test_mode and max_sections and len(important_sections) >= max_sections:
                break

            # ä¼˜å…ˆå¤„ç†é‡è¦ç±»å‹çš„ç« èŠ‚
            if (
                section.section_type in important_types
                and len(section.content.strip()) > 200
            ):
                important_sections.append(section)
            # æˆ–è€…å¤„ç†å†…å®¹è¾ƒé•¿çš„æœªåˆ†ç±»ç« èŠ‚
            elif (
                section.section_type == 'unknown'
                and len(section.content.strip()) > 1000
            ):
                important_sections.append(section)

        if not important_sections:
            logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„ç« èŠ‚è¿›è¡Œå¤„ç†ï¼Œå›é€€åˆ°å…¨æ–‡å¤„ç†")
            return extract_sections_with_ai(text, ai_client, session_id)

        logger.info(f"é€‰æ‹© {len(important_sections)} ä¸ªé‡è¦ç« èŠ‚è¿›è¡Œå¤„ç†")

        # 4. åˆ†ç« èŠ‚æå–ä¿¡æ¯
        extracted_results = {}
        for i, section in enumerate(important_sections, 1):
            logger.info(
                f"å¤„ç†ç¬¬ {i}/{len(important_sections)} ä¸ªç« èŠ‚: {section.title[:30]}..."
            )

            # æ„å»ºç« èŠ‚ç‰¹å®šçš„æç¤ºè¯
            section_prompt = f"""è¯·ä»ä¸‹æ–¹è®ºæ–‡ç« èŠ‚ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œç« èŠ‚æ ‡é¢˜ï¼š{section.title}

ç« èŠ‚å†…å®¹ï¼š
{section.content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœåœ¨æ­¤ç« èŠ‚ä¸­å­˜åœ¨ï¼‰ï¼š
1. ä¸­æ–‡ç¯‡åï¼ˆtitle_cnï¼‰
2. ä¸­æ–‡å…³é”®è¯ï¼ˆkeywords_cnï¼‰  
3. ä¸­æ–‡æ‘˜è¦ï¼ˆabstract_cnï¼‰
4. è‹±æ–‡ç¯‡åï¼ˆtitle_enï¼‰
5. è‹±æ–‡å…³é”®è¯ï¼ˆkeywords_enï¼‰
6. è‹±æ–‡æ‘˜è¦ï¼ˆabstract_enï¼‰
7. ç ”ç©¶æ–¹æ³•ï¼ˆresearch_methodsï¼‰
8. ç†è®ºæ¡†æ¶ï¼ˆtheoretical_frameworkï¼‰
9. ä¸»è¦åˆ›æ–°ç‚¹ï¼ˆMainInnovationsï¼‰
10. å®è·µé—®é¢˜ï¼ˆpractical_problemsï¼‰
11. è§£å†³æ–¹æ¡ˆï¼ˆproposed_solutionsï¼‰
12. ç ”ç©¶ç»“è®ºï¼ˆresearch_conclusionsï¼‰
13. åº”ç”¨ä»·å€¼ï¼ˆapplication_valueï¼‰
14. å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼ˆreferencesï¼‰

è¾“å‡ºè¦æ±‚ï¼š
- è¿”å›JSONæ ¼å¼
- å¦‚æŸé¡¹åœ¨æ­¤ç« èŠ‚ä¸­ä¸å­˜åœ¨è¯·è¾“å‡ºç©ºå­—ç¬¦ä¸²""
- JSONå¿…é¡»æ˜¯æœ‰æ•ˆæ ¼å¼
"""

            try:
                response = ai_client.send_message(section_prompt, session_id=session_id)
                if response and response.content:
                    # æå–å¹¶è§£æJSON
                    json_content = _extract_json_from_response(response.content)
                    if json_content:
                        cleaned_json = _clean_json_content(json_content)
                        logger.debug(
                            f"è°ƒç”¨JSONå¤‡ç”¨è§£ææœºåˆ¶å¤„ç†ç« èŠ‚ '{section.title}' çš„åˆ†æå“åº”"
                        )
                        section_result = _parse_json_with_fallback(cleaned_json)

                        if section_result:
                            # åˆå¹¶ç»“æœï¼Œéç©ºå­—æ®µè¦†ç›–å·²æœ‰ç»“æœ
                            for key, value in section_result.items():
                                if value and value.strip():
                                    if (
                                        key not in extracted_results
                                        or not extracted_results[key]
                                    ):
                                        extracted_results[key] = value
                                    else:
                                        # å¯¹äºæŸäº›å­—æ®µï¼Œè¿½åŠ å†…å®¹è€Œä¸æ˜¯è¦†ç›–
                                        if key in ['references']:
                                            extracted_results[key] += "\n\n" + value
                                        else:
                                            extracted_results[key] = value

                            logger.info(
                                f"ç« èŠ‚ {i} å¤„ç†æˆåŠŸï¼Œæå–åˆ° {len([v for v in section_result.values() if v and v.strip()])} ä¸ªéç©ºå­—æ®µ"
                            )
                        else:
                            logger.warning(f"ç« èŠ‚ {i} JSONè§£æå¤±è´¥")
                    else:
                        logger.warning(f"ç« èŠ‚ {i} æœªæ‰¾åˆ°æœ‰æ•ˆJSON")
                else:
                    logger.warning(f"ç« èŠ‚ {i} AIå“åº”ä¸ºç©º")

            except Exception as e:
                logger.error(f"å¤„ç†ç« èŠ‚ {i} æ—¶å‡ºé”™: {e}")
                continue

        # 5. ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨ (ä½¿ç”¨snake_caseå’Œè¯­è¨€åç¼€)
        required_fields = [
            'title_cn',
            'keywords_cn',
            'abstract_cn',
            'title_en',
            'keywords_en',
            'abstract_en',
            'theoretical_framework',
            'references',
        ]

        for field in required_fields:
            if field not in extracted_results:
                extracted_results[field] = ""

        # 6. å¦‚æœå…³é”®ä¿¡æ¯ç¼ºå¤±å¤ªå¤šï¼Œè¡¥å……å…¨æ–‡å¤„ç†
        key_fields = ['title_cn', 'abstract_cn']
        missing_key_fields = sum(
            1 for field in key_fields if not extracted_results.get(field, '').strip()
        )

        if missing_key_fields >= 2:
            logger.info("å…³é”®ä¿¡æ¯ç¼ºå¤±è¾ƒå¤šï¼Œè¿›è¡Œè¡¥å……å…¨æ–‡å¤„ç†...")
            full_result = extract_sections_with_ai(text, ai_client, session_id)
            if full_result:
                for key, value in full_result.items():
                    if (
                        not extracted_results.get(key, '').strip()
                        and value
                        and value.strip()
                    ):
                        extracted_results[key] = value

        logger.info(
            f"ç« èŠ‚å¤„ç†å®Œæˆï¼Œå…±æå– {len([v for v in extracted_results.values() if v and v.strip()])} ä¸ªéç©ºå­—æ®µ"
        )
        return extracted_results

    except Exception as e:
        logger.error(f"ç« èŠ‚å¤„ç†å‡ºé”™: {e}")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        return None


class DocumentCache:
    """æ–‡æ¡£ç¼“å­˜ç®¡ç†å™¨ - ç¼“å­˜PDF/Wordè½¬æ¢çš„Markdownæ–‡æœ¬"""

    def __init__(self, cache_dir: str = "cache/documents"):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _get_cache_key(self, file_path: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        file_path_obj = Path(file_path)
        file_hash = self._get_file_hash(file_path)
        return f"{file_path_obj.stem}_{file_hash}"

    def _get_cache_file_path(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.md"

    def _get_metadata_file_path(self, cache_key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}_metadata.json"

    def is_cached(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¼“å­˜"""
        try:
            cache_key = self._get_cache_key(file_path)
            cache_file = self._get_cache_file_path(cache_key)
            metadata_file = self._get_metadata_file_path(cache_key)

            return cache_file.exists() and metadata_file.exists()
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return False

    def get_cached_content(self, file_path: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®"""
        try:
            if not self.is_cached(file_path):
                return None

            cache_key = self._get_cache_key(file_path)
            cache_file = self._get_cache_file_path(cache_key)
            metadata_file = self._get_metadata_file_path(cache_key)

            # è¯»å–Markdownå†…å®¹
            with open(cache_file, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

            # è¯»å–å…ƒæ•°æ®
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            return {'content': markdown_content, 'metadata': metadata}

        except Exception as e:
            self.logger.error(f"è¯»å–ç¼“å­˜å†…å®¹å¤±è´¥: {e}")
            return None

    def save_to_cache(
        self,
        file_path: str,
        content: str,
        file_type: Optional[str] = None,
        char_count: Optional[int] = None,
    ) -> bool:
        """ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ç¼“å­˜"""
        try:
            cache_key = self._get_cache_key(file_path)
            cache_file = self._get_cache_file_path(cache_key)
            metadata_file = self._get_metadata_file_path(cache_key)

            # ä¿å­˜Markdownå†…å®¹
            with open(cache_file, 'w', encoding='utf-8') as f:
                f.write(content)

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'source_file': str(file_path),
                'file_type': file_type or Path(file_path).suffix.lower(),
                'cached_time': datetime.now().isoformat(),
                'file_size': Path(file_path).stat().st_size,
                'content_length': len(content),
                'char_count': char_count or len(content),
                'cache_key': cache_key,
            }

            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.logger.info(f"æ–‡æ¡£ç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_file}")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False

    def clear_cache(self, file_path: Optional[str] = None) -> bool:
        """æ¸…é™¤ç¼“å­˜ï¼ˆæŒ‡å®šæ–‡ä»¶æˆ–å…¨éƒ¨ï¼‰"""
        try:
            if file_path:
                # æ¸…é™¤æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜
                cache_key = self._get_cache_key(file_path)
                cache_file = self._get_cache_file_path(cache_key)
                metadata_file = self._get_metadata_file_path(cache_key)

                if cache_file.exists():
                    cache_file.unlink()
                if metadata_file.exists():
                    metadata_file.unlink()

                self.logger.info(f"å·²æ¸…é™¤æ–‡ä»¶ç¼“å­˜: {file_path}")
            else:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                for file in self.cache_dir.glob("*"):
                    if file.is_file():
                        file.unlink()

                self.logger.info("å·²æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç¼“å­˜")

            return True

        except Exception as e:
            self.logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False

    def get_cache_info(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            cache_files = list(self.cache_dir.glob("*.md"))
            metadata_files = list(self.cache_dir.glob("*_metadata.json"))

            total_size = sum(f.stat().st_size for f in cache_files)

            return {
                'cache_dir': str(self.cache_dir),
                'cached_files': len(cache_files),
                'metadata_files': len(metadata_files),
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
            }

        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return {}


def extract_text_with_cache(file_path: str, use_cache: bool = True) -> str:
    """
    ä»æ–‡æ¡£æå–æ–‡æœ¬ï¼Œæ”¯æŒç¼“å­˜

    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    logger.info(f"å¼€å§‹æå–æ–‡æ¡£æ–‡æœ¬: {file_path}")

    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache_manager = DocumentCache()

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    if use_cache and cache_manager.is_cached(file_path):
        logger.info("å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥è¯»å–...")
        cached_data = cache_manager.get_cached_content(file_path)
        if cached_data:
            content = cached_data['content']
            metadata = cached_data['metadata']
            logger.info(
                f"âœ… ç¼“å­˜å‘½ä¸­: {metadata['cached_time']}, {len(content):,} å­—ç¬¦"
            )
            return content

    # æå–åŸå§‹æ–‡æœ¬
    file_ext = Path(file_path).suffix.lower()

    if file_ext == '.docx':
        logger.info("æå–Wordæ–‡æœ¬...")
        text = extract_text_from_word(file_path)
    else:
        logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        return ""

    if not text:
        logger.error("æ–‡æœ¬æå–å¤±è´¥")
        return ""

    # è½¬æ¢ä¸ºMarkdownæ ¼å¼
    markdown_content = convert_text_to_markdown(text, file_path)

    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        success = cache_manager.save_to_cache(
            file_path, markdown_content, file_ext, len(text)
        )
        if success:
            logger.info("âœ… æ–‡æ¡£å·²ç¼“å­˜")
        else:
            logger.warning("âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥")

    return markdown_content


def convert_text_to_markdown(text: str, file_path: Optional[str] = None) -> str:
    """
    å°†çº¯æ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼

    Args:
        text: åŸå§‹æ–‡æœ¬
        file_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰

    Returns:
        Markdownæ ¼å¼çš„æ–‡æœ¬
    """
    lines = text.split('\n')
    markdown_lines = []

    # æ·»åŠ æ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
    if file_path:
        filename = Path(file_path).name
        markdown_lines.extend(
            [
                f"# {filename}",
                "",
                f"**æºæ–‡ä»¶**: {file_path}",
                f"**è½¬æ¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                f"**åŸå§‹å­—ç¬¦æ•°**: {len(text):,}",
                "",
                "---",
                "",
            ]
        )

    # å¤„ç†æ–‡æœ¬å†…å®¹
    for line in lines:
        line = line.strip()

        if not line:
            markdown_lines.append("")
            continue

        # æ£€æµ‹å¯èƒ½çš„æ ‡é¢˜
        if _is_potential_title(line):
            # æ ¹æ®é•¿åº¦å’Œä½ç½®åˆ¤æ–­æ ‡é¢˜çº§åˆ«
            if len(line) < 50 and any(
                keyword in line
                for keyword in ['æ‘˜è¦', 'Abstract', 'å¼•è¨€', 'ç»“è®º', 'å‚è€ƒæ–‡çŒ®']
            ):
                markdown_lines.append(f"## {line}")
            elif len(line) < 80:
                markdown_lines.append(f"### {line}")
            else:
                markdown_lines.append(line)
        else:
            markdown_lines.append(line)

    return '\n'.join(markdown_lines)


def _is_potential_title(line: str) -> bool:
    """åˆ¤æ–­ä¸€è¡Œæ–‡æœ¬æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜"""
    # æ ‡é¢˜ç‰¹å¾ï¼šè¾ƒçŸ­ã€åŒ…å«ç‰¹å®šå…³é”®è¯ã€æ•°å­—ç¼–å·ç­‰
    title_indicators = [
        r'^\d+[\.ã€]\s*',  # æ•°å­—ç¼–å·
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€]\s*',  # ä¸­æ–‡æ•°å­—ç¼–å·
        r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« ',  # ç« èŠ‚
        r'^Abstract$|^æ‘˜è¦$',  # ç‰¹æ®Šç« èŠ‚
        r'^Introduction$|^å¼•è¨€$',
        r'^Conclusion$|^ç»“è®º$',
        r'^References$|^å‚è€ƒæ–‡çŒ®$',
        r'^è‡´è°¢$|^Acknowledgments?$',
    ]

    return any(re.match(pattern, line.strip()) for pattern in title_indicators)


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_document_cache = None


def get_document_cache() -> DocumentCache:
    """è·å–å…¨å±€æ–‡æ¡£ç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _document_cache
    if _document_cache is None:
        _document_cache = DocumentCache()
    return _document_cache


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œå…¥å£ç‚¹"""
    import sys
    import argparse
    from pathlib import Path
    
    parser = argparse.ArgumentParser(
        description="ä¸“ä¸šç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python -m thesis_inno_eval.extract_sections_with_ai thesis.docx
  extract-sections thesis.docx
  extract-sections --output result.json thesis.docx
        """
    )
    
    parser.add_argument(
        'file_path',
        type=str,
        help='è®ºæ–‡æ–‡ä»¶è·¯å¾„ (æ”¯æŒ.docxæ ¼å¼)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è¾“å…¥æ–‡ä»¶å_extracted.json)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.WARNING, format='%(levelname)s - %(message)s')
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_path = Path(args.file_path)
    if not file_path.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ '{file_path}' ä¸å­˜åœ¨")
        sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    if file_path.suffix.lower() != '.docx':
        print(f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ '{file_path.suffix}'ï¼Œå½“å‰ä»…æ”¯æŒ.docxæ ¼å¼")
        sys.exit(1)
    
    try:
        print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
        
        # ä½¿ç”¨å…±ç”¨çš„æ–‡æ¡£è¯»å–å‡½æ•°
        text = _read_document_text(str(file_path))
        if not text:
            print("âŒ é”™è¯¯: æ— æ³•è¯»å–æ–‡æ¡£å†…å®¹")
            sys.exit(1)
            
        print(f"âœ… æ–‡æ¡£è¯»å–æˆåŠŸï¼Œå…± {len(text)} å­—ç¬¦")
        
        # åˆ›å»ºæå–å™¨å®ä¾‹
        extractor = ThesisExtractorPro()
        
        # æ‰§è¡Œæå–
        result = extractor.extract_with_integrated_strategy(text, str(file_path))
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if args.output:
            output_path = Path(args.output)
        else:
            output_path = file_path.with_suffix('.json').with_name(file_path.stem + '_extracted.json')
        
        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ æå–å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ€»å­—æ®µæ•°: {extractor.extraction_stats['total_fields']}")
        print(f"  - å·²æå–å­—æ®µ: {extractor.extraction_stats['extracted_fields']}")
        print(f"  - æå–å®Œæ•´åº¦: {extractor.extraction_stats['confidence']:.1%}")
        print(f"  - å¤„ç†æ—¶é—´: {extractor.extraction_stats['processing_time']:.3f}ç§’")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ æå–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def _read_document_text(file_path: str) -> Optional[str]:
    """
    è¯»å–Wordæ–‡æ¡£å¹¶æå–æ–‡æœ¬å†…å®¹çš„é€šç”¨å‡½æ•°
    
    Args:
        file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ–‡æ¡£æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        from pathlib import Path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
            
        # è¯»å–Wordæ–‡æ¡£
        doc = Document(str(file_path_obj))
        text = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
        
        if not text.strip():
            logger.error(f"æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path}")
            return None
            
        logger.info(f"âœ… æ–‡æ¡£è¯»å–æˆåŠŸï¼Œå…± {len(text)} å­—ç¬¦")
        return text
        
    except Exception as e:
        logger.error(f"è¯»å–æ–‡æ¡£å¤±è´¥: {e}")
        return None


def extract_sections_with_pro_strategy(file_path: str, use_cache: bool = True) -> Optional[Dict]:
    """
    ä½¿ç”¨ä¸“ä¸šç­–ç•¥æå–è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯çš„ç®€åŒ–æ¥å£
    
    Args:
        file_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
    Returns:
        æå–çš„ç»“æ„åŒ–ä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    try:
        # è¯»å–æ–‡æ¡£æ–‡æœ¬
        text = _read_document_text(file_path)
        if not text:
            return None
            
        # åˆ›å»ºä¸“ä¸šç‰ˆæå–å™¨
        extractor = ThesisExtractorPro()
        
        # ä½¿ç”¨é›†æˆç­–ç•¥è¿›è¡Œæå–
        result = extractor.extract_with_integrated_strategy(text, file_path)
        
        logger.info(f"âœ… ä¸“ä¸šç­–ç•¥æå–å®Œæˆï¼Œæå–å­—æ®µæ•°: {extractor.extraction_stats['extracted_fields']}")
        
        return result
        
    except Exception as e:
        logger.error(f"ä¸“ä¸šç­–ç•¥æå–å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    main()
