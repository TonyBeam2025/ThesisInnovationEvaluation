"""
è®ºæ–‡ç« èŠ‚åˆ†æ®µå¤„ç†æ¨¡å—
æŒ‰ç…§è®ºæ–‡é€»è¾‘ç« èŠ‚è¿›è¡Œåˆ†æ®µï¼Œè€Œä¸æ˜¯ç®€å•æŒ‰é•¿åº¦åˆ†æ®µ
"""

import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class PaperSection:
    """è®ºæ–‡ç« èŠ‚æ•°æ®ç»“æ„"""
    title: str  # ç« èŠ‚æ ‡é¢˜
    content: str  # ç« èŠ‚å†…å®¹
    section_type: str  # ç« èŠ‚ç±»å‹
    level: int  # ç« èŠ‚å±‚çº§ (1-æ ‡é¢˜, 2-ä¸€çº§æ ‡é¢˜, 3-äºŒçº§æ ‡é¢˜ç­‰)
    order: int  # ç« èŠ‚é¡ºåº

class PaperSectionParser:
    """è®ºæ–‡ç« èŠ‚è§£æå™¨"""
    
    def __init__(self):
        # å®šä¹‰è®ºæ–‡ç« èŠ‚æ¨¡å¼ - ä¸­è‹±æ–‡æ··åˆ
        self.section_patterns = {
            # æ‘˜è¦ç›¸å…³
            'abstract': [
                r'(?:æ‘˜\s*è¦|ABSTRACT|Abstract|abstract)',
                r'(?:ä¸­æ–‡æ‘˜è¦|è‹±æ–‡æ‘˜è¦)',
                r'(?:å…³é”®è¯|Keywords|å…³é”®å­—)',
            ],
            
            # å¼•è¨€/å¯¼è®º
            'introduction': [
                r'(?:å¼•\s*è¨€|å¯¼\s*è®º|å‰\s*è¨€|ç»ª\s*è®º)',
                r'(?:INTRODUCTION|Introduction|introduction)',
                r'(?:èƒŒæ™¯|ç ”ç©¶èƒŒæ™¯|Background)',
            ],
            
            # æ–‡çŒ®ç»¼è¿°/ç›¸å…³å·¥ä½œ
            'literature_review': [
                r'(?:æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|ç ”ç©¶ç°çŠ¶)',
                r'(?:å›½å†…å¤–ç ”ç©¶ç°çŠ¶|å›½å†…å¤–å‘å±•ç°çŠ¶)',
                r'(?:LITERATURE\s*REVIEW|Literature\s*Review|Related\s*Work)',
                r'(?:ç†è®ºåŸºç¡€|ç†è®ºèƒŒæ™¯)',
            ],
            
            # ç†è®ºæ¡†æ¶/ç³»ç»Ÿè®¾è®¡
            'theory_framework': [
                r'(?:ç†è®ºæ¡†æ¶|ç†è®ºåŸºç¡€|æ¦‚å¿µæ¡†æ¶)',
                r'(?:ç³»ç»Ÿè®¾è®¡|ç³»ç»Ÿæ¶æ„|æ¶æ„è®¾è®¡)',
                r'(?:THEORY|Theory|FRAMEWORK|Framework)',
                r'(?:è®¾è®¡æ€è·¯|è®¾è®¡åŸç†)',
            ],
            
            # æ–¹æ³•/ç®—æ³•
            'methodology': [
                r'(?:ç ”ç©¶æ–¹æ³•|æ–¹æ³•|ç®—æ³•|æ¨¡å‹)',
                r'(?:METHODOLOGY|Methodology|METHOD|Method)',
                r'(?:æŠ€æœ¯è·¯çº¿|å®ç°æ–¹æ¡ˆ|è§£å†³æ–¹æ¡ˆ)',
                r'(?:ç®—æ³•è®¾è®¡|æ¨¡å‹æ„å»º)',
            ],
            
            # å®éªŒ/å®ç°
            'experiment': [
                r'(?:å®éªŒ|è¯•éªŒ|æµ‹è¯•|éªŒè¯)',
                r'(?:EXPERIMENT|Experiment|IMPLEMENTATION|Implementation)',
                r'(?:å®éªŒè®¾è®¡|å®éªŒæ–¹æ¡ˆ|å®éªŒè¿‡ç¨‹)',
                r'(?:æ€§èƒ½æµ‹è¯•|åŠŸèƒ½æµ‹è¯•)',
            ],
            
            # ç»“æœ/åˆ†æ
            'results': [
                r'(?:ç»“æœ|å®éªŒç»“æœ|æµ‹è¯•ç»“æœ)',
                r'(?:RESULTS|Results|ANALYSIS|Analysis)',
                r'(?:æ•°æ®åˆ†æ|ç»“æœåˆ†æ|æ€§èƒ½åˆ†æ)',
                r'(?:è®¨è®º|Discussion)',
            ],
            
            # ç»“è®º
            'conclusion': [
                r'(?:ç»“è®º|æ€»ç»“|å°ç»“)',
                r'(?:CONCLUSION|Conclusion|SUMMARY|Summary)',
                r'(?:ç ”ç©¶ç»“è®º|ä¸»è¦ç»“è®º)',
                r'(?:å±•æœ›|æœªæ¥å·¥ä½œ|Future\s*Work)',
            ],
            
            # å‚è€ƒæ–‡çŒ®
            'references': [
                r'(?:å‚è€ƒæ–‡çŒ®|å¼•ç”¨æ–‡çŒ®|æ–‡çŒ®)',
                r'(?:REFERENCES|References|BIBLIOGRAPHY|Bibliography)',
            ],
            
            # é™„å½•
            'appendix': [
                r'(?:é™„å½•|APPENDIX|Appendix)',
            ]
        }
        
        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼ - æ£€æµ‹å„ç§ç« èŠ‚æ ‡é¢˜æ ¼å¼
        self.title_patterns = [
            # æ•°å­—ç¼–å·: 1. 1.1 1.1.1 ç­‰
            r'^(\d+(?:\.\d+)*\.?)\s*(.+?)(?:\s*\.{3,}.*)?$',
            # ä¸­æ–‡ç¼–å·: ä¸€ã€äºŒã€ä¸‰ã€ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰
            r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+[ã€ï¼.]|ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ï¼‰)\s*(.+?)(?:\s*\.{3,}.*)?$',
            # è‹±æ–‡ç¼–å·: I. II. III. A. B. C. ç­‰
            r'^([IVX]+[\.ã€]|[A-Z][\.ã€])\s*(.+?)(?:\s*\.{3,}.*)?$',
            # çº¯æ ‡é¢˜ï¼ˆæ— ç¼–å·ä½†æœ‰ç‰¹æ®Šæ ¼å¼ï¼‰
            r'^([A-Z][A-Z\s]+|[^\w\s]*(.{1,50})[^\w\s]*)$',
        ]
    
    def parse_sections(self, text: str) -> List[PaperSection]:
        """è§£æè®ºæ–‡æ–‡æœ¬ä¸ºç« èŠ‚åˆ—è¡¨"""
        logger.info("å¼€å§‹è§£æè®ºæ–‡ç« èŠ‚...")
        
        sections = []
        lines = text.split('\n')
        current_section = None
        section_order = 0
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                i += 1
                continue
            
            # æ£€æµ‹æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜
            section_info = self._detect_section_title(line)
            
            if section_info:
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_section and current_section.content.strip():
                    sections.append(current_section)
                
                # åˆ›å»ºæ–°ç« èŠ‚
                section_order += 1
                current_section = PaperSection(
                    title=section_info['title'],
                    content="",
                    section_type=section_info['type'],
                    level=section_info['level'],
                    order=section_order
                )
                logger.debug(f"å‘ç°ç« èŠ‚: {current_section.title} (ç±»å‹: {current_section.section_type})")
            else:
                # æ·»åŠ å†…å®¹åˆ°å½“å‰ç« èŠ‚
                if current_section:
                    current_section.content += line + '\n'
                else:
                    # åˆ›å»ºé»˜è®¤çš„å¼€å¤´ç« èŠ‚
                    if not sections:
                        section_order += 1
                        current_section = PaperSection(
                            title="è®ºæ–‡å¼€å¤´",
                            content=line + '\n',
                            section_type='header',
                            level=1,
                            order=section_order
                        )
            
            i += 1
        
        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
        if current_section and current_section.content.strip():
            sections.append(current_section)
        
        logger.info(f"å…±è§£æå‡º {len(sections)} ä¸ªç« èŠ‚")
        for section in sections:
            logger.debug(f"ç« èŠ‚: {section.title} ({len(section.content)} å­—ç¬¦, ç±»å‹: {section.section_type})")
        
        return sections
    
    def _detect_section_title(self, line: str) -> Optional[Dict]:
        """æ£€æµ‹æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜"""
        line_clean = line.strip()
        
        # é•¿åº¦è¿‡é•¿æˆ–è¿‡çŸ­çš„è¡Œä¸å¤ªå¯èƒ½æ˜¯æ ‡é¢˜
        if len(line_clean) > 100 or len(line_clean) < 2:
            return None
        
        # æ£€æµ‹å„ç§æ ‡é¢˜æ¨¡å¼
        for pattern in self.title_patterns:
            match = re.match(pattern, line_clean, re.IGNORECASE)
            if match:
                # æå–æ ‡é¢˜æ–‡æœ¬
                if len(match.groups()) >= 2 and match.group(2):
                    title_text = match.group(2).strip()
                else:
                    title_text = match.group(1).strip() if match.group(1) else line_clean
                
                # åˆ¤æ–­ç« èŠ‚ç±»å‹
                section_type = self._classify_section_type(title_text)
                
                # åˆ¤æ–­ç« èŠ‚å±‚çº§
                level = self._determine_section_level(match.group(1) if match.group(1) else "")
                
                return {
                    'title': title_text,
                    'type': section_type,
                    'level': level
                }
        
        # ç‰¹æ®Šæƒ…å†µï¼šæ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ˜¾çš„ç« èŠ‚å…³é”®è¯
        section_type = self._classify_section_type(line_clean)
        if section_type != 'unknown':
            return {
                'title': line_clean,
                'type': section_type,
                'level': 2
            }
        
        return None
    
    def _classify_section_type(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜å†…å®¹åˆ†ç±»ç« èŠ‚ç±»å‹"""
        title_lower = title.lower()
        
        for section_type, patterns in self.section_patterns.items():
            for pattern in patterns:
                if re.search(pattern, title, re.IGNORECASE):
                    return section_type
        
        return 'unknown'
    
    def _determine_section_level(self, number_part: str) -> int:
        """æ ¹æ®ç¼–å·ç¡®å®šç« èŠ‚å±‚çº§ï¼ˆé™åˆ¶åˆ°çº§åˆ«2ï¼‰"""
        if not number_part:
            return 2
        
        # è®¡ç®—æ•°å­—ç¼–å·çš„å±‚çº§ï¼Œä½†é™åˆ¶æœ€å¤§ä¸º2çº§
        dots = number_part.count('.')
        if dots == 0:
            return 1  # ä¸€çº§æ ‡é¢˜
        else:
            return 2  # äºŒçº§åŠä»¥ä¸‹ç»Ÿä¸€ä¸ºäºŒçº§
    
    def merge_small_sections(self, sections: List[PaperSection], min_length: int = 200) -> List[PaperSection]:
        """åˆå¹¶è¿‡å°çš„ç« èŠ‚åˆ°ç›¸é‚»ç« èŠ‚"""
        if not sections:
            return sections
        
        merged_sections = []
        current_section = sections[0]
        
        for i in range(1, len(sections)):
            next_section = sections[i]
            
            # å¦‚æœå½“å‰ç« èŠ‚å¤ªå°ï¼Œåˆå¹¶åˆ°ä¸‹ä¸€ä¸ªç« èŠ‚
            if len(current_section.content.strip()) < min_length and next_section.section_type != 'references':
                current_section.content += f"\n\n## {next_section.title}\n{next_section.content}"
                if current_section.section_type == 'unknown':
                    current_section.section_type = next_section.section_type
            else:
                merged_sections.append(current_section)
                current_section = next_section
        
        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
        merged_sections.append(current_section)
        
        logger.info(f"ç« èŠ‚åˆå¹¶åï¼š{len(sections)} -> {len(merged_sections)} ä¸ªç« èŠ‚")
        return merged_sections

class PaperSectionProcessor:
    """è®ºæ–‡ç« èŠ‚å¤„ç†å™¨ - åè°ƒè§£æå’ŒAIå¤„ç†"""
    
    def __init__(self, ai_client):
        self.ai_client = ai_client
        self.parser = PaperSectionParser()
    
    def create_section_batches(self, sections: List[PaperSection], 
                             max_chars: int = 10000) -> List[List[PaperSection]]:
        """
        å°†ç« èŠ‚æŒ‰å­—ç¬¦æ•°å’Œé€»è¾‘é¡ºåºæ‰“åŒ…æˆæ‰¹æ¬¡
        
        Args:
            sections: ç« èŠ‚åˆ—è¡¨ï¼ˆæŒ‰ç…§ç« èŠ‚é¡ºåºæ’åˆ—ï¼‰
            max_chars: æ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•°ï¼ˆé»˜è®¤1ä¸‡å­—ï¼‰
            
        Returns:
            æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«ç›¸é‚»çš„å¤šä¸ªç« èŠ‚
        """
        batches = []
        current_batch = []
        current_chars = 0
        
        logger.info(f"å¼€å§‹åˆ›å»ºç« èŠ‚æ‰¹æ¬¡ï¼Œæ€»ç« èŠ‚æ•°: {len(sections)}ï¼Œæ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•°: {max_chars:,}")
        
        for i, section in enumerate(sections):
            section_chars = len(section.content)
            
            # å¦‚æœå•ä¸ªç« èŠ‚å°±è¶…è¿‡é™åˆ¶ï¼Œå•ç‹¬æˆä¸€æ‰¹æ¬¡
            if section_chars > max_chars:
                # å…ˆä¿å­˜å½“å‰æ‰¹æ¬¡ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
                if current_batch:
                    batches.append(current_batch)
                    logger.info(f"æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªç« èŠ‚, {current_chars:,} å­—ç¬¦")
                    current_batch = []
                    current_chars = 0
                
                # å¤§ç« èŠ‚å•ç‹¬æˆæ‰¹æ¬¡
                batches.append([section])
                logger.info(f"æ‰¹æ¬¡ {len(batches)}: 1 ä¸ªå¤§ç« èŠ‚ ('{section.title}'), {section_chars:,} å­—ç¬¦")
                continue
            
            # å¦‚æœæ·»åŠ å½“å‰ç« èŠ‚ä¼šè¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰æ‰¹æ¬¡
            if current_chars + section_chars > max_chars and current_batch:
                batches.append(current_batch)
                logger.info(f"æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªç« èŠ‚, {current_chars:,} å­—ç¬¦")
                current_batch = []
                current_chars = 0
            
            # æ·»åŠ å½“å‰ç« èŠ‚åˆ°æ‰¹æ¬¡
            current_batch.append(section)
            current_chars += section_chars
            
            logger.debug(f"ç« èŠ‚ {i+1}: '{section.title}' ({section_chars:,} å­—ç¬¦) -> å½“å‰æ‰¹æ¬¡: {current_chars:,} å­—ç¬¦")
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batches.append(current_batch)
            logger.info(f"æ‰¹æ¬¡ {len(batches)}: {len(current_batch)} ä¸ªç« èŠ‚, {current_chars:,} å­—ç¬¦")
        
        logger.info(f"ç« èŠ‚æ‰¹æ¬¡åˆ›å»ºå®Œæˆï¼Œå…± {len(batches)} ä¸ªæ‰¹æ¬¡")
        return batches
    
    def process_paper_by_batches(self, text: str, session_id: Optional[str] = None, 
                               max_chars_per_batch: int = 10000) -> Optional[Dict[str, Any]]:
        """
        æŒ‰æ‰¹æ¬¡å¤„ç†è®ºæ–‡ï¼Œå°†ç›¸é‚»ç« èŠ‚æ‰“åŒ…æˆ1ä¸‡å­—ä»¥å†…çš„æ•°æ®åŒ…
        
        Args:
            text: è®ºæ–‡æ–‡æœ¬
            session_id: ä¼šè¯ID
            max_chars_per_batch: æ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            åˆå¹¶åçš„ç»“æ„åŒ–ä¿¡æ¯
        """
        logger.info("å¼€å§‹æŒ‰æ‰¹æ¬¡å¤„ç†è®ºæ–‡...")
        
        # 1. è§£æç« èŠ‚
        sections = self.parser.parse_sections(text)
        if not sections:
            logger.error("æ— æ³•è§£æè®ºæ–‡ç« èŠ‚")
            return None
        
        # 2. åˆå¹¶è¿‡å°çš„ç« èŠ‚
        sections = self.parser.merge_small_sections(sections, min_length=200)
        
        # 3. åˆ›å»ºç« èŠ‚æ‰¹æ¬¡
        batches = self.create_section_batches(sections, max_chars_per_batch)
        
        # 4. é€æ‰¹æ¬¡å¤„ç†
        all_results = {}
        total_batches = len(batches)
        
        for batch_idx, batch in enumerate(batches, 1):
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx}/{total_batches}: {len(batch)} ä¸ªç« èŠ‚")
            
            try:
                # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œé¿å…APIé™æµ
                if batch_idx > 1:
                    import time
                    time.sleep(2)  # 2ç§’é—´éš”
                
                batch_result = self._process_section_batch(batch, session_id)
                if batch_result:
                    # åˆå¹¶ç»“æœ
                    for key, value in batch_result.items():
                        if key in all_results:
                            if isinstance(value, str) and isinstance(all_results[key], str):
                                all_results[key] += f"\n\n{value}"
                            elif isinstance(value, list) and isinstance(all_results[key], list):
                                all_results[key].extend(value)
                            else:
                                all_results[key] = value
                        else:
                            all_results[key] = value
                    
                    logger.info(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†æˆåŠŸï¼Œç´¯è®¡å­—æ®µ: {len(all_results)}")
                else:
                    logger.warning(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å‡ºé”™: {e}")
                continue
        
        if all_results:
            logger.info(f"è®ºæ–‡æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå…±æå– {len(all_results)} ä¸ªå­—æ®µ")
            return all_results
        else:
            logger.error("æ‰€æœ‰æ‰¹æ¬¡å¤„ç†éƒ½å¤±è´¥")
            return None
    
    def _process_section_batch(self, batch: List[PaperSection], session_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†ä¸€ä¸ªç« èŠ‚æ‰¹æ¬¡
        
        Args:
            batch: ç« èŠ‚æ‰¹æ¬¡ï¼ˆç›¸é‚»çš„å¤šä¸ªç« èŠ‚ï¼‰
            session_id: ä¼šè¯ID
            
        Returns:
            æå–çš„ç»“æ„åŒ–ä¿¡æ¯
        """
        if not batch:
            return None
        
        # æ„å»ºæ‰¹æ¬¡å†…å®¹
        batch_content = ""
        section_titles = []
        
        for section in batch:
            section_titles.append(section.title)
            batch_content += f"\n\n## {section.title}\n{section.content}"
        
        batch_chars = len(batch_content)
        logger.info(f"å¤„ç†æ‰¹æ¬¡: {', '.join(section_titles)} ({batch_chars:,} å­—ç¬¦)")
        
        # æ„å»ºé’ˆå¯¹æ‰¹æ¬¡çš„æç¤ºè¯
        prompt = f"""è¯·ä»ä»¥ä¸‹è®ºæ–‡ç« èŠ‚å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚

è¿™ä¸ªæ‰¹æ¬¡åŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š
{', '.join(section_titles)}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœç« èŠ‚ä¸­åŒ…å«ç›¸å…³å†…å®¹ï¼‰ï¼š
{{
  "title_cn": "ä¸­æ–‡æ ‡é¢˜",
  "title_en": "è‹±æ–‡æ ‡é¢˜", 
  "abstract_cn": "ä¸­æ–‡æ‘˜è¦",
  "abstract_en": "è‹±æ–‡æ‘˜è¦",
  "keywords_cn": "ä¸­æ–‡å…³é”®è¯",
  "keywords_en": "è‹±æ–‡å…³é”®è¯",
  "literature_review": "æ–‡çŒ®ç»¼è¿°å†…å®¹",
  "research_methods": "ç ”ç©¶æ–¹æ³•",
  "theoretical_framework": "ç†è®ºæ¡†æ¶",
  "main_innovations": "ä¸»è¦åˆ›æ–°ç‚¹",
  "practical_problems": "å®è·µé—®é¢˜",
  "proposed_solutions": "æå‡ºçš„è§£å†³æ–¹æ¡ˆ",
  "research_conclusions": "ç ”ç©¶ç»“è®º",
  "application_value": "åº”ç”¨ä»·å€¼",
  "references": "å‚è€ƒæ–‡çŒ®åˆ—è¡¨"
}}

è¾“å‡ºè¦æ±‚ï¼š
- åªæå–å½“å‰æ‰¹æ¬¡ç« èŠ‚ä¸­åŒ…å«çš„ä¿¡æ¯
- å¦‚æœæŸé¡¹ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè¯·è¾“å‡ºç©ºå­—ç¬¦ä¸²""
- ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ä»£ç å—æ ‡è®°
- å­—ç¬¦ä¸²å€¼ä¸­ä¸è¦åŒ…å«æ§åˆ¶å­—ç¬¦

ç« èŠ‚å†…å®¹ï¼š
{batch_content}
"""
        
        try:
            # å‘é€è¯·æ±‚åˆ°AI
            response = self.ai_client.send_message(prompt, session_id=session_id)
            if not response or not response.content:
                logger.error("AIè¿”å›ç©ºå“åº”")
                return None
            
            response_text = response.content.strip()
            logger.debug(f"AIå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            
            # è§£æJSONå“åº”
            from .extract_sections_with_ai import _extract_json_from_response, _clean_json_content, _parse_json_with_fallback
            
            json_content = _extract_json_from_response(response_text)
            if not json_content:
                logger.error("æ— æ³•ä»å“åº”ä¸­æå–JSONå†…å®¹")
                return None
            
            cleaned_json = _clean_json_content(json_content)
            result = _parse_json_with_fallback(cleaned_json)
            
            if result:
                # è¿‡æ»¤æ‰ç©ºå€¼
                filtered_result = {k: v for k, v in result.items() if v and v.strip()}
                logger.info(f"æ‰¹æ¬¡å¤„ç†æˆåŠŸï¼Œæå–å­—æ®µ: {list(filtered_result.keys())}")
                return filtered_result
            else:
                logger.error("JSONè§£æå¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
            return None
    
    def process_paper_by_sections(self, text: str, session_id: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """æŒ‰ç« èŠ‚å¤„ç†è®ºæ–‡ï¼Œè¿”å›ç»“æ„åŒ–ä¿¡æ¯"""
        logger.info(f"ğŸš€ å¼€å§‹æŒ‰ç« èŠ‚å¤„ç†è®ºæ–‡ï¼Œæ–‡æœ¬é•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # 1. è§£æç« èŠ‚
        logger.info("ğŸ“– ç¬¬ä¸€æ­¥ï¼šè§£æè®ºæ–‡ç« èŠ‚ç»“æ„...")
        sections = self.parser.parse_sections(text)
        if not sections:
            logger.error("âŒ æ— æ³•è§£æè®ºæ–‡ç« èŠ‚")
            return None
        
        logger.info(f"âœ… è§£æåˆ° {len(sections)} ä¸ªç« èŠ‚")
        for i, section in enumerate(sections, 1):
            logger.info(f"   ç« èŠ‚ {i}: {section.title} ({section.section_type}, {len(section.content):,} å­—ç¬¦)")
        
        # 2. åˆå¹¶è¿‡å°çš„ç« èŠ‚
        logger.info("ğŸ”— ç¬¬äºŒæ­¥ï¼šåˆå¹¶è¿‡å°çš„ç« èŠ‚...")
        original_count = len(sections)
        sections = self.parser.merge_small_sections(sections, min_length=200)
        if len(sections) != original_count:
            logger.info(f"âœ… ç« èŠ‚åˆå¹¶å®Œæˆï¼Œä» {original_count} ä¸ªåˆå¹¶ä¸º {len(sections)} ä¸ª")
        else:
            logger.info("âœ… æ— éœ€åˆå¹¶ç« èŠ‚")
        
        # 3. æŒ‰ç« èŠ‚æå–ä¿¡æ¯
        logger.info(f"ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šé€ä¸ªå¤„ç†ç« èŠ‚å†…å®¹...")
        extraction_results = {}
        total_sections = len(sections)
        
        for i, section in enumerate(sections, 1):
            logger.info(f"ğŸ“ å¤„ç†ç« èŠ‚ {i}/{total_sections}: ã€Š{section.title}ã€‹")
            logger.info(f"   â”œâ”€ ç« èŠ‚ç±»å‹: {section.section_type}")
            logger.info(f"   â”œâ”€ å†…å®¹é•¿åº¦: {len(section.content):,} å­—ç¬¦")
            logger.info(f"   â””â”€ å¼€å§‹AIåˆ†æ...")
            
            try:
                section_info = self._extract_section_info(section, session_id)
                if section_info:
                    extraction_results[section.section_type] = section_info
                    logger.info(f"   âœ… ç« èŠ‚ã€Š{section.title}ã€‹å¤„ç†æˆåŠŸï¼Œæå–åˆ° {len(section_info)} ä¸ªå­—æ®µ")
                else:
                    logger.warning(f"   âŒ ç« èŠ‚ã€Š{section.title}ã€‹å¤„ç†å¤±è´¥ - AIè¿”å›ç©ºç»“æœ")
            except Exception as e:
                logger.error(f"   ğŸ’¥ å¤„ç†ç« èŠ‚ã€Š{section.title}ã€‹æ—¶å‡ºé”™: {e}")
                continue
        
        # 4. æ•´åˆæ‰€æœ‰ç« èŠ‚ä¿¡æ¯
        logger.info(f"ğŸ”— ç¬¬å››æ­¥ï¼šæ•´åˆæ‰€æœ‰ç« èŠ‚ä¿¡æ¯...")
        logger.info(f"   æˆåŠŸå¤„ç†çš„ç« èŠ‚: {list(extraction_results.keys())}")
        
        final_result = self._integrate_section_results(extraction_results, sections)
        
        if final_result:
            logger.info(f"ğŸ‰ è®ºæ–‡ç« èŠ‚å¤„ç†å®Œæˆï¼")
            logger.info(f"   â”œâ”€ å¤„ç†æˆåŠŸ: {len(extraction_results)}/{total_sections} ä¸ªç« èŠ‚")
            logger.info(f"   â”œâ”€ æœ€ç»ˆå­—æ®µ: {len(final_result)} ä¸ª")
            logger.info(f"   â””â”€ å­—æ®µåˆ—è¡¨: {list(final_result.keys())}")
        else:
            logger.error(f"âŒ ç« èŠ‚ä¿¡æ¯æ•´åˆå¤±è´¥")
        
        return final_result
    
    def _extract_section_info(self, section: PaperSection, session_id: Optional[str] = None) -> Optional[Dict]:
        """ä»å•ä¸ªç« èŠ‚æå–ä¿¡æ¯"""
        # æ ¹æ®ç« èŠ‚ç±»å‹ç”Ÿæˆç‰¹å®šçš„æç¤ºè¯
        logger.info(f"     ğŸ¤– ä¸ºç« èŠ‚ã€Š{section.title}ã€‹ç”ŸæˆAIæç¤ºè¯...")
        prompt = self._generate_section_prompt(section)
        prompt_length = len(prompt)
        logger.info(f"     ğŸ“ æç¤ºè¯é•¿åº¦: {prompt_length:,} å­—ç¬¦")
        
        try:
            logger.info(f"     ğŸ”— å‘é€AIè¯·æ±‚...")
            import time
            start_time = time.time()
            
            response = self.ai_client.send_message(prompt, session_id=session_id)
            
            elapsed_time = time.time() - start_time
            logger.info(f"     â±ï¸  AIå“åº”æ—¶é—´: {elapsed_time:.2f} ç§’")
            
            if response and response.content:
                response_length = len(response.content)
                logger.info(f"     ğŸ“¤ AIå“åº”é•¿åº¦: {response_length:,} å­—ç¬¦")
                logger.info(f"     ğŸ” å¼€å§‹è§£æAIå“åº”...")
                
                # è§£æAIè¿”å›çš„ç»“æ„åŒ–ä¿¡æ¯
                result = self._parse_section_response(response.content, section.section_type)
                if result:
                    logger.info(f"     âœ… ç« èŠ‚ä¿¡æ¯è§£ææˆåŠŸï¼Œæå–å­—æ®µ: {list(result.keys())}")
                else:
                    logger.warning(f"     âŒ AIå“åº”è§£æå¤±è´¥")
                return result
            else:
                logger.warning(f"     âŒ AIå¯¹ç« èŠ‚ã€Š{section.title}ã€‹è¿”å›ç©ºå“åº”")
                return None
        except Exception as e:
            logger.error(f"     ğŸ’¥ AIå¤„ç†ç« èŠ‚ã€Š{section.title}ã€‹å¤±è´¥: {e}")
            return None
    
    def _generate_section_prompt(self, section: PaperSection) -> str:
        """æ ¹æ®ç« èŠ‚ç±»å‹ç”Ÿæˆä¸“é—¨çš„æç¤ºè¯"""
        base_prompt = f"""è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ç« èŠ‚å†…å®¹ï¼Œå¹¶æå–å…³é”®ä¿¡æ¯ã€‚

ç« èŠ‚æ ‡é¢˜: {section.title}
ç« èŠ‚ç±»å‹: {section.section_type}

"""
        
        # æ ¹æ®ç« èŠ‚ç±»å‹å®šåˆ¶æç¤ºè¯
        if section.section_type == 'abstract':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "title_cn": "ä¸­æ–‡æ ‡é¢˜",
  "title_en": "è‹±æ–‡æ ‡é¢˜", 
  "abstract_cn": "ä¸­æ–‡æ‘˜è¦",
  "abstract_en": "è‹±æ–‡æ‘˜è¦",
  "keywords_cn": "ä¸­æ–‡å…³é”®è¯",
  "keywords_en": "è‹±æ–‡å…³é”®è¯"
}"""
        
        elif section.section_type == 'literature_review':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "literature_review": "å®Œæ•´çš„æ–‡çŒ®ç»¼è¿°å†…å®¹",
  "theoretical_framework": "ç†è®ºæ¡†æ¶å’ŒåŸºç¡€ç†è®º",
  "research_gap": "è¯†åˆ«çš„ç ”ç©¶ç©ºç™½",
  "key_references": "ä¸»è¦å‚è€ƒæ–‡çŒ®"
}"""
        
        elif section.section_type == 'methodology':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "research_methods": "å…·ä½“ç ”ç©¶æ–¹æ³•å’ŒæŠ€æœ¯è·¯å¾„",
  "theoretical_framework": "ç†è®ºæ¡†æ¶",
  "technical_approach": "æŠ€æœ¯å®ç°æ–¹æ¡ˆ",
  "tools": "ä½¿ç”¨çš„å·¥å…·å’Œå¹³å°"
}"""
        
        elif section.section_type == 'experiment':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "experiment_design": "å®éªŒè®¾è®¡æ–¹æ¡ˆ",
  "experiment_process": "å®éªŒè¿‡ç¨‹æè¿°",
  "data_collection": "æ•°æ®æ”¶é›†æ–¹æ³•",
  "validation_method": "éªŒè¯æ–¹æ³•"
}"""
        
        elif section.section_type == 'results':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "research_results": "ä¸»è¦ç ”ç©¶ç»“æœ",
  "data_analysis": "æ•°æ®åˆ†æç»“æœ",
  "performance_metrics": "æ€§èƒ½æŒ‡æ ‡",
  "key_findings": "å…³é”®å‘ç°"
}"""
        
        elif section.section_type == 'conclusion':
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "research_conclusions": "ä¸»è¦ç ”ç©¶ç»“è®º",
  "main_innovations": "ä¸»è¦åˆ›æ–°ç‚¹",
  "application_value": "åº”ç”¨ä»·å€¼å’Œå®é™…æ„ä¹‰",
  "future_work": "æœªæ¥å·¥ä½œå±•æœ›",
  "limitations": "ç ”ç©¶å±€é™æ€§"
}"""
        
        else:
            specific_prompt = """è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{
  "section_content": "ç« èŠ‚ä¸»è¦å†…å®¹æ€»ç»“",
  "key_points": "å…³é”®è¦ç‚¹",
  "important_info": "é‡è¦ä¿¡æ¯"
}"""
        
        full_prompt = base_prompt + specific_prompt + f"\n\nç« èŠ‚å†…å®¹:\n{section.content[:4000]}"  # é™åˆ¶é•¿åº¦é¿å…è¶…æ—¶
        return full_prompt
    
    def _parse_section_response(self, response_content: str, section_type: str) -> Optional[Dict]:
        """è§£æAIå“åº”ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯"""
        try:
            import json
            import re
            
            # æå–JSONå†…å®¹
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            else:
                logger.warning(f"æ— æ³•ä» {section_type} ç« èŠ‚å“åº”ä¸­æå–JSON")
                return None
        except Exception as e:
            logger.error(f"è§£æ {section_type} ç« èŠ‚å“åº”å¤±è´¥: {e}")
            return None
    
    def _integrate_section_results(self, section_results: Dict, sections: List[PaperSection]) -> Dict:
        """æ•´åˆæ‰€æœ‰ç« èŠ‚çš„æå–ç»“æœ"""
        integrated = {
            # åˆå§‹åŒ–æ‰€æœ‰æ ‡å‡†å­—æ®µ - ä½¿ç”¨æ–°çš„å‘½åè§„èŒƒ
            'title_cn': '',
            'title_en': '',
            'abstract_cn': '',
            'abstract_en': '',
            'keywords_cn': '',
            'keywords_en': '',
            'literature_review': '',
            'research_methods': '',
            'theoretical_framework': '',
            'main_innovations': '',
            'practical_problems': '',
            'proposed_solutions': '',
            'research_conclusions': '',
            'application_value': '',
            'references': ''
        }
        
        # æ•´åˆå„ç« èŠ‚çš„ä¿¡æ¯
        for section_type, section_data in section_results.items():
            if not section_data:
                continue
            
            # ç›´æ¥æ˜ å°„å­—æ®µ
            for key, value in section_data.items():
                if key in integrated and value:
                    if integrated[key]:
                        integrated[key] += f"\n\n{value}"
                    else:
                        integrated[key] = value
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæŸäº›å­—æ®µä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å­—æ®µæ¨æ–­
        if not integrated['practical_problems']:
            # ä»å¼•è¨€æˆ–ç»“è®ºä¸­æ¨æ–­å®è·µé—®é¢˜
            introduction_sections = [s for s in sections if s.section_type == 'introduction']
            if introduction_sections:
                integrated['practical_problems'] = f"æ ¹æ®å¼•è¨€åˆ†æï¼Œ{introduction_sections[0].content[:500]}..."
        
        if not integrated['proposed_solutions']:
            # ä»æ–¹æ³•ç« èŠ‚ä¸­æå–è§£å†³æ–¹æ¡ˆ
            if integrated['research_methods']:
                integrated['proposed_solutions'] = integrated['research_methods']
        
        # ç”Ÿæˆå‚è€ƒæ–‡çŒ®åˆ—è¡¨
        ref_sections = [s for s in sections if s.section_type == 'references']
        if ref_sections:
            integrated['references'] = ref_sections[0].content
        
        return integrated

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºç« èŠ‚å¤„ç†å™¨
def create_section_processor(ai_client):
    """åˆ›å»ºè®ºæ–‡ç« èŠ‚å¤„ç†å™¨å®ä¾‹"""
    return PaperSectionProcessor(ai_client)
