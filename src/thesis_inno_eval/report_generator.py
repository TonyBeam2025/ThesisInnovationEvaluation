#!/usr/bin/env python3
"""
MarkdownæŠ¥å‘Šç”Ÿæˆå™¨
ç”¨äºç”Ÿæˆè®ºæ–‡è¯„ä¼°çš„Markdownæ ¼å¼æŠ¥å‘Š
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import Counter
import re
from .config_manager import get_config_manager
from .literature_review_analyzer import LiteratureReviewAnalyzer
from .ai_client import ConcurrentAIClient

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class MarkdownReportGenerator:
    """Markdownè¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.config_mgr = get_config_manager()
        self.report_config = self.config_mgr.get_report_config()
        self.literature_analyzer = LiteratureReviewAnalyzer()  # åˆå§‹åŒ–æ–‡çŒ®ç»¼è¿°åˆ†æå™¨
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.ai_client = ConcurrentAIClient(max_workers=3, max_connections=5)
        try:
            self.ai_client.initialize()
            self.ai_enabled = True
            logger.info("AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œå°†ä½¿ç”¨AIé©±åŠ¨çš„åˆ›æ–°æ€§åˆ†æ")
        except Exception as e:
            logger.warning(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æ: {e}")
            self.ai_enabled = False
        
        # åˆ›æ–°æ€§åˆ†ææç¤ºè¯æ¨¡æ¿
        self.innovation_analysis_prompts = {
            'methodology': """
åŸºäºä»¥ä¸‹æ–‡çŒ®æ•°æ®ï¼Œåˆ†æç›®æ ‡è®ºæ–‡åœ¨æ–¹æ³•å­¦æ–¹é¢çš„åˆ›æ–°ç‚¹ï¼š

è®ºæ–‡ä¸»é¢˜ï¼š{thesis_title}
è®ºæ–‡å…³é”®è¯ï¼š{thesis_keywords}
è®ºæ–‡æ‘˜è¦ï¼š{thesis_abstract}

ç›¸å…³æ–‡çŒ®æ•°æ®ï¼š
{literature_data}

è¯·ä»ä»¥ä¸‹è§’åº¦æ·±å…¥åˆ†ææ–¹æ³•å­¦åˆ›æ–°ï¼š
1. åˆ›æ–°çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ï¼ˆä¸ç°æœ‰æ–‡çŒ®å¯¹æ¯”ï¼‰
2. ç°æœ‰æ–¹æ³•çš„æ”¹è¿›å’Œä¼˜åŒ–ï¼ˆå…·ä½“æ”¹è¿›ç‚¹ï¼‰
3. æ–¹æ³•è®ºçªç ´çš„æ„ä¹‰å’Œä»·å€¼

è¾“å‡ºæ ¼å¼ï¼šç®€æ´çš„åˆ†ææ–‡æœ¬ï¼Œçªå‡ºåˆ›æ–°ç‚¹å’Œå·®å¼‚åŒ–ä¼˜åŠ¿ã€‚
""",
            
            'theory': """
åŸºäºæ–‡çŒ®å¯¹æ¯”åˆ†æï¼Œè¯„ä¼°ç›®æ ‡è®ºæ–‡çš„ç†è®ºè´¡çŒ®ï¼š

è®ºæ–‡ç†è®ºæ¡†æ¶ï¼š{theoretical_framework}
ç›¸å…³ç†è®ºç ”ç©¶ï¼š{theory_literature}

è¯·åˆ†æï¼š
1. æ–°çš„ç†è®ºæ¡†æ¶æˆ–æ¨¡å‹çš„åŸåˆ›æ€§
2. å¯¹ç°æœ‰ç†è®ºçš„è¡¥å……å’Œå®Œå–„ç¨‹åº¦
3. ç†è®ºè´¡çŒ®çš„å­¦æœ¯ä»·å€¼å’Œå½±å“

é‡ç‚¹å…³æ³¨ï¼šç†è®ºåˆ›æ–°çš„ç‹¬ç‰¹æ€§å’Œåœ¨ç›¸å…³é¢†åŸŸçš„é¦–åˆ›æ€§ã€‚
""",
            
            'practice': """
è¯„ä¼°è®ºæ–‡çš„å®è·µä»·å€¼å’Œåº”ç”¨å‰æ™¯ï¼š

å®è·µé—®é¢˜ï¼š{practical_problems}
è§£å†³æ–¹æ¡ˆï¼š{proposed_solutions}
åº”ç”¨èƒŒæ™¯æ–‡çŒ®ï¼š{application_literature}

åˆ†æè¦ç‚¹ï¼š
1. è§£å†³å®é™…é—®é¢˜çš„æ–°æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§
2. æŠ€æœ¯æˆæœçš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºä»·å€¼
3. ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆçš„æ¯”è¾ƒä¼˜åŠ¿

è¾“å‡ºï¼šçªå‡ºå®è·µåˆ›æ–°å’Œåº”ç”¨ä»·å€¼çš„å…·ä½“è¡¨ç°ã€‚
"""
        }
    
    def generate_evaluation_report(self, input_file: str, output_dir: Optional[str] = None, 
                                 thesis_extracted_info: Optional[Dict[str, str]] = None,
                                 papers_by_lang: Optional[Dict[str, List]] = None,
                                 literature_metadata_analysis: Optional[Dict] = None) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            input_file: è¾“å…¥è®ºæ–‡æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„è¾“å‡ºç›®å½•
            thesis_extracted_info: é€šè¿‡extract_sections_with_aiæŠ½å–çš„è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯
            papers_by_lang: CNKIæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®ï¼ŒæŒ‰è¯­è¨€åˆ†ç±»
            literature_metadata_analysis: æ–‡çŒ®å…ƒæ•°æ®åˆ†æç»“æœ
            
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if output_dir is None:
            output_dir = self.config_mgr.get_output_dir()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
        report_filename = self.config_mgr.generate_output_filename(
            input_file, 'evaluation_report'
        )
        report_path = os.path.join(output_dir, report_filename)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹ï¼Œä¼ é€’æŠ½å–çš„è®ºæ–‡ä¿¡æ¯å’Œæ£€ç´¢åˆ°çš„æ–‡çŒ®
        report_content = self._generate_report_content(input_file, thesis_extracted_info, papers_by_lang)
        
        # å†™å…¥æ–‡ä»¶
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        return report_path
    
    def _analyze_literature_themes(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ–‡çŒ®ä¸»é¢˜åˆ†å¸ƒ"""
        theme_analysis = {
            'chinese_themes': {},
            'english_themes': {},
            'year_distribution': {},
            'research_trends': []
        }
        
        # åˆ†æä¸­æ–‡æ–‡çŒ®ä¸»é¢˜
        if analysis_data['top_chinese']:
            for paper in analysis_data['top_chinese']:
                keywords = paper.get('KeyWords', '').split(';;')
                for keyword in keywords:
                    keyword = keyword.strip()
                    if keyword:
                        theme_analysis['chinese_themes'][keyword] = \
                            theme_analysis['chinese_themes'].get(keyword, 0) + 1
        
        # åˆ†æè‹±æ–‡æ–‡çŒ®ä¸»é¢˜
        if analysis_data['top_english']:
            for paper in analysis_data['top_english']:
                keywords = paper.get('KeyWords', '').split(';')
                for keyword in keywords:
                    keyword = keyword.strip()
                    if keyword:
                        theme_analysis['english_themes'][keyword] = \
                            theme_analysis['english_themes'].get(keyword, 0) + 1
        
        # åˆ†æå¹´ä»½åˆ†å¸ƒ
        all_papers = []
        if analysis_data['top_chinese']:
            all_papers.extend(analysis_data['top_chinese'])
        if analysis_data['top_english']:
            all_papers.extend(analysis_data['top_english'])
        
        for paper in all_papers:
            year = paper.get('PublicationYear', '')
            if year:
                try:
                    year_int = int(year)
                    theme_analysis['year_distribution'][year_int] = \
                        theme_analysis['year_distribution'].get(year_int, 0) + 1
                except ValueError:
                    continue
        
        return theme_analysis
    
    def _generate_innovation_analysis(self, analysis_data: Dict[str, Any], 
                                    theme_analysis: Dict[str, Any],
                                    thesis_extracted_info: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        """åŸºäºæ–‡çŒ®å¯¹æ¯”ç”Ÿæˆåˆ›æ–°æ€§åˆ†æ"""
        
        # æå–è®ºæ–‡åŸºæœ¬ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ä¼ é€’çš„æŠ½å–ä¿¡æ¯
        thesis_info = self._extract_thesis_info(analysis_data, thesis_extracted_info)
        
        innovation_analysis = {}
        
        if self.ai_enabled:
            # ä½¿ç”¨AIé©±åŠ¨çš„åˆ†æ
            logger.info("ä½¿ç”¨AIé©±åŠ¨çš„åˆ›æ–°æ€§åˆ†æ")
            innovation_analysis = self._generate_ai_innovation_analysis(thesis_info, analysis_data, theme_analysis)
        else:
            # AIæœªå¯ç”¨æ—¶è¿”å›ç©ºåˆ†æ
            logger.warning("AIæœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œåˆ›æ–°æ€§åˆ†æ")
            innovation_analysis = {
                'methodology': 'éœ€è¦å¯ç”¨AIæœåŠ¡æ‰èƒ½è¿›è¡Œæ–¹æ³•å­¦åˆ›æ–°åˆ†æ',
                'theory': 'éœ€è¦å¯ç”¨AIæœåŠ¡æ‰èƒ½è¿›è¡Œç†è®ºè´¡çŒ®åˆ†æ',
                'practice': 'éœ€è¦å¯ç”¨AIæœåŠ¡æ‰èƒ½è¿›è¡Œå®è·µä»·å€¼åˆ†æ'
            }
        
        return innovation_analysis
    
    def _generate_ai_innovation_analysis(self, thesis_info: Dict[str, str], 
                                       analysis_data: Dict[str, Any],
                                       theme_analysis: Dict[str, Any]) -> Dict[str, str]:
        """ä½¿ç”¨AIç”Ÿæˆåˆ›æ–°æ€§åˆ†æï¼ˆ64K Tokenæ™ºèƒ½ç®¡ç†ç‰ˆï¼‰"""
        
        innovation_analysis = {}
        
        try:
            # Step 1: æ„å»ºç²¾ç®€çš„æ ¸å¿ƒä¿¡æ¯
            core_thesis_info = self._extract_core_thesis_info(thesis_info)
            condensed_literature_context = self._build_condensed_literature_context(analysis_data, theme_analysis)
            
            # Step 2: ä½¿ç”¨åˆ†å±‚åˆ†æç­–ç•¥ï¼Œæ¯ä¸ªç»´åº¦ç‹¬ç«‹åˆ†æé¿å…Tokenç´¯ç§¯
            methodology_analysis = self._analyze_with_token_limit(
                'methodology', core_thesis_info, condensed_literature_context
            )
            theory_analysis = self._analyze_with_token_limit(
                'theory', core_thesis_info, condensed_literature_context
            )
            practice_analysis = self._analyze_with_token_limit(
                'practice', core_thesis_info, condensed_literature_context
            )
            
            # å¤„ç†ç»“æœ
            innovation_analysis['methodology'] = methodology_analysis if methodology_analysis else 'æ–¹æ³•å­¦åˆ›æ–°åˆ†æå¤±è´¥'
            innovation_analysis['theory'] = theory_analysis if theory_analysis else 'ç†è®ºè´¡çŒ®åˆ†æå¤±è´¥'
            innovation_analysis['practice'] = practice_analysis if practice_analysis else 'å®è·µä»·å€¼åˆ†æå¤±è´¥'
            
        except Exception as e:
            logger.error(f"AIåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯å›é€€
            innovation_analysis = {
                'methodology': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}',
                'theory': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}',
                'practice': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}'
            }
        
        return innovation_analysis
    
    def _extract_core_thesis_info(self, thesis_info: Dict[str, str]) -> Dict[str, str]:
        """æå–è®ºæ–‡æ ¸å¿ƒä¿¡æ¯ï¼Œæ§åˆ¶Tokenæ¶ˆè€—"""
        
        core_info = {}
        
        # è®ºæ–‡æ ‡é¢˜ï¼ˆå¿…éœ€ï¼‰
        core_info['title'] = thesis_info.get('title', 'æœªæä¾›')[:200]  # é™åˆ¶200å­—ç¬¦
        
        # å…³é”®è¯ï¼ˆå‹ç¼©ï¼‰
        keywords_cn = thesis_info.get('keywords_cn', '')[:150]
        keywords_en = thesis_info.get('keywords_en', '')[:150]
        core_info['keywords'] = f"{keywords_cn} | {keywords_en}"
        
        # ä¸»è¦åˆ›æ–°ç‚¹ï¼ˆæ ¸å¿ƒï¼‰
        core_info['innovations'] = thesis_info.get('main_innovations', 'æœªæä¾›')[:300]
        
        # ç ”ç©¶æ–¹æ³•ï¼ˆç²¾ç®€ï¼‰
        core_info['methodology'] = thesis_info.get('methodology', 'æœªæä¾›')[:250]
        
        # ç†è®ºæ¡†æ¶ï¼ˆç²¾ç®€ï¼‰
        core_info['theory_framework'] = thesis_info.get('theoretical_framework', 'æœªæä¾›')[:250]
        
        # å®é™…é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼ˆå®è·µç»´åº¦ï¼‰
        core_info['problems'] = thesis_info.get('practical_problems', 'æœªæä¾›')[:200]
        core_info['solutions'] = thesis_info.get('proposed_solutions', 'æœªæä¾›')[:200]
        
        return core_info
    
    def _build_condensed_literature_context(self, analysis_data: Dict[str, Any], theme_analysis: Dict[str, Any]) -> str:
        """æ„å»ºç²¾ç®€çš„æ–‡çŒ®èƒŒæ™¯ä¿¡æ¯ï¼Œä¸¥æ ¼æ§åˆ¶Tokenæ¶ˆè€—"""
        
        context_parts = []
        
        # åŸºç¡€ç»Ÿè®¡ï¼ˆå¿…éœ€ï¼‰
        total_chinese = len(analysis_data['top_chinese']) if analysis_data['top_chinese'] else 0
        total_english = len(analysis_data['top_english']) if analysis_data['top_english'] else 0
        context_parts.append(f"æ–‡çŒ®ç»Ÿè®¡: ä¸­æ–‡{total_chinese}ç¯‡, è‹±æ–‡{total_english}ç¯‡")
        
        # Top5ä¸»é¢˜ï¼ˆç²¾ç®€ï¼‰
        if theme_analysis.get('chinese_themes'):
            top_chinese_themes = sorted(theme_analysis['chinese_themes'].items(), key=lambda x: x[1], reverse=True)[:5]
            themes_str = ', '.join([f"{theme}({count})" for theme, count in top_chinese_themes])
            context_parts.append(f"ä¸­æ–‡ä¸»é¢˜: {themes_str}")
        
        if theme_analysis.get('english_themes'):
            top_english_themes = sorted(theme_analysis['english_themes'].items(), key=lambda x: x[1], reverse=True)[:5]
            themes_str = ', '.join([f"{theme}({count})" for theme, count in top_english_themes])
            context_parts.append(f"è‹±æ–‡ä¸»é¢˜: {themes_str}")
        
        # å¹´ä»½ä¿¡æ¯ï¼ˆç²¾ç®€ï¼‰
        if theme_analysis.get('year_distribution'):
            recent_years = [year for year in theme_analysis['year_distribution'].keys() if year >= 2020]
            if recent_years:
                recent_count = sum(theme_analysis['year_distribution'][year] for year in recent_years)
                total_papers = sum(theme_analysis['year_distribution'].values())
                recent_percentage = (recent_count / total_papers) * 100
                context_parts.append(f"è¿‘5å¹´æ–‡çŒ®: {recent_percentage:.0f}%")
        
        # ä»£è¡¨æ€§è®ºæ–‡ï¼ˆä»…æ ‡é¢˜ï¼Œæœ€å¤š3ç¯‡ï¼‰
        if analysis_data.get('top_chinese'):
            chinese_titles = [paper.get('Title', '')[:50] + '...' for paper in analysis_data['top_chinese'][:2]]
            context_parts.append(f"ä»£è¡¨ä¸­æ–‡æ–‡çŒ®: {'; '.join(chinese_titles)}")
        
        if analysis_data.get('top_english'):
            english_titles = [paper.get('Title', '')[:50] + '...' for paper in analysis_data['top_english'][:2]]
            context_parts.append(f"ä»£è¡¨è‹±æ–‡æ–‡çŒ®: {'; '.join(english_titles)}")
        
        # ç»„åˆå¹¶é™åˆ¶æ€»é•¿åº¦
        full_context = '\n'.join(context_parts)
        if len(full_context) > 800:  # ä¸¥æ ¼é™åˆ¶åœ¨800å­—ç¬¦ä»¥å†…
            full_context = full_context[:800] + '...'
        
        return full_context
    
    def _analyze_with_token_limit(self, analysis_type: str, core_thesis_info: Dict[str, str], 
                                 condensed_context: str) -> str:
        """åœ¨Tokené™åˆ¶å†…è¿›è¡Œåˆ†æ"""
        
        try:
            if analysis_type == 'methodology':
                prompt = f"""ä½ æ˜¯å­¦æœ¯æ–¹æ³•è®ºä¸“å®¶ã€‚è¯·åˆ†æè®ºæ–‡çš„æ–¹æ³•å­¦åˆ›æ–°ï¼š

**è®ºæ–‡**: {core_thesis_info['title']}
**æ–¹æ³•**: {core_thesis_info['methodology']}
**åˆ›æ–°ç‚¹**: {core_thesis_info['innovations']}

**æ–‡çŒ®èƒŒæ™¯**: {condensed_context}

è¯·ä»ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§ã€æŠ€æœ¯è·¯å¾„çªç ´ã€æ–¹æ³•è®ºè´¡çŒ®ä¸‰ä¸ªè§’åº¦ç®€è¦åˆ†æï¼Œ200-300å­—ã€‚"""

            elif analysis_type == 'theory':
                prompt = f"""ä½ æ˜¯ç†è®ºç ”ç©¶ä¸“å®¶ã€‚è¯·åˆ†æè®ºæ–‡çš„ç†è®ºè´¡çŒ®ï¼š

**è®ºæ–‡**: {core_thesis_info['title']}
**ç†è®ºæ¡†æ¶**: {core_thesis_info['theory_framework']}
**åˆ›æ–°ç‚¹**: {core_thesis_info['innovations']}

**æ–‡çŒ®èƒŒæ™¯**: {condensed_context}

è¯·ä»ç†è®ºåˆ›æ–°æ€§ã€ç†è®ºæ•´åˆæ€§ã€å­¦ç§‘å½±å“åŠ›ä¸‰ä¸ªè§’åº¦ç®€è¦åˆ†æï¼Œ200-300å­—ã€‚"""

            elif analysis_type == 'practice':
                prompt = f"""ä½ æ˜¯äº§å­¦ç ”ä¸“å®¶ã€‚è¯·åˆ†æè®ºæ–‡çš„å®è·µä»·å€¼ï¼š

**è®ºæ–‡**: {core_thesis_info['title']}
**å®é™…é—®é¢˜**: {core_thesis_info['problems']}
**è§£å†³æ–¹æ¡ˆ**: {core_thesis_info['solutions']}

**æ–‡çŒ®èƒŒæ™¯**: {condensed_context}

è¯·ä»é—®é¢˜è§£å†³èƒ½åŠ›ã€åº”ç”¨å‰æ™¯ã€ç¤¾ä¼šç»æµä»·å€¼ä¸‰ä¸ªè§’åº¦ç®€è¦åˆ†æï¼Œ200-300å­—ã€‚"""

            else:
                return f"{analysis_type}åˆ†æç±»å‹ä¸æ”¯æŒ"
            
            # ä½¿ç”¨ç‹¬ç«‹ä¼šè¯é¿å…ä¸Šä¸‹æ–‡ç´¯ç§¯
            session_id = f'{analysis_type}_analysis_{hash(prompt) % 10000}'
            response = self.ai_client.send_message(prompt, session_id=session_id)
            
            if response and response.content:
                return response.content.strip()
            else:
                return f"{analysis_type}åˆ†ææ— å“åº”"
                
        except Exception as e:
            logger.error(f"{analysis_type}åˆ†æå¤±è´¥: {e}")
            return f"{analysis_type}åˆ†æå¤±è´¥: {str(e)}"
    
    def _fallback_independent_analysis(self, thesis_info: Dict[str, str], literature_context: str) -> Dict[str, str]:
        """å¤‡ç”¨çš„ç‹¬ç«‹åˆ†ææ–¹æ³•ï¼ˆå½“å…±äº«ä¸Šä¸‹æ–‡å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        
        innovation_analysis = {}
        
        try:
            # ä½¿ç”¨ç²¾ç®€çš„æç¤ºè¯è¿›è¡Œç‹¬ç«‹åˆ†æ
            methodology_prompt = self._analyze_methodology_innovation_compact(thesis_info, literature_context)
            theory_prompt = self._analyze_theory_contribution_compact(thesis_info, literature_context)  
            practice_prompt = self._analyze_practice_value_compact(thesis_info, literature_context)
            
            # ä½¿ç”¨ç‹¬ç«‹ä¼šè¯
            methodology_response = self.ai_client.send_message(methodology_prompt, session_id='methodology_fallback')
            theory_response = self.ai_client.send_message(theory_prompt, session_id='theory_fallback')
            practice_response = self.ai_client.send_message(practice_prompt, session_id='practice_fallback')
            
            # å¤„ç†ç»“æœ
            if methodology_response and methodology_response.content:
                innovation_analysis['methodology'] = methodology_response.content
            else:
                innovation_analysis['methodology'] = 'æ–¹æ³•å­¦åˆ›æ–°åˆ†æå¤±è´¥'
            
            if theory_response and theory_response.content:
                innovation_analysis['theory'] = theory_response.content
            else:
                innovation_analysis['theory'] = 'ç†è®ºè´¡çŒ®åˆ†æå¤±è´¥'
            
            if practice_response and practice_response.content:
                innovation_analysis['practice'] = practice_response.content
            else:
                innovation_analysis['practice'] = 'å®è·µä»·å€¼åˆ†æå¤±è´¥'
            
        except Exception as e:
            logger.error(f"å¤‡ç”¨åˆ†æä¹Ÿå¤±è´¥: {e}")
            innovation_analysis = {
                'methodology': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}',
                'theory': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}',
                'practice': f'AIåˆ†æå‡ºç°é”™è¯¯: {str(e)}'
            }
        
        return innovation_analysis
    
    def _analyze_methodology_innovation(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """åˆ†ææ–¹æ³•å­¦åˆ›æ–°ï¼ˆç‹¬ç«‹ä¼šè¯ï¼‰"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯æ–¹æ³•è®ºä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°ç ”ç©¶æ–¹æ³•çš„åˆ›æ–°æ€§ã€‚

**è®ºæ–‡ä¿¡æ¯**ï¼š
- æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ç ”ç©¶æ–¹æ³•ï¼š{thesis_info.get('methodology', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- å…³é”®è¯ï¼š{thesis_info.get('keywords_cn', 'æœªæä¾›')}

**æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

è¯·ä»ä¸“ä¸šè§’åº¦åˆ†æè¯¥è®ºæ–‡çš„æ–¹æ³•å­¦åˆ›æ–°ï¼š

1. **ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§**ï¼šä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æœ‰ä½•çªç ´ï¼Ÿ
2. **æŠ€æœ¯è·¯å¾„åˆ›æ–°**ï¼šæŠ€æœ¯å®ç°ä¸Šçš„ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
3. **æ–¹æ³•è®ºè´¡çŒ®**ï¼šä¸ºç ”ç©¶é¢†åŸŸæä¾›äº†ä»€ä¹ˆæ–°å·¥å…·ï¼Ÿ

è¦æ±‚ï¼šå­¦æœ¯å®¢è§‚ï¼Œçªå‡ºåˆ›æ–°ç‚¹ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _analyze_theory_contribution(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """åˆ†æç†è®ºè´¡çŒ®ï¼ˆç‹¬ç«‹ä¼šè¯ï¼‰"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç†è®ºç ”ç©¶ä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°å­¦æœ¯ç ”ç©¶çš„ç†è®ºè´¡çŒ®ã€‚

**è®ºæ–‡ä¿¡æ¯**ï¼š
- æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ç†è®ºæ¡†æ¶ï¼š{thesis_info.get('theoretical_framework', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- ç ”ç©¶ç»“è®ºï¼š{thesis_info.get('research_conclusions', 'æœªæä¾›')}

**æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

è¯·ä»ç†è®ºå»ºæ„è§’åº¦åˆ†æè¯¥è®ºæ–‡çš„è´¡çŒ®ï¼š

1. **ç†è®ºåˆ›æ–°æ€§**ï¼šæ˜¯å¦æå‡ºæ–°çš„ç†è®ºæ¡†æ¶æˆ–æ¦‚å¿µï¼Ÿ
2. **ç†è®ºæ•´åˆæ€§**ï¼šå¦‚ä½•æ•´åˆä¸åŒç†è®ºè§†è§’ï¼Ÿ
3. **ç†è®ºå½±å“åŠ›**ï¼šå¯¹å­¦ç§‘å‘å±•çš„æ„ä¹‰ï¼Ÿ

è¦æ±‚ï¼šä¸¥è°¨å®¢è§‚ï¼Œçªå‡ºç†è®ºåˆ›æ–°ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _analyze_practice_value(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """åˆ†æå®è·µä»·å€¼ï¼ˆç‹¬ç«‹ä¼šè¯ï¼‰"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å­¦ç ”ä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°ç ”ç©¶æˆæœçš„å®è·µåº”ç”¨ä»·å€¼ã€‚

**è®ºæ–‡ä¿¡æ¯**ï¼š
- æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- å®é™…é—®é¢˜ï¼š{thesis_info.get('practical_problems', 'æœªæä¾›')}
- è§£å†³æ–¹æ¡ˆï¼š{thesis_info.get('proposed_solutions', 'æœªæä¾›')}
- åº”ç”¨ä»·å€¼ï¼š{thesis_info.get('application_value', 'æœªæä¾›')}

**æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

è¯·ä»å®è·µåº”ç”¨è§’åº¦åˆ†æè¯¥è®ºæ–‡çš„ä»·å€¼ï¼š

1. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šè§£å†³äº†ä»€ä¹ˆå®é™…é—®é¢˜ï¼Ÿ
2. **åº”ç”¨å‰æ™¯è¯„ä¼°**ï¼šå¸‚åœºåº”ç”¨æ½œåŠ›å¦‚ä½•ï¼Ÿ
3. **ç¤¾ä¼šç»æµä»·å€¼**ï¼šèƒ½åˆ›é€ ä»€ä¹ˆä»·å€¼ï¼Ÿ

è¦æ±‚ï¼šå®ç”¨å®¢è§‚ï¼Œçªå‡ºåº”ç”¨ä»·å€¼ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _extract_thesis_info(self, analysis_data: Dict[str, Any], 
                           thesis_extracted_info: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        """æå–è®ºæ–‡åŸºæœ¬ä¿¡æ¯"""
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ é€’çš„æŠ½å–ä¿¡æ¯
        if thesis_extracted_info:
            print("ä½¿ç”¨ä¼ é€’çš„è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯")
            return {
                'title': thesis_extracted_info.get('title_cn', '') or thesis_extracted_info.get('ChineseTitle', ''),
                'keywords_cn': thesis_extracted_info.get('keywords_cn', '') or thesis_extracted_info.get('ChineseKeywords', ''),
                'keywords_en': thesis_extracted_info.get('keywords_en', '') or thesis_extracted_info.get('EnglishKeywords', ''),
                'keywords': thesis_extracted_info.get('keywords_cn', '') or thesis_extracted_info.get('ChineseKeywords', ''),  # å…¼å®¹æ—§ä»£ç 
                'abstract': thesis_extracted_info.get('abstract_cn', '') or thesis_extracted_info.get('ChineseAbstract', ''),
                'methodology': thesis_extracted_info.get('ResearchMethods', ''),
                'theoretical_framework': thesis_extracted_info.get('TheoreticalFramework', ''),
                'practical_problems': thesis_extracted_info.get('PracticalProblems', ''),
                'main_innovations': thesis_extracted_info.get('MainInnovations', ''),
                'proposed_solutions': thesis_extracted_info.get('ProposedSolutions', ''),
                'research_conclusions': thesis_extracted_info.get('ResearchConclusions', ''),
                'application_value': thesis_extracted_info.get('ApplicationValue', '')
            }
        else:
            # å¦‚æœæ²¡æœ‰ä¼ é€’æŠ½å–ä¿¡æ¯ï¼Œç›´æ¥è¿”å›ç©ºå€¼ï¼Œä¸ä½¿ç”¨ç¼“å­˜ä¹Ÿä¸è¿›è¡Œç°åœºæŠ½å–
            print("è­¦å‘Šï¼šæœªä¼ é€’è®ºæ–‡æŠ½å–ä¿¡æ¯ï¼Œå°†ä½¿ç”¨ç©ºå€¼ç”Ÿæˆé€šç”¨åˆ†æ")
            return {
                'title': '',
                'keywords': '',
                'abstract': '',
                'methodology': '',
                'theoretical_framework': '',
                'practical_problems': '',
                'main_innovations': '',
                'proposed_solutions': '',
                'research_conclusions': '',
                'application_value': ''
            }
    
    def _build_theory_context(self, analysis_data: Dict[str, Any], 
                            theme_analysis: Dict[str, Any]) -> str:
        """æ„å»ºç†è®ºåˆ†æä¸Šä¸‹æ–‡"""
        context = "ç›¸å…³ç†è®ºç ”ç©¶ç°çŠ¶ï¼š\n"
        
        theory_keywords = ['risk management', 'internal control', 'governance', 'framework',
                          'theory', 'é£é™©ç®¡ç†', 'å†…éƒ¨æ§åˆ¶', 'æ²»ç†', 'ç†è®ºæ¡†æ¶']
        
        for keyword in theory_keywords:
            if keyword in theme_analysis['chinese_themes']:
                context += f"- {keyword}: {theme_analysis['chinese_themes'][keyword]}ç¯‡ä¸­æ–‡æ–‡çŒ®\n"
            if keyword in theme_analysis['english_themes']:
                context += f"- {keyword}: {theme_analysis['english_themes'][keyword]}ç¯‡è‹±æ–‡æ–‡çŒ®\n"
        
        return context
    
    def _build_practice_context(self, analysis_data: Dict[str, Any], 
                              theme_analysis: Dict[str, Any]) -> str:
        """æ„å»ºå®è·µä»·å€¼åˆ†æä¸Šä¸‹æ–‡"""
        context = "å®è·µåº”ç”¨ç ”ç©¶ç°çŠ¶ï¼š\n"
        
        practice_keywords = ['telemedicine', 'internet hospital', 'healthcare', 'implementation',
                           'application', 'äº’è”ç½‘åŒ»é™¢', 'è¿œç¨‹åŒ»ç–—', 'åŒ»ç–—å¥åº·', 'åº”ç”¨']
        
        for keyword in practice_keywords:
            if keyword in theme_analysis['chinese_themes']:
                context += f"- {keyword}å®è·µç ”ç©¶: {theme_analysis['chinese_themes'][keyword]}ç¯‡\n"
            if keyword in theme_analysis['english_themes']:
                context += f"- {keyword}åº”ç”¨ç ”ç©¶: {theme_analysis['english_themes'][keyword]}ç¯‡\n"
        
        return context
    
    def _analyze_methodological_innovation(self, thesis_info: Dict[str, str], 
                                         context: str) -> str:
        """åˆ†ææ–¹æ³•å­¦åˆ›æ–°ï¼ˆåŸºäºè§„åˆ™çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        methodology = thesis_info.get('methodology', '')
        main_innovations = thesis_info.get('main_innovations', '')
        
        analysis = ""
        
        # åŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œé€šç”¨æ–¹æ³•å­¦åˆ†æ
        if methodology:
            # æ£€æµ‹é‡åŒ–æ–¹æ³•
            if any(keyword in methodology.lower() for keyword in ['é‡åŒ–', 'å®šé‡', 'quantitative', 'ç»Ÿè®¡', 'statistical', 'æ•°å€¼', 'è®¡é‡']):
                analysis += """**é‡åŒ–ç ”ç©¶æ–¹æ³•åˆ›æ–°**ï¼šæœ¬ç ”ç©¶é‡‡ç”¨é‡åŒ–åˆ†ææ–¹æ³•ï¼Œé€šè¿‡æ•°å€¼åŒ–æŒ‡æ ‡å’Œç»Ÿè®¡åˆ†ææŠ€æœ¯ï¼Œä¸ºç ”ç©¶é—®é¢˜æä¾›äº†å®¢è§‚ã€å¯é‡å¤çš„åˆ†ææ¡†æ¶ï¼Œç›¸æ¯”ä¼ ç»Ÿå®šæ€§ç ”ç©¶å…·æœ‰æ›´å¼ºçš„ç§‘å­¦æ€§å’Œå¯éªŒè¯æ€§ã€‚

"""
            
            # æ£€æµ‹å®éªŒæ–¹æ³•
            if any(keyword in methodology.lower() for keyword in ['å®éªŒ', 'experiment', 'å¯¹ç…§', 'control', 'æµ‹è¯•', 'test']):
                analysis += """**å®éªŒæ–¹æ³•å­¦çªç ´**ï¼šé€šè¿‡è®¾è®¡ä¸¥æ ¼çš„å®éªŒæ–¹æ¡ˆå’Œå¯¹ç…§ç»„è®¾ç½®ï¼Œæœ¬ç ”ç©¶å»ºç«‹äº†ç§‘å­¦çš„éªŒè¯æœºåˆ¶ï¼Œä¸ºç†è®ºå‡è®¾æä¾›äº†å®è¯æ”¯æ’‘ï¼Œæå‡äº†ç ”ç©¶ç»“è®ºçš„å¯ä¿¡åº¦ã€‚

"""
            
            # æ£€æµ‹è·¨å­¦ç§‘æ–¹æ³•
            if any(keyword in methodology.lower() for keyword in ['è·¨å­¦ç§‘', 'interdisciplinary', 'èåˆ', 'ç»¼åˆ', 'å¤šå…ƒ']):
                analysis += """**è·¨å­¦ç§‘æ–¹æ³•è®ºåˆ›æ–°**ï¼šæœ¬ç ”ç©¶æ•´åˆå¤šå­¦ç§‘ç†è®ºå’Œæ–¹æ³•ï¼Œçªç ´äº†å•ä¸€å­¦ç§‘ç ”ç©¶çš„å±€é™æ€§ï¼Œä¸ºå¤æ‚é—®é¢˜çš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹æ³•è®ºè§†è§’ã€‚

"""
            
            # æ£€æµ‹è®¡ç®—æ–¹æ³•
            if any(keyword in methodology.lower() for keyword in ['ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model', 'ä»¿çœŸ', 'simulation', 'æœºå™¨å­¦ä¹ ', 'ai']):
                analysis += """**è®¡ç®—æ–¹æ³•åˆ›æ–°**ï¼šè¿ç”¨å…ˆè¿›çš„è®¡ç®—æŠ€æœ¯å’Œç®—æ³•æ¨¡å‹ï¼Œæœ¬ç ”ç©¶å®ç°äº†ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å¤„ç†çš„å¤æ‚é—®é¢˜åˆ†æï¼Œä¸ºç›¸å…³é¢†åŸŸæä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚

"""
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ–¹æ³•å­¦ä¿¡æ¯ï¼Œæä¾›é€šç”¨åˆ†ææ¡†æ¶
        if not analysis:
            analysis = """**æ–¹æ³•å­¦åˆ›æ–°è¯„ä¼°**ï¼šåŸºäºæ–‡çŒ®å¯¹æ¯”åˆ†æï¼Œå»ºè®®ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°æ–¹æ³•å­¦åˆ›æ–°ï¼š

1. **ç ”ç©¶è®¾è®¡åˆ›æ–°**ï¼šè¯„ä¼°ç ”ç©¶è®¾è®¡çš„åŸåˆ›æ€§å’Œç§‘å­¦æ€§ï¼Œå¯¹æ¯”ç°æœ‰ç ”ç©¶æ–¹æ³•çš„ä¼˜åŠ¿å’Œæ”¹è¿›
2. **æŠ€æœ¯è·¯å¾„çªç ´**ï¼šåˆ†æé‡‡ç”¨çš„æŠ€æœ¯æ‰‹æ®µå’Œåˆ†æå·¥å…·çš„å…ˆè¿›æ€§ï¼Œè¯†åˆ«æ–¹æ³•è®ºå±‚é¢çš„çªç ´ç‚¹
3. **å¯é‡å¤æ€§æå‡**ï¼šè¯„ä¼°æ–¹æ³•çš„æ ‡å‡†åŒ–ç¨‹åº¦å’Œå¯æ¨å¹¿æ€§ï¼Œä¸ºåç»­ç ”ç©¶æä¾›æ–¹æ³•è®ºå‚è€ƒ

**æ”¹è¿›å»ºè®®**ï¼šæ˜ç¡®é˜è¿°ç ”ç©¶æ–¹æ³•çš„åˆ›æ–°ä¹‹å¤„ï¼Œé€šè¿‡ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”çªå‡ºä¼˜åŠ¿å’Œæ”¹è¿›ç‚¹ã€‚"""
        
        return analysis.strip()
    
    def _analyze_theoretical_contribution(self, thesis_info: Dict[str, str], 
                                        context: str) -> str:
        """åˆ†æç†è®ºè´¡çŒ®ï¼ˆåŸºäºè§„åˆ™çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        theoretical_framework = thesis_info.get('theoretical_framework', '')
        main_innovations = thesis_info.get('main_innovations', '')
        
        analysis = ""
        
        # åŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œé€šç”¨ç†è®ºè´¡çŒ®åˆ†æ
        if theoretical_framework:
            # æ£€æµ‹æ–°ç†è®ºæ„å»º
            if any(keyword in theoretical_framework.lower() for keyword in ['ç†è®º', 'theory', 'æ¨¡å‹', 'model', 'æ¡†æ¶', 'framework']):
                analysis += """**ç†è®ºæ¡†æ¶æ„å»º**ï¼šæœ¬ç ”ç©¶æ„å»ºäº†ç³»ç»Ÿçš„ç†è®ºåˆ†ææ¡†æ¶ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç†è®ºå‘å±•åšå‡ºäº†è´¡çŒ®ã€‚é€šè¿‡æ•´åˆç°æœ‰ç†è®ºèµ„æºï¼Œå½¢æˆäº†å…·æœ‰è§£é‡ŠåŠ›å’Œé¢„æµ‹æ€§çš„ç†è®ºä½“ç³»ã€‚

"""
            
            # æ£€æµ‹ç†è®ºèåˆ
            if any(keyword in theoretical_framework.lower() for keyword in ['èåˆ', 'integration', 'ç»“åˆ', 'combination', 'ç»¼åˆ', 'synthesis']):
                analysis += """**ç†è®ºæ•´åˆåˆ›æ–°**ï¼šæœ¬ç ”ç©¶åˆ›æ–°æ€§åœ°æ•´åˆäº†å¤šä¸ªç†è®ºè§†è§’ï¼Œçªç ´äº†å•ä¸€ç†è®ºçš„å±€é™æ€§ï¼Œä¸ºè·¨ç†è®ºç ”ç©¶æä¾›äº†æ–°çš„æ•´åˆæ¨¡å¼å’Œåˆ†æè·¯å¾„ã€‚

"""
            
            # æ£€æµ‹ç†è®ºæ‰©å±•
            if any(keyword in theoretical_framework.lower() for keyword in ['æ‰©å±•', 'extension', 'å‘å±•', 'development', 'å®Œå–„', 'improvement']):
                analysis += """**ç†è®ºæ‰©å±•è´¡çŒ®**ï¼šåœ¨ç°æœ‰ç†è®ºåŸºç¡€ä¸Šï¼Œæœ¬ç ”ç©¶è¿›è¡Œäº†é‡è¦çš„ç†è®ºæ‰©å±•å’Œå®Œå–„ï¼Œä¸°å¯Œäº†ç†è®ºå†…æ¶µï¼Œæ‹“å±•äº†ç†è®ºçš„é€‚ç”¨èŒƒå›´å’Œè§£é‡Šèƒ½åŠ›ã€‚

"""
        
        # æ£€æµ‹åˆ›æ–°ç‚¹ä¸­çš„ç†è®ºè´¡çŒ®
        if main_innovations:
            if any(keyword in main_innovations.lower() for keyword in ['é¦–æ¬¡', 'first', 'é¦–åˆ›', 'pioneer', 'åŸåˆ›', 'original']):
                analysis += """**åŸåˆ›æ€§ç†è®ºè´¡çŒ®**ï¼šæœ¬ç ”ç©¶æå‡ºäº†å…·æœ‰åŸåˆ›æ€§çš„ç†è®ºè§‚ç‚¹ï¼Œåœ¨ç›¸å…³é¢†åŸŸå…·æœ‰å¼€åˆ›æ€§æ„ä¹‰ï¼Œä¸ºåç»­ç†è®ºç ”ç©¶å¥ å®šäº†é‡è¦åŸºç¡€ã€‚

"""
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„ç†è®ºä¿¡æ¯ï¼Œæä¾›é€šç”¨åˆ†ææ¡†æ¶
        if not analysis:
            analysis = """**ç†è®ºè´¡çŒ®è¯„ä¼°**ï¼šåŸºäºæ–‡çŒ®å¯¹æ¯”åˆ†æï¼Œå»ºè®®ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ç†è®ºè´¡çŒ®ï¼š

1. **ç†è®ºåˆ›æ–°åº¦**ï¼šè¯„ä¼°æå‡ºçš„ç†è®ºè§‚ç‚¹ã€æ¦‚å¿µæˆ–æ¨¡å‹çš„åŸåˆ›æ€§å’Œæ–°é¢–æ€§
2. **ç†è®ºè§£é‡ŠåŠ›**ï¼šåˆ†æç†è®ºæ¡†æ¶å¯¹ç ”ç©¶é—®é¢˜çš„è§£é‡Šèƒ½åŠ›å’Œé€‚ç”¨èŒƒå›´
3. **ç†è®ºå½±å“åŠ›**ï¼šè¯„ä¼°ç†è®ºè´¡çŒ®å¯¹å­¦ç§‘å‘å±•å’Œåç»­ç ”ç©¶çš„æ¨åŠ¨ä½œç”¨

**æ”¹è¿›å»ºè®®**ï¼šæ˜ç¡®é˜è¿°ç†è®ºè´¡çŒ®çš„ç‹¬ç‰¹æ€§ï¼Œé€šè¿‡ä¸ç°æœ‰ç†è®ºçš„å¯¹æ¯”çªå‡ºåˆ›æ–°ä»·å€¼ã€‚"""
        
        return analysis.strip()
    
    def _analyze_practical_value(self, thesis_info: Dict[str, str], 
                               context: str) -> str:
        """åˆ†æå®è·µä»·å€¼ï¼ˆåŸºäºè§„åˆ™çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        application_value = thesis_info.get('application_value', '')
        proposed_solutions = thesis_info.get('proposed_solutions', '')
        practical_problems = thesis_info.get('practical_problems', '')
        
        analysis = ""
        
        # åŸºäºè®ºæ–‡å†…å®¹è¿›è¡Œé€šç”¨å®è·µä»·å€¼åˆ†æ
        if practical_problems:
            # æ£€æµ‹é—®é¢˜è§£å†³èƒ½åŠ›
            if practical_problems.strip():
                analysis += """**å®é™…é—®é¢˜è§£å†³**ï¼šæœ¬ç ”ç©¶é’ˆå¯¹ç°å®ä¸­çš„å…·ä½“é—®é¢˜æå‡ºäº†ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ˜ç¡®çš„é—®é¢˜å¯¼å‘å’Œåº”ç”¨é’ˆå¯¹æ€§ï¼Œä¸ºå®è·µå·¥ä½œæä¾›äº†å¯æ“ä½œçš„æŒ‡å¯¼ã€‚

"""
        
        if proposed_solutions:
            # æ£€æµ‹è§£å†³æ–¹æ¡ˆåˆ›æ–°
            if any(keyword in proposed_solutions.lower() for keyword in ['æ–¹æ¡ˆ', 'solution', 'ç­–ç•¥', 'strategy', 'å»ºè®®', 'recommendation']):
                analysis += """**è§£å†³æ–¹æ¡ˆåˆ›æ–°**ï¼šç ”ç©¶æå‡ºçš„è§£å†³æ–¹æ¡ˆå…·æœ‰åˆ›æ–°æ€§å’Œå¯è¡Œæ€§ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„å®è·µå·¥ä½œæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰è¾ƒå¼ºçš„æ¨å¹¿åº”ç”¨ä»·å€¼ã€‚

"""
            
            # æ£€æµ‹æŠ€æœ¯åº”ç”¨
            if any(keyword in proposed_solutions.lower() for keyword in ['æŠ€æœ¯', 'technology', 'ç³»ç»Ÿ', 'system', 'å¹³å°', 'platform', 'å·¥å…·', 'tool']):
                analysis += """**æŠ€æœ¯åº”ç”¨åˆ›æ–°**ï¼šæœ¬ç ”ç©¶å°†å…ˆè¿›æŠ€æœ¯åº”ç”¨äºå®é™…é—®é¢˜è§£å†³ï¼Œå®ç°äº†æŠ€æœ¯ä¸åº”ç”¨çš„æœ‰æ•ˆç»“åˆï¼Œä¸ºæŠ€æœ¯è½¬åŒ–å’Œäº§ä¸šåº”ç”¨æä¾›äº†ç¤ºèŒƒã€‚

"""
        
        if application_value:
            # æ£€æµ‹åº”ç”¨å‰æ™¯
            if any(keyword in application_value.lower() for keyword in ['åº”ç”¨', 'application', 'æ¨å¹¿', 'promotion', 'ä»·å€¼', 'value']):
                analysis += """**åº”ç”¨ä»·å€¼æ˜¾è‘—**ï¼šç ”ç©¶æˆæœå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œæ¨å¹¿ä»·å€¼ï¼Œèƒ½å¤Ÿä¸ºç›¸å…³è¡Œä¸šå’Œé¢†åŸŸçš„å‘å±•æä¾›æœ‰æ•ˆæ”¯æ’‘ï¼Œå…·æœ‰é‡è¦çš„ç°å®æ„ä¹‰ã€‚

"""
            
            # æ£€æµ‹ç»æµæ•ˆç›Š
            if any(keyword in application_value.lower() for keyword in ['ç»æµ', 'economic', 'æ•ˆç›Š', 'benefit', 'æˆæœ¬', 'cost', 'æ•ˆç‡', 'efficiency']):
                analysis += """**ç»æµæ•ˆç›Šæ½œåŠ›**ï¼šç ”ç©¶æˆæœåœ¨æé«˜æ•ˆç‡ã€é™ä½æˆæœ¬ã€åˆ›é€ ç»æµä»·å€¼ç­‰æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œä¸ºç›¸å…³äº§ä¸šçš„å¯æŒç»­å‘å±•æä¾›äº†é‡è¦æ”¯æ’‘ã€‚

"""
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å®è·µä¿¡æ¯ï¼Œæä¾›é€šç”¨åˆ†ææ¡†æ¶
        if not analysis:
            analysis = """**å®è·µä»·å€¼è¯„ä¼°**ï¼šåŸºäºæ–‡çŒ®å¯¹æ¯”åˆ†æï¼Œå»ºè®®ä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°å®è·µä»·å€¼ï¼š

1. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šè¯„ä¼°ç ”ç©¶æˆæœå¯¹å®é™…é—®é¢˜çš„è§£å†³æ•ˆæœå’Œé€‚ç”¨èŒƒå›´
2. **åº”ç”¨å¯è¡Œæ€§**ï¼šåˆ†æç ”ç©¶ç»“æœçš„å¯æ“ä½œæ€§å’Œå®æ–½æ¡ä»¶
3. **æ¨å¹¿ä»·å€¼**ï¼šè¯„ä¼°æˆæœçš„å¯å¤åˆ¶æ€§å’Œåœ¨ä¸åŒåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§

**æ”¹è¿›å»ºè®®**ï¼šæ˜ç¡®é˜è¿°ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼ï¼Œé€šè¿‡æ¡ˆä¾‹åˆ†ææˆ–æ•ˆæœéªŒè¯å¢å¼ºè¯´æœåŠ›ã€‚"""
        
        return analysis.strip()
    
    def _generate_literature_review_analysis(self, thesis_extracted_info: Optional[Dict[str, str]] = None,
                                           papers_by_lang: Optional[Dict[str, List]] = None,
                                           analysis_data: Optional[Dict] = None) -> str:
        """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°è´¨é‡åˆ†æ"""
        
        # å¦‚æœæ²¡æœ‰ä¼ å…¥papers_by_langï¼Œå°è¯•ä»analysis_dataæ„å»º
        if not papers_by_lang and analysis_data:
            papers_by_lang = {}
            if analysis_data.get('top_chinese'):
                papers_by_lang['Chinese'] = analysis_data['top_chinese']
            if analysis_data.get('top_english'):
                papers_by_lang['English'] = analysis_data['top_english']
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ–‡çŒ®æ•°æ®æˆ–è®ºæ–‡ä¿¡æ¯ï¼Œè¿”å›æ•°æ®ä¸è¶³æç¤º
        if not papers_by_lang or not thesis_extracted_info:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ–‡çŒ®æ•°æ®
            total_cached_papers = 0
            if analysis_data:
                total_cached_papers = sum([
                    len(analysis_data[key]) if analysis_data[key] else 0
                    for key in ['top_chinese', 'top_english']
                ])
            
            if total_cached_papers > 0:
                return f"""## æ–‡çŒ®ç»¼è¿°åˆ†æ

**åŸºäºç¼“å­˜æ•°æ®çš„ç®€åŒ–åˆ†æ**ï¼š
- å‘ç°å·²æœ‰çš„ç¼“å­˜æ–‡çŒ®æ•°æ®ï¼š{total_cached_papers}ç¯‡ç›¸å…³ç ”ç©¶
- ç”±äºç¼ºå°‘è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†çš„æ–‡çŒ®ç»¼è¿°å¯¹æ¯”åˆ†æ
- å»ºè®®è¡¥å……è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®ä¿¡æ¯ä»¥è·å¾—æ›´å®Œæ•´çš„æ–‡çŒ®ç»¼è¿°åˆ†æ

**æ”¹è¿›å»ºè®®**ï¼š
1. ç¡®ä¿è®ºæ–‡ä¸­åŒ…å«å®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
2. é‡æ–°è¿è¡Œè¯„ä¼°ä»¥è·å–è®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯
3. è€ƒè™‘è¿›è¡Œæ–‡çŒ®æ£€ç´¢ä»¥è·å¾—æ›´å…¨é¢çš„å¯¹æ¯”æ•°æ®"""
            else:
                return "## æ–‡çŒ®ç»¼è¿°åˆ†æ\n\n**æ•°æ®ä¸è¶³**ï¼šç¼ºå°‘ç›¸å…³æ–‡çŒ®æ•°æ®æˆ–è®ºæ–‡æŠ½å–ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ–‡çŒ®ç»¼è¿°åˆ†æã€‚å»ºè®®ä½¿ç”¨æ–‡çŒ®æ£€ç´¢åŠŸèƒ½æˆ–ç¡®ä¿æœ‰å¯ç”¨çš„ç¼“å­˜æ–‡çŒ®æ•°æ®ã€‚"
        
        # è·å–è®ºæ–‡å‚è€ƒæ–‡çŒ®
        reference_list = thesis_extracted_info.get('references', '') or thesis_extracted_info.get('ReferenceList', '')
        if not reference_list:
            # å¦‚æœæ²¡æœ‰å‚è€ƒæ–‡çŒ®ï¼Œä½†æœ‰æ–‡çŒ®æ•°æ®ï¼Œè¿›è¡Œç®€åŒ–åˆ†æ
            total_papers = sum(len(papers) for papers in papers_by_lang.values())
            return f"""## æ–‡çŒ®ç»¼è¿°åˆ†æ

**è­¦å‘Š**ï¼šæœªèƒ½æå–åˆ°è®ºæ–‡å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œæä¾›åŸºäºç›¸å…³æ–‡çŒ®æ•°æ®çš„ç®€åŒ–åˆ†æã€‚

### ğŸ“Š ç›¸å…³æ–‡çŒ®æ¦‚è§ˆ

- **æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®æ€»æ•°**ï¼š{total_papers}ç¯‡
- **æ–‡çŒ®è¯­è¨€åˆ†å¸ƒ**ï¼š
""" + "\n".join([f"  - {lang}æ–‡çŒ®ï¼š{len(papers)}ç¯‡" for lang, papers in papers_by_lang.items()]) + """

**å»ºè®®**ï¼š
- è¡¥å……è®ºæ–‡çš„å®Œæ•´å‚è€ƒæ–‡çŒ®åˆ—è¡¨ä»¥è¿›è¡Œæ·±åº¦å¯¹æ¯”åˆ†æ
- æ£€æŸ¥è®ºæ–‡æ ¼å¼ï¼Œç¡®ä¿å‚è€ƒæ–‡çŒ®éƒ¨åˆ†èƒ½å¤Ÿè¢«æ­£ç¡®æå–
- è€ƒè™‘æ‰‹åŠ¨è¡¥å……å…³é”®å‚è€ƒæ–‡çŒ®ä¿¡æ¯"""
        
        # è¿›è¡Œå®Œæ•´çš„æ–‡çŒ®ç»¼è¿°åˆ†æ
        # åˆ†æè¦†ç›–åº¦
        coverage_analysis = self._analyze_literature_coverage(reference_list, papers_by_lang)
        
        # åˆ†ææ·±åº¦ï¼ˆä½¿ç”¨CoTï¼‰
        depth_analysis = self._analyze_literature_depth_cot(reference_list, papers_by_lang, thesis_extracted_info)
        
        # åˆ†æç›¸å…³æ€§
        relevance_analysis = self._analyze_literature_relevance(reference_list, papers_by_lang, thesis_extracted_info)
        
        # åˆ†ææ—¶æ•ˆæ€§
        timeliness_analysis = self._analyze_literature_timeliness(reference_list, papers_by_lang)
        
        # å‘ç°ç¼ºå¤±çš„é‡è¦æ–‡çŒ®
        missing_refs = self._find_missing_references(reference_list, papers_by_lang)
        
        return f"""## æ–‡çŒ®ç»¼è¿°åˆ†æ

{coverage_analysis}

{depth_analysis}

{relevance_analysis}

{timeliness_analysis}

{missing_refs}"""

    def _analyze_literature_coverage(self, reference_list, papers_by_lang: Dict[str, List]) -> str:
        """åˆ†ææ–‡çŒ®è¦†ç›–åº¦"""
        # ç»Ÿè®¡CNKIæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®æ•°é‡
        total_relevant = sum(len(papers) for papers in papers_by_lang.values())
        
        # ç»Ÿä¸€çš„å¤„ç†æ¨¡å¼
        if isinstance(reference_list, list):
            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼šç›´æ¥ä½¿ç”¨é•¿åº¦æˆ–åˆå¹¶ä¸ºå­—ç¬¦ä¸²
            ref_count = len(reference_list)
            reference_text = ' '.join(str(ref) for ref in reference_list)
        elif isinstance(reference_list, str):
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            ref_lines = reference_list.split('\n')
            ref_count = len([line for line in ref_lines if line.strip()])
            reference_text = reference_list
        else:
            # å¤„ç†å…¶ä»–ç±»å‹ï¼šå®‰å…¨é»˜è®¤å€¼
            ref_count = 0
            reference_text = ""
        
        coverage_ratio = min(ref_count / max(total_relevant, 1), 1.0)
        
        if coverage_ratio >= 0.7:
            coverage_level = "ä¼˜ç§€"
            coverage_desc = "æ–‡çŒ®è¦†ç›–åº¦å¾ˆé«˜ï¼Œæ˜¾ç¤ºäº†å¯¹ç ”ç©¶é¢†åŸŸçš„å…¨é¢äº†è§£"
        elif coverage_ratio >= 0.5:
            coverage_level = "è‰¯å¥½"
            coverage_desc = "æ–‡çŒ®è¦†ç›–åº¦è¾ƒå¥½ï¼Œä½†ä»æœ‰æå‡ç©ºé—´"
        elif coverage_ratio >= 0.3:
            coverage_level = "ä¸€èˆ¬"
            coverage_desc = "æ–‡çŒ®è¦†ç›–åº¦æœ‰é™ï¼Œå»ºè®®å¢åŠ ç›¸å…³æ–‡çŒ®çš„å¼•ç”¨"
        else:
            coverage_level = "ä¸è¶³"
            coverage_desc = "æ–‡çŒ®è¦†ç›–åº¦æ˜æ˜¾ä¸è¶³ï¼Œéœ€è¦å¤§å¹…å¢åŠ ç›¸å…³æ–‡çŒ®"
        
        return f"""### ğŸ“Š æ–‡çŒ®è¦†ç›–åº¦åˆ†æ

**è¦†ç›–åº¦è¯„çº§**ï¼š{coverage_level} ({coverage_ratio:.1%})
- æ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®ï¼š{total_relevant} ç¯‡
- è®ºæ–‡å¼•ç”¨æ–‡çŒ®ï¼šçº¦ {ref_count} ç¯‡
- **è¯„ä¼°**ï¼š{coverage_desc}

**æ”¹è¿›å»ºè®®**ï¼š
- å»ºè®®è¡¥å……æœ€æ–°çš„é«˜å½±å“å› å­æ–‡çŒ®
- æ³¨æ„åŒ…å«ä¸åŒç ”ç©¶è§’åº¦çš„ä»£è¡¨æ€§æ–‡çŒ®
- å¹³è¡¡å›½å†…å¤–æ–‡çŒ®çš„æ¯”ä¾‹"""

    def _analyze_literature_depth_cot(self, reference_list, papers_by_lang: Dict[str, List], 
                                    thesis_extracted_info: Dict[str, str]) -> str:
        """ä½¿ç”¨Chain of Thoughtåˆ†ææ–‡çŒ®ç»¼è¿°æ·±åº¦ï¼ˆåŸºäºç›¸å…³æ–‡çŒ®å…ƒæ•°æ®ï¼‰"""
        # CoTæ¨ç†æ­¥éª¤
        thinking_process = """
**æ€è€ƒè¿‡ç¨‹**ï¼š
1. åˆ†æç›¸å…³æ–‡çŒ®çš„ç ”ç©¶ä¸»é¢˜åˆ†å¸ƒå’Œå±‚æ¬¡ç»“æ„
2. è¯„ä¼°æ–‡çŒ®åœ¨ç†è®ºåŸºç¡€ã€æ–¹æ³•è®ºå’Œåº”ç”¨ä¸‰ä¸ªå±‚é¢çš„è¦†ç›–åº¦
3. æ£€æŸ¥æ–‡çŒ®çš„æ—¶æ•ˆæ€§å’Œæƒå¨æ€§åˆ†å¸ƒ
4. è¯„ä¼°æ–‡çŒ®ç»¼è¿°çš„ç³»ç»Ÿæ€§å’Œå®Œæ•´æ€§
        """
        
        # åŸºäºç›¸å…³æ–‡çŒ®å…ƒæ•°æ®è¿›è¡Œæ·±åº¦åˆ†æ
        depth_analysis = self._evaluate_literature_depth_by_metadata(papers_by_lang, thesis_extracted_info)
        
        depth_level = depth_analysis['level']
        depth_score = depth_analysis['score']
        depth_desc = depth_analysis['description']
        detailed_analysis = depth_analysis['detailed_analysis']
        
        return f"""### ğŸ¤” æ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†æï¼ˆCoTæ–¹æ³•ï¼‰

{thinking_process}

**æ·±åº¦è¯„ä¼°ç»“æœ**ï¼š
- **æ·±åº¦ç­‰çº§**ï¼š{depth_level} ({depth_score}/100)
- **åˆ†æ**ï¼š{depth_desc}

**è¯¦ç»†åˆ†æ**ï¼š
{detailed_analysis}

**CoTæ¨ç†ç»“è®º**ï¼š
åŸºäºç›¸å…³æ–‡çŒ®çš„ä¸»é¢˜åˆ†å¸ƒã€ç ”ç©¶å±‚æ¬¡å’Œæ—¶æ•ˆæ€§åˆ†æï¼Œ
è¯¥è®ºæ–‡çš„æ–‡çŒ®ç»¼è¿°åœ¨ç†è®ºæ·±åº¦ã€æ–¹æ³•è®ºè®¨è®ºå’Œæ‰¹åˆ¤æ€§åˆ†ææ–¹é¢{'è¡¨ç°ä¼˜ç§€' if depth_score >= 80 else 'æœ‰å¾…åŠ å¼º' if depth_score >= 60 else 'éœ€è¦æ˜¾è‘—æ”¹è¿›'}ã€‚

**æå‡å»ºè®®**ï¼š
- å¢å¼ºæ–‡çŒ®é—´çš„å¯¹æ¯”åˆ†æå’Œæ‰¹åˆ¤æ€§è¯„è¿°
- æ˜ç¡®æŒ‡å‡ºç°æœ‰ç ”ç©¶çš„å±€é™æ€§å’Œç ”ç©¶ç©ºç™½
- æ„å»ºæ›´æ¸…æ™°çš„ç†è®ºæ¡†æ¶å’Œç ”ç©¶è„‰ç»œ"""
    
    def _evaluate_literature_depth_by_metadata(self, papers_by_lang: Dict[str, List], 
                                             thesis_extracted_info: Dict[str, str]) -> Dict[str, Any]:
        """åŸºäºç›¸å…³æ–‡çŒ®å…ƒæ•°æ®è¯„ä¼°æ–‡çŒ®ç»¼è¿°æ·±åº¦"""
        
        all_papers = []
        if papers_by_lang:
            for papers in papers_by_lang.values():
                if papers:
                    all_papers.extend(papers)
        
        if not all_papers:
            return {
                'level': 'æ•°æ®ä¸è¶³',
                'score': 0,
                'description': 'ç¼ºå°‘ç›¸å…³æ–‡çŒ®æ•°æ®ï¼Œæ— æ³•è¯„ä¼°æ–‡çŒ®ç»¼è¿°æ·±åº¦',
                'detailed_analysis': 'éœ€è¦æä¾›ç›¸å…³æ–‡çŒ®æ•°æ®ä»¥è¿›è¡Œæ·±åº¦åˆ†æã€‚'
            }
        
        # 1. åˆ†æç ”ç©¶å±‚æ¬¡åˆ†å¸ƒ
        theory_papers = 0
        method_papers = 0
        application_papers = 0
        review_papers = 0
        
        for paper in all_papers:
            # å®‰å…¨è·å–å­—ç¬¦ä¸²å­—æ®µï¼Œç¡®ä¿ç±»å‹æ­£ç¡®
            title = str(paper.get('Title', '')).lower()
            keywords = str(paper.get('KeyWords', '')).lower()
            abstract = str(paper.get('Abstract', '')).lower()
            
            paper_text = f"{title} {keywords} {abstract}"
            
            # ç†è®ºç ”ç©¶è¯†åˆ«
            if any(keyword in paper_text for keyword in ['ç†è®º', 'theory', 'æ¡†æ¶', 'framework', 'æ¨¡å‹', 'model', 'æœºåˆ¶', 'mechanism']):
                theory_papers += 1
            
            # æ–¹æ³•ç ”ç©¶è¯†åˆ«
            if any(keyword in paper_text for keyword in ['æ–¹æ³•', 'method', 'ç®—æ³•', 'algorithm', 'æŠ€æœ¯', 'technique', 'ç­–ç•¥', 'strategy']):
                method_papers += 1
            
            # åº”ç”¨ç ”ç©¶è¯†åˆ«
            if any(keyword in paper_text for keyword in ['åº”ç”¨', 'application', 'å®éªŒ', 'experiment', 'æ¡ˆä¾‹', 'case', 'ä¸´åºŠ', 'clinical']):
                application_papers += 1
            
            # ç»¼è¿°æ–‡çŒ®è¯†åˆ«
            if any(keyword in paper_text for keyword in ['ç»¼è¿°', 'review', 'è¿›å±•', 'progress', 'ç°çŠ¶', 'state']):
                review_papers += 1
        
        total_papers = len(all_papers)
        
        # 2. åˆ†ææ–‡çŒ®è´¨é‡å’Œæƒå¨æ€§
        high_quality_papers = 0
        recent_papers = 0
        core_journal_papers = 0
        
        current_year = 2025
        for paper in all_papers:
            # æ—¶æ•ˆæ€§åˆ†æ
            year = paper.get('PublicationYear', '')
            if year:
                try:
                    year_int = int(year)
                    if current_year - year_int <= 5:
                        recent_papers += 1
                except:
                    pass
            
            # æ ¸å¿ƒæœŸåˆŠè¯†åˆ«ï¼ˆç®€åŒ–ç‰ˆï¼‰
            journal = str(paper.get('Source', '')).lower()
            if any(indicator in journal for indicator in ['ieee', 'acm', 'å­¦æŠ¥', 'journal', 'transactions']):
                core_journal_papers += 1
            
            # é«˜è´¨é‡æ–‡çŒ®è¯†åˆ«ï¼ˆåŸºäºå…³é”®è¯ä¸°å¯Œåº¦ï¼‰
            keywords = str(paper.get('KeyWords', ''))
            if keywords and len(keywords.split(';;')) >= 3:
                high_quality_papers += 1
        
        # 3. è®¡ç®—æ·±åº¦è¯„åˆ†
        depth_score = 0
        
        # ç ”ç©¶å±‚æ¬¡åˆ†å¸ƒè¯„åˆ† (40åˆ†)
        layer_coverage = 0
        if theory_papers > 0:
            layer_coverage += 1
        if method_papers > 0:
            layer_coverage += 1
        if application_papers > 0:
            layer_coverage += 1
        if review_papers > 0:
            layer_coverage += 1
        
        depth_score += (layer_coverage / 4) * 40
        
        # æ–‡çŒ®è´¨é‡è¯„åˆ† (30åˆ†)
        quality_ratio = high_quality_papers / max(total_papers, 1)
        depth_score += quality_ratio * 30
        
        # æ—¶æ•ˆæ€§è¯„åˆ† (20åˆ†)
        recent_ratio = recent_papers / max(total_papers, 1)
        depth_score += recent_ratio * 20
        
        # æƒå¨æ€§è¯„åˆ† (10åˆ†)
        authority_ratio = core_journal_papers / max(total_papers, 1)
        depth_score += authority_ratio * 10
        
        depth_score = min(100, max(0, depth_score))
        
        # 4. ç¡®å®šæ·±åº¦ç­‰çº§å’Œæè¿°
        if depth_score >= 85:
            level = "ä¼˜ç§€"
            description = "æ–‡çŒ®ç»¼è¿°æ·±åº¦ä¼˜ç§€ï¼Œä½“ç°äº†ç³»ç»Ÿæ€§çš„ç†è®ºåˆ†æå’Œå…¨é¢çš„ç ”ç©¶è¦†ç›–"
        elif depth_score >= 70:
            level = "è‰¯å¥½"
            description = "æ–‡çŒ®ç»¼è¿°æ·±åº¦è‰¯å¥½ï¼Œå…·å¤‡è¾ƒå®Œæ•´çš„ç ”ç©¶å±‚æ¬¡è¦†ç›–"
        elif depth_score >= 55:
            level = "ä¸­ç­‰"
            description = "æ–‡çŒ®ç»¼è¿°æ·±åº¦ä¸­ç­‰ï¼ŒåŸºæœ¬è¦†ç›–ä¸»è¦ç ”ç©¶æ–¹å‘"
        elif depth_score >= 40:
            level = "ä¸€èˆ¬"
            description = "æ–‡çŒ®ç»¼è¿°æ·±åº¦ä¸€èˆ¬ï¼Œå­˜åœ¨æ˜æ˜¾çš„è¦†ç›–ä¸è¶³"
        else:
            level = "ä¸è¶³"
            description = "æ–‡çŒ®ç»¼è¿°æ·±åº¦ä¸è¶³ï¼Œç¼ºä¹ç³»ç»Ÿæ€§åˆ†æ"
        
        # 5. ç”Ÿæˆè¯¦ç»†åˆ†æ
        detailed_analysis = f"""
**ç ”ç©¶å±‚æ¬¡åˆ†å¸ƒåˆ†æ**ï¼š
- ç†è®ºç ”ç©¶æ–‡çŒ®ï¼š{theory_papers}ç¯‡ ({theory_papers/total_papers*100:.1f}%)
- æ–¹æ³•æŠ€æœ¯æ–‡çŒ®ï¼š{method_papers}ç¯‡ ({method_papers/total_papers*100:.1f}%)
- åº”ç”¨å®è¯æ–‡çŒ®ï¼š{application_papers}ç¯‡ ({application_papers/total_papers*100:.1f}%)
- ç»¼è¿°æ€§æ–‡çŒ®ï¼š{review_papers}ç¯‡ ({review_papers/total_papers*100:.1f}%)

**æ–‡çŒ®è´¨é‡åˆ†æ**ï¼š
- é«˜è´¨é‡æ–‡çŒ®æ¯”ä¾‹ï¼š{quality_ratio:.1%}
- è¿‘5å¹´æ–‡çŒ®æ¯”ä¾‹ï¼š{recent_ratio:.1%}
- æƒå¨æœŸåˆŠæ–‡çŒ®æ¯”ä¾‹ï¼š{authority_ratio:.1%}

**ç»¼åˆè¯„ä¼°**ï¼š
æ–‡çŒ®ç»¼è¿°åœ¨{'ç†è®º-æ–¹æ³•-åº”ç”¨' if layer_coverage >= 3 else 'éƒ¨åˆ†ç ”ç©¶å±‚æ¬¡'}æ–¹é¢æœ‰æ‰€è¦†ç›–ï¼Œ
{'æ—¶æ•ˆæ€§è‰¯å¥½' if recent_ratio >= 0.6 else 'æ—¶æ•ˆæ€§ä¸€èˆ¬' if recent_ratio >= 0.3 else 'æ—¶æ•ˆæ€§ä¸è¶³'}ï¼Œ
{'æƒå¨æ€§è¾ƒå¼º' if authority_ratio >= 0.3 else 'æƒå¨æ€§ä¸€èˆ¬'}ã€‚
        """
        
        return {
            'level': level,
            'score': int(depth_score),
            'description': description,
            'detailed_analysis': detailed_analysis.strip()
        }

    def _analyze_literature_relevance(self, reference_list, papers_by_lang: Dict[str, List],
                                    thesis_extracted_info: Dict[str, str]) -> str:
        """åˆ†ææ–‡çŒ®ç›¸å…³æ€§"""
        # è·å–è®ºæ–‡å…³é”®è¯å’Œä¸»é¢˜
        keywords = thesis_extracted_info.get('Keywords', '')
        abstract = thesis_extracted_info.get('Abstract', '')
        
        # ç»Ÿè®¡é«˜ç›¸å…³æ€§æ–‡çŒ®ï¼ˆåŸºäºæ£€ç´¢ç»“æœçš„topæ–‡çŒ®ï¼‰
        high_relevance_count = 0
        total_searched = 0
        
        for lang, papers in papers_by_lang.items():
            total_searched += len(papers)
            # å‡è®¾å‰30%çš„æ£€ç´¢ç»“æœä¸ºé«˜ç›¸å…³æ–‡çŒ®
            high_relevance_count += len(papers) * 0.3
        
        relevance_ratio = high_relevance_count / max(total_searched, 1)
        
        if relevance_ratio >= 0.8:
            relevance_level = "é«˜åº¦ç›¸å…³"
            relevance_desc = "å¼•ç”¨æ–‡çŒ®ä¸ç ”ç©¶ä¸»é¢˜é«˜åº¦ç›¸å…³ï¼Œä½“ç°äº†ç²¾å‡†çš„æ–‡çŒ®é€‰æ‹©"
        elif relevance_ratio >= 0.6:
            relevance_level = "è¾ƒä¸ºç›¸å…³"
            relevance_desc = "å¤§éƒ¨åˆ†å¼•ç”¨æ–‡çŒ®ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³ï¼Œæœ‰å°‘é‡è¾¹ç¼˜æ–‡çŒ®"
        elif relevance_ratio >= 0.4:
            relevance_level = "éƒ¨åˆ†ç›¸å…³"
            relevance_desc = "éƒ¨åˆ†å¼•ç”¨æ–‡çŒ®ç›¸å…³æ€§ä¸å¤Ÿå¼ºï¼Œå»ºè®®æé«˜æ–‡çŒ®é€‰æ‹©çš„ç²¾å‡†åº¦"
        else:
            relevance_level = "ç›¸å…³æ€§ä¸è¶³"
            relevance_desc = "å¤šæ•°å¼•ç”¨æ–‡çŒ®ä¸æ ¸å¿ƒç ”ç©¶ä¸»é¢˜å…³è”åº¦è¾ƒä½"
        
        return f"""### ğŸ¯ æ–‡çŒ®ç›¸å…³æ€§åˆ†æ

**ç›¸å…³æ€§è¯„çº§**ï¼š{relevance_level} ({relevance_ratio:.1%})

**åˆ†æç»´åº¦**ï¼š
- **ä¸»é¢˜åŒ¹é…åº¦**ï¼šåŸºäºå…³é”®è¯å’Œæ‘˜è¦åˆ†ææ–‡çŒ®ä¸»é¢˜å¥‘åˆåº¦
- **æ–¹æ³•è®ºç›¸å…³æ€§**ï¼šè¯„ä¼°æ–‡çŒ®åœ¨ç ”ç©¶æ–¹æ³•ä¸Šçš„ç›¸å…³æ€§
- **ç†è®ºæ¡†æ¶å»åˆåº¦**ï¼šåˆ†ææ–‡çŒ®ç†è®ºåŸºç¡€çš„ç›¸å…³ç¨‹åº¦

**è¯„ä¼°ç»“æœ**ï¼š{relevance_desc}

**ä¼˜åŒ–ç­–ç•¥**ï¼š
- ä¼˜å…ˆå¼•ç”¨ä¸æ ¸å¿ƒç ”ç©¶é—®é¢˜ç›´æ¥ç›¸å…³çš„æ–‡çŒ®
- å¹³è¡¡ç†è®ºæ–‡çŒ®å’Œå®è¯ç ”ç©¶æ–‡çŒ®çš„æ¯”ä¾‹
- ç¡®ä¿æ–‡çŒ®èƒ½å¤Ÿæ”¯æ’‘ç ”ç©¶å‡è®¾å’Œæ–¹æ³•é€‰æ‹©"""

    def _analyze_literature_timeliness(self, reference_list, papers_by_lang: Dict[str, List]) -> str:
        """åˆ†ææ–‡çŒ®æ—¶æ•ˆæ€§ - åŸºäºè®ºæ–‡å‚è€ƒæ–‡çŒ®"""
        import re
        from datetime import datetime
        
        current_year = datetime.now().year
        
        # ç»Ÿä¸€çš„å¤„ç†æ¨¡å¼
        if isinstance(reference_list, list):
            # å¤„ç†åˆ—è¡¨ç±»å‹ï¼šç›´æ¥ä½¿ç”¨é•¿åº¦æˆ–åˆå¹¶ä¸ºå­—ç¬¦ä¸²
            ref_count = len(reference_list)
            reference_text = ' '.join(str(ref) for ref in reference_list)
        elif isinstance(reference_list, str):
            # å¤„ç†å­—ç¬¦ä¸²ç±»å‹ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            if not reference_list or reference_list == 'æ— å‚è€ƒæ–‡çŒ®':
                # å¦‚æœæ²¡æœ‰å‚è€ƒæ–‡çŒ®ï¼Œæä¾›åŸºäºæ£€ç´¢æ–‡çŒ®çš„å‚è€ƒæ€§åˆ†æ
                all_papers = []
                for papers in papers_by_lang.values():
                    all_papers.extend(papers)
                
                if all_papers:
                    years = []
                    for paper in all_papers:
                        year = paper.get('PublicationYear', '')
                        if year:
                            try:
                                year_int = int(year)
                                if 1900 <= year_int <= current_year:
                                    years.append(year_int)
                            except ValueError:
                                continue
                    
                    if years:
                        recent_count = len([y for y in years if current_year - y <= 5])
                        recent_ratio = recent_count / len(years)
                        
                        return f"""### â° æ–‡çŒ®æ—¶æ•ˆæ€§åˆ†æ

**è­¦å‘Š**ï¼šæœªèƒ½ä»è®ºæ–‡ä¸­æå–å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼Œä»¥ä¸‹åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®è¿›è¡Œå‚è€ƒæ€§åˆ†æã€‚

**ç›¸å…³æ–‡çŒ®æ—¶æ•ˆæ€§å‚è€ƒ**ï¼š
- æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®æ€»æ•°ï¼š{len(years)} ç¯‡
- è¿‘5å¹´ç›¸å…³æ–‡çŒ®ï¼š{recent_count} ç¯‡ ({recent_ratio:.1%})

**é‡è¦è¯´æ˜**ï¼š
- æ­¤åˆ†æåŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³é¢†åŸŸæ–‡çŒ®ï¼Œä¸æ˜¯è®ºæ–‡å®é™…å¼•ç”¨çš„å‚è€ƒæ–‡çŒ®
- çœŸæ­£çš„æ—¶æ•ˆæ€§è¯„ä¼°åº”åŸºäºè®ºæ–‡å‚è€ƒæ–‡çŒ®åˆ—è¡¨
- å»ºè®®æ£€æŸ¥è®ºæ–‡PDFæ˜¯å¦åŒ…å«å®Œæ•´çš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†

**æ”¹è¿›å»ºè®®**ï¼š
- ç¡®ä¿è®ºæ–‡åŒ…å«å®Œæ•´çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
- æ£€æŸ¥PDFæå–è´¨é‡ï¼Œå¿…è¦æ—¶æ‰‹åŠ¨è¡¥å……å‚è€ƒæ–‡çŒ®ä¿¡æ¯
- é‡æ–°è¿è¡Œåˆ†æä»¥è·å¾—å‡†ç¡®çš„æ—¶æ•ˆæ€§è¯„ä¼°"""
                
                return """### â° æ–‡çŒ®æ—¶æ•ˆæ€§åˆ†æ
            
**è­¦å‘Š**ï¼šæ— æ³•è·å–è®ºæ–‡å‚è€ƒæ–‡çŒ®ä¿¡æ¯ï¼Œä¹Ÿæ— ç›¸å…³æ–‡çŒ®æ•°æ®å¯ä¾›å‚è€ƒåˆ†æã€‚"""
            
            ref_lines = reference_list.split('\n')
            ref_count = len([line for line in ref_lines if line.strip()])
            reference_text = reference_list
        else:
            # å¤„ç†å…¶ä»–ç±»å‹ï¼šå®‰å…¨é»˜è®¤å€¼
            ref_count = 0
            reference_text = ""
        
        # ä»å‚è€ƒæ–‡çŒ®ä¸­æå–å¹´ä»½
        years = re.findall(r'\b(19|20)\d{2}\b', reference_text)
        years = [int(year) for year in years if int(year) <= current_year]
        
        if not years:
            return """### â° æ–‡çŒ®æ—¶æ•ˆæ€§åˆ†æ
            
**è­¦å‘Š**ï¼šæ— æ³•ä»å‚è€ƒæ–‡çŒ®ä¸­æå–æœ‰æ•ˆçš„å‘è¡¨å¹´ä»½ä¿¡æ¯ã€‚

**å¯èƒ½åŸå› **ï¼š
- å‚è€ƒæ–‡çŒ®æ ¼å¼ä¸è§„èŒƒ
- æ–‡çŒ®æå–è¿‡ç¨‹ä¸­ä¿¡æ¯ä¸¢å¤±
- å‚è€ƒæ–‡çŒ®ä¸»è¦ä¸ºç½‘é¡µæˆ–å…¶ä»–éæœŸåˆŠèµ„æº

**å»ºè®®**ï¼š
- æ£€æŸ¥åŸå§‹è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®æ ¼å¼
- ç¡®ä¿å‚è€ƒæ–‡çŒ®åŒ…å«æ˜ç¡®çš„å‘è¡¨å¹´ä»½"""
        
        # åˆ†ææ—¶æ•ˆæ€§
        recent_years = [year for year in years if current_year - year <= 5]  # è¿‘5å¹´
        very_recent = [year for year in years if current_year - year <= 2]   # è¿‘2å¹´
        
        recent_ratio = len(recent_years) / len(years)
        very_recent_ratio = len(very_recent) / len(years)
        avg_age = sum(current_year - year for year in years) / len(years)
        
        if recent_ratio >= 0.6 and very_recent_ratio >= 0.3:
            timeliness_level = "ä¼˜ç§€"
            timeliness_desc = "å‚è€ƒæ–‡çŒ®æ—¶æ•ˆæ€§å¾ˆå¥½ï¼Œå……åˆ†åæ˜ äº†ç ”ç©¶é¢†åŸŸçš„æœ€æ–°è¿›å±•"
        elif recent_ratio >= 0.4 and very_recent_ratio >= 0.2:
            timeliness_level = "è‰¯å¥½"
            timeliness_desc = "å‚è€ƒæ–‡çŒ®æ—¶æ•ˆæ€§è¾ƒå¥½ï¼Œä½†å¯ä»¥å¢åŠ æ›´å¤šæœ€æ–°æ–‡çŒ®"
        elif recent_ratio >= 0.3:
            timeliness_level = "ä¸€èˆ¬"
            timeliness_desc = "å‚è€ƒæ–‡çŒ®æ—¶æ•ˆæ€§ä¸€èˆ¬ï¼Œå»ºè®®å¢åŠ è¿‘å¹´æ¥çš„é‡è¦æ–‡çŒ®"
        else:
            timeliness_level = "ä¸è¶³"
            timeliness_desc = "å‚è€ƒæ–‡çŒ®æ—¶æ•ˆæ€§ä¸è¶³ï¼Œè¿‡å¤šä¾èµ–è¾ƒè€çš„æ–‡çŒ®"
        
        return f"""### â° æ–‡çŒ®æ—¶æ•ˆæ€§åˆ†æ

**æ—¶æ•ˆæ€§è¯„çº§**ï¼š{timeliness_level}

**ç»Ÿè®¡æ•°æ®**ï¼š
- å‚è€ƒæ–‡çŒ®æ€»æ•°ï¼š{len(years)} ç¯‡ï¼ˆå«å¹´ä»½ä¿¡æ¯ï¼‰
- è¿‘5å¹´æ–‡çŒ®ï¼š{len(recent_years)} ç¯‡ ({recent_ratio:.1%})
- è¿‘2å¹´æ–‡çŒ®ï¼š{len(very_recent)} ç¯‡ ({very_recent_ratio:.1%})
- å¹³å‡æ–‡çŒ®å¹´é¾„ï¼š{avg_age:.1f} å¹´

**è¯„ä¼°**ï¼š{timeliness_desc}

**æ”¹è¿›å»ºè®®**ï¼š
- {'å½“å‰å‚è€ƒæ–‡çŒ®æ—¶æ•ˆæ€§è‰¯å¥½ï¼Œå»ºè®®ä¿æŒå¯¹æœ€æ–°ç ”ç©¶çš„å…³æ³¨' if timeliness_level == 'ä¼˜ç§€' else 'è¡¥å……æœ€æ–°çš„ç ”ç©¶æˆæœå’Œç†è®ºå‘å±•'}
- {'å¯é€‚å½“è¡¥å……ç»å…¸å¥ åŸºæ€§æ–‡çŒ®å¹³è¡¡ç†è®ºåŸºç¡€' if recent_ratio > 0.8 else 'å…³æ³¨è¿‘æœŸçš„é«˜å½±å“å› å­æ–‡çŒ®'}
- {'æŒç»­å…³æ³¨é¡¶çº§æœŸåˆŠçš„æœ€æ–°å‘è¡¨' if timeliness_level in ['ä¼˜ç§€', 'è‰¯å¥½'] else 'å¹³è¡¡ç»å…¸æ–‡çŒ®ä¸å‰æ²¿æ–‡çŒ®çš„æ¯”ä¾‹'}"""

    def _find_missing_references(self, reference_list, papers_by_lang: Dict[str, List]) -> str:
        """å‘ç°å¯èƒ½ç¼ºå¤±çš„é‡è¦æ–‡çŒ®"""
        missing_suggestions = []
        
        # åŸºäºCNKIæ£€ç´¢ç»“æœæ¨èé«˜ç›¸å…³æ–‡çŒ®
        for lang, papers in papers_by_lang.items():
            if papers:
                # æ¨èå‰3ç¯‡é«˜ç›¸å…³æ–‡çŒ®ï¼ˆå‡è®¾æŒ‰ç›¸å…³æ€§æ’åºï¼‰
                top_papers = papers[:3]
                for paper in top_papers:
                    if isinstance(paper, dict):
                        title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                        author = paper.get('author', 'æœªçŸ¥ä½œè€…')
                        missing_suggestions.append(f"- **{title}** (ä½œè€…: {author})")
        
        if not missing_suggestions:
            return """### ğŸ” ç¼ºå¤±æ–‡çŒ®å»ºè®®
            
**çŠ¶æ€**ï¼šåŸºäºç°æœ‰æ£€ç´¢ç»“æœï¼Œæš‚æœªå‘ç°æ˜æ˜¾ç¼ºå¤±çš„é‡è¦æ–‡çŒ®ã€‚"""
        
        return f"""### ğŸ” ç¼ºå¤±æ–‡çŒ®å»ºè®®

**æ¨èè¡¥å……çš„é‡è¦æ–‡çŒ®**ï¼š
åŸºäºCNKIæ£€ç´¢ç»“æœï¼Œä»¥ä¸‹æ–‡çŒ®å¯èƒ½å¯¹ç ”ç©¶æœ‰é‡è¦ä»·å€¼ä½†æœªè¢«å¼•ç”¨ï¼š

{chr(10).join(missing_suggestions[:6])}

**å»ºè®®**ï¼š
- è¯„ä¼°è¿™äº›æ–‡çŒ®ä¸ç ”ç©¶ä¸»é¢˜çš„ç›¸å…³æ€§
- è€ƒè™‘åœ¨æ–‡çŒ®ç»¼è¿°ä¸­è¡¥å……ç›¸å…³è®¨è®º
- å…³æ³¨è¿™äº›æ–‡çŒ®æå‡ºçš„æ–°è§‚ç‚¹æˆ–æ–¹æ³•"""

    def _generate_metadata_analysis_content(self, literature_metadata_analysis: Dict) -> str:
        """ç”Ÿæˆæ–‡çŒ®å…ƒæ•°æ®åˆ†æå†…å®¹"""
        if not literature_metadata_analysis:
            return ""
        
        # è·å–å„é¡¹åˆ†æç»“æœ
        journal_analysis = literature_metadata_analysis.get('journal_analysis', {})
        author_analysis = literature_metadata_analysis.get('author_analysis', {})
        affiliation_analysis = literature_metadata_analysis.get('affiliation_analysis', {})
        core_journal_analysis = literature_metadata_analysis.get('core_journal_analysis', {})
        subject_analysis = literature_metadata_analysis.get('subject_analysis', {})
        year_analysis = literature_metadata_analysis.get('year_analysis', {})
        citation_analysis = literature_metadata_analysis.get('citation_analysis', {})
        total_stats = literature_metadata_analysis.get('total_statistics', {})
        
        content = "## ğŸ“Š ç›¸å…³æ–‡çŒ®å…ƒæ•°æ®åˆ†æ\n\n"
        
        # æ€»ä½“ç»Ÿè®¡
        if total_stats:
            content += "### ğŸ“ˆ æ€»ä½“ç»Ÿè®¡\n\n"
            content += f"- **æ–‡çŒ®æ€»æ•°**: {total_stats.get('total_papers', 0)} ç¯‡\n"
            
            papers_by_lang = total_stats.get('papers_by_language', {})
            for lang, count in papers_by_lang.items():
                content += f"- **{lang}æ–‡çŒ®**: {count} ç¯‡\n"
            
            content += f"- **æ ¸å¿ƒæœŸåˆŠæ–‡çŒ®**: {total_stats.get('papers_with_core_index', 0)} ç¯‡\n"
            content += f"- **æœ‰å¼•ç”¨è®°å½•æ–‡çŒ®**: {total_stats.get('papers_with_citations', 0)} ç¯‡\n"
            content += f"- **æœ‰åŸºé‡‘æ”¯æŒæ–‡çŒ®**: {total_stats.get('papers_with_funds', 0)} ç¯‡\n\n"
        
        # æœŸåˆŠåˆ†æ
        if journal_analysis:
            content += "### ğŸ“š æœŸåˆŠåˆ†å¸ƒåˆ†æ\n\n"
            top_journals = journal_analysis.get('top_journals', [])
            if top_journals:
                content += "**ä¸»è¦å‘è¡¨æœŸåˆŠï¼ˆå‰10åï¼‰**ï¼š\n\n"
                for i, (journal, count) in enumerate(top_journals[:10], 1):
                    content += f"{i}. **{journal}** ({count} ç¯‡)\n"
                content += "\n"
            
            journal_types = journal_analysis.get('journal_types', {})
            if journal_types:
                content += "**æœŸåˆŠç±»å‹åˆ†å¸ƒ**ï¼š\n"
                for jtype, count in journal_types.items():
                    content += f"- {jtype}: {count} ç¯‡\n"
                content += "\n"
        
        # æ ¸å¿ƒæœŸåˆŠåˆ†æ
        if core_journal_analysis:
            content += "### ğŸ† æ ¸å¿ƒæœŸåˆŠåˆ†æ\n\n"
            index_dist = core_journal_analysis.get('index_distribution', {})
            if index_dist:
                content += "**æ ¸å¿ƒæœŸåˆŠç´¢å¼•åˆ†å¸ƒ**ï¼š\n"
                for index_name, count in index_dist.items():
                    content += f"- **{index_name}**: {count} ç¯‡\n"
                content += "\n"
        
        # å­¦ç§‘åˆ†æ
        if subject_analysis:
            content += "### ğŸ“– å­¦ç§‘åˆ†å¸ƒåˆ†æ\n\n"
            l1_subjects = subject_analysis.get('level1_subjects', [])
            if l1_subjects:
                content += "**ä¸€çº§å­¦ç§‘åˆ†å¸ƒ**ï¼š\n"
                for subject, count in l1_subjects[:8]:
                    content += f"- **{subject}**: {count} ç¯‡\n"
                content += "\n"
            
            l2_subjects = subject_analysis.get('level2_subjects', [])
            if l2_subjects:
                content += "**äºŒçº§å­¦ç§‘åˆ†å¸ƒï¼ˆå‰10åï¼‰**ï¼š\n"
                for subject, count in l2_subjects[:10]:
                    content += f"- **{subject}**: {count} ç¯‡\n"
                content += "\n"
        
        # å¹´ä»½åˆ†æ
        if year_analysis:
            content += "### ğŸ“… æ—¶é—´åˆ†å¸ƒåˆ†æ\n\n"
            year_dist = year_analysis.get('year_distribution', {})
            earliest = year_analysis.get('earliest_year', '')
            latest = year_analysis.get('latest_year', '')
            
            if earliest and latest:
                content += f"**æ—¶é—´è·¨åº¦**: {earliest} - {latest} å¹´\n"
                content += f"**å¹´ä»½è¦†ç›–**: {year_analysis.get('year_span', 0)} å¹´\n\n"
            
            if year_dist:
                content += "**å¹´ä»½åˆ†å¸ƒ**ï¼š\n"
                # åªæ˜¾ç¤ºæœ€è¿‘10å¹´çš„åˆ†å¸ƒ
                recent_years = sorted(year_dist.items(), reverse=True)[:10]
                for year, count in recent_years:
                    if year.strip():  # è¿‡æ»¤ç©ºå¹´ä»½
                        content += f"- **{year}å¹´**: {count} ç¯‡\n"
                content += "\n"
        
        # ä½œè€…åˆ†æ
        if author_analysis:
            content += "### ğŸ‘¥ ä½œè€…åˆ†æ\n\n"
            content += f"**ä½œè€…æ€»æ•°**: {author_analysis.get('total_authors', 0)} äºº\n"
            content += f"**é€šè®¯ä½œè€…æ€»æ•°**: {author_analysis.get('total_corresponding_authors', 0)} äºº\n\n"
            
            top_authors = author_analysis.get('top_authors', [])
            if top_authors:
                content += "**é«˜äº§ä½œè€…ï¼ˆå‰10åï¼‰**ï¼š\n"
                for i, (author, count) in enumerate(top_authors[:10], 1):
                    if count > 1:  # åªæ˜¾ç¤ºå‘è¡¨å¤šç¯‡æ–‡ç« çš„ä½œè€…
                        content += f"{i}. **{author}** ({count} ç¯‡)\n"
                content += "\n"
        
        # å•ä½åˆ†æ
        if affiliation_analysis:
            content += "### ğŸ›ï¸ æœºæ„åˆ†æ\n\n"
            content += f"**æœºæ„æ€»æ•°**: {affiliation_analysis.get('total_affiliations', 0)} ä¸ª\n\n"
            
            top_affs = affiliation_analysis.get('top_affiliations', [])
            if top_affs:
                content += "**ä¸»è¦ç ”ç©¶æœºæ„ï¼ˆå‰10åï¼‰**ï¼š\n"
                for i, (aff, count) in enumerate(top_affs[:10], 1):
                    if count > 1:  # åªæ˜¾ç¤ºæœ‰å¤šç¯‡æ–‡ç« çš„æœºæ„
                        content += f"{i}. **{aff}** ({count} ç¯‡)\n"
                content += "\n"
        
        # å¼•ç”¨åˆ†æ
        if citation_analysis:
            content += "### ğŸ“ˆ å¼•ç”¨åˆ†æ\n\n"
            total_citations = citation_analysis.get('total_citations', 0)
            avg_citations = citation_analysis.get('avg_citations', 0)
            max_citations = citation_analysis.get('max_citations', 0)
            
            content += f"**æ€»å¼•ç”¨æ¬¡æ•°**: {total_citations} æ¬¡\n"
            content += f"**å¹³å‡å¼•ç”¨æ¬¡æ•°**: {avg_citations:.2f} æ¬¡/ç¯‡\n"
            content += f"**æœ€é«˜å¼•ç”¨æ¬¡æ•°**: {max_citations} æ¬¡\n\n"
            
            total_downloads = citation_analysis.get('total_downloads', 0)
            avg_downloads = citation_analysis.get('avg_downloads', 0)
            
            content += f"**æ€»ä¸‹è½½æ¬¡æ•°**: {total_downloads} æ¬¡\n"
            content += f"**å¹³å‡ä¸‹è½½æ¬¡æ•°**: {avg_downloads:.2f} æ¬¡/ç¯‡\n\n"
        
        content += "**åˆ†æè¯´æ˜**ï¼šä»¥ä¸Šåˆ†æåŸºäºCNKIæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡çŒ®ï¼Œåæ˜ äº†è¯¥ç ”ç©¶é¢†åŸŸçš„åŸºæœ¬ç‰¹å¾å’Œå‘å±•è¶‹åŠ¿ã€‚\n\n"
        
        return content
    
    def _generate_report_content(self, input_file: str, thesis_extracted_info: Optional[Dict[str, str]] = None,
                               papers_by_lang: Optional[Dict[str, List]] = None,
                               literature_metadata_analysis: Optional[Dict] = None) -> str:
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        base_name = Path(input_file).stem
        
        # å°è¯•åŠ è½½ç›¸å…³çš„åˆ†ææ•°æ®
        analysis_data = self._load_analysis_data(base_name)
        
        # è¿›è¡Œä¸»é¢˜åˆ†æ
        theme_analysis = self._analyze_literature_themes(analysis_data)
        
        # ç”Ÿæˆåˆ›æ–°æ€§åˆ†æï¼Œä¼ é€’è®ºæ–‡æŠ½å–ä¿¡æ¯
        innovation_analysis = self._generate_innovation_analysis(analysis_data, theme_analysis, thesis_extracted_info)
        
        # ç”Ÿæˆç‹¬ç«‹æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå¹¶è·å–æ€»ç»“ä¿¡æ¯
        literature_review_summary = None
        if thesis_extracted_info and papers_by_lang:
            # ç”Ÿæˆç‹¬ç«‹çš„è¯¦ç»†æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Š
            detailed_report_path = self.literature_analyzer.analyze_literature_review(
                input_file, thesis_extracted_info, papers_by_lang, self.config_mgr.get_output_dir()
            )
            logger.info(f"ç‹¬ç«‹æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {detailed_report_path}")
            
            # æš‚æ—¶ç¦ç”¨æ€»ç»“åŠŸèƒ½ï¼Œå› ä¸ºæ–¹æ³•ä¸å­˜åœ¨
            # literature_review_summary = self.literature_analyzer.generate_summary_for_main_report(
            #     thesis_extracted_info, papers_by_lang
            # )
        
        # ç”Ÿæˆæ–‡çŒ®ç»¼è¿°åˆ†æï¼ˆå‘åå…¼å®¹ï¼‰
        literature_review_analysis = self._generate_literature_review_analysis(
            thesis_extracted_info, papers_by_lang, analysis_data
        )
        
        # ç”Ÿæˆæ–‡çŒ®å…ƒæ•°æ®åˆ†æï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
        metadata_analysis_content = self._generate_metadata_analysis_content(literature_metadata_analysis) if literature_metadata_analysis else ""
        
        # ç”ŸæˆMarkdownå†…å®¹
        content = self._create_markdown_content(input_file, analysis_data, theme_analysis, 
                                              innovation_analysis, literature_review_analysis,
                                              metadata_analysis_content, literature_review_summary)
        
        return content
    
    def _load_analysis_data(self, base_name: str) -> Dict[str, Any]:
        """åŠ è½½åˆ†ææ•°æ®"""
        output_dir = self.config_mgr.get_output_dir()
        
        data = {
            'chinese_papers': None,
            'english_papers': None,
            'dedup_chinese': None,
            'dedup_english': None,
            'top_chinese': None,
            'top_english': None
        }
        
        # å®šä¹‰æ–‡ä»¶æ¨¡å¼
        patterns = {
            'chinese_papers': f"{base_name}_relevant_papers_Chinese.json",
            'english_papers': f"{base_name}_relevant_papers_English.json",
            'dedup_chinese': f"{base_name}_relevant_papers_dedup_Chinese.json",
            'dedup_english': f"{base_name}_relevant_papers_dedup_English.json",
        }
        
        # æŸ¥æ‰¾TOPè®ºæ–‡æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰ä¸åŒçš„æ•°é‡ï¼‰
        top_count = self.config_mgr.get_top_papers_count()
        patterns.update({
            'top_chinese': f"{base_name}_TOP{top_count}PAPERS_Chinese.json",
            'top_english': f"{base_name}_TOP{top_count}PAPERS_English.json",
        })
        
        # åŠ è½½å­˜åœ¨çš„æ–‡ä»¶
        for key, filename in patterns.items():
            file_path = os.path.join(output_dir, filename)
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data[key] = json.load(f)
                except Exception as e:
                    print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½ {filename}: {e}")
        
        return data
    
    def _build_literature_context(self, analysis_data: Dict[str, Any], theme_analysis: Dict[str, Any]) -> str:
        """æ„å»ºæ–‡çŒ®ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        
        context = ""
        
        # ç»Ÿè®¡æ–‡çŒ®æ•°é‡
        total_chinese = len(analysis_data['top_chinese']) if analysis_data['top_chinese'] else 0
        total_english = len(analysis_data['top_english']) if analysis_data['top_english'] else 0
        
        context += f"**æ–‡çŒ®æ£€ç´¢ç»Ÿè®¡**ï¼š\n"
        context += f"- ä¸­æ–‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®ï¼š{total_chinese}ç¯‡\n"
        context += f"- è‹±æ–‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®ï¼š{total_english}ç¯‡\n"
        context += f"- æ–‡çŒ®æ€»æ•°ï¼š{total_chinese + total_english}ç¯‡\n\n"
        
        # ä¸»è¦ç ”ç©¶ä¸»é¢˜
        if theme_analysis.get('chinese_themes') or theme_analysis.get('english_themes'):
            context += "**ä¸»è¦ç ”ç©¶ä¸»é¢˜åˆ†å¸ƒ**ï¼š\n"
            
            # åˆå¹¶ä¸­è‹±æ–‡ä¸»é¢˜
            all_themes = {}
            if theme_analysis.get('chinese_themes'):
                all_themes.update(theme_analysis['chinese_themes'])
            if theme_analysis.get('english_themes'):
                for theme, count in theme_analysis['english_themes'].items():
                    all_themes[theme] = all_themes.get(theme, 0) + count
            
            # æ’åºå¹¶å–å‰10ä¸ªä¸»é¢˜
            top_themes = sorted(all_themes.items(), key=lambda x: x[1], reverse=True)[:10]
            for theme, count in top_themes:
                context += f"- {theme}: {count}ç¯‡\n"
            context += "\n"
        
        # å¹´ä»½åˆ†å¸ƒ
        if theme_analysis.get('year_distribution'):
            context += "**å¹´ä»½åˆ†å¸ƒ**ï¼š\n"
            sorted_years = sorted(theme_analysis['year_distribution'].items())
            total_papers = sum(theme_analysis['year_distribution'].values())
            
            # è¿‘5å¹´çš„æ–‡çŒ®æ¯”ä¾‹
            recent_years = [year for year in theme_analysis['year_distribution'].keys() if year >= 2020]
            if recent_years:
                recent_count = sum(theme_analysis['year_distribution'][year] for year in recent_years)
                recent_percentage = (recent_count / total_papers) * 100
                context += f"- è¿‘5å¹´æ–‡çŒ®å æ¯”ï¼š{recent_percentage:.1f}% ({recent_count}ç¯‡)\n"
            
            context += f"- ç ”ç©¶æ—¶é—´è·¨åº¦ï¼š{min(theme_analysis['year_distribution'].keys())}-{max(theme_analysis['year_distribution'].keys())}å¹´\n\n"
        
        # ä»£è¡¨æ€§è®ºæ–‡
        if analysis_data.get('top_chinese') or analysis_data.get('top_english'):
            context += "**ä»£è¡¨æ€§ç›¸å…³ç ”ç©¶**ï¼š\n"
            
            # ä¸­æ–‡ä»£è¡¨æ€§è®ºæ–‡
            if analysis_data.get('top_chinese'):
                context += "ä¸­æ–‡æ–‡çŒ®ä»£è¡¨ï¼š\n"
                for i, paper in enumerate(analysis_data['top_chinese'][:3], 1):
                    context += f"{i}. {paper.get('Title', 'æœªçŸ¥æ ‡é¢˜')}\n"
                    context += f"   å…³é”®è¯: {paper.get('KeyWords', '')}\n"
                context += "\n"
            
            # è‹±æ–‡ä»£è¡¨æ€§è®ºæ–‡
            if analysis_data.get('top_english'):
                context += "è‹±æ–‡æ–‡çŒ®ä»£è¡¨ï¼š\n"
                for i, paper in enumerate(analysis_data['top_english'][:3], 1):
                    context += f"{i}. {paper.get('Title', 'Unknown Title')}\n"
                    context += f"   Keywords: {paper.get('KeyWords', '').replace(';', ', ')}\n"
                context += "\n"
        
        return context
    
    def _build_context_prompt(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡è®¾ç½®æç¤ºè¯ï¼ˆå¤šè½®å¯¹è¯ç¬¬ä¸€è½®ï¼‰"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è¯„å®¡ä¸“å®¶ï¼Œæˆ‘å°†è¯·ä½ åˆ†æä¸€ç¯‡å­¦æœ¯è®ºæ–‡çš„åˆ›æ–°æ€§ã€‚é¦–å…ˆï¼Œè¯·æ¥æ”¶å¹¶ç†è§£ä»¥ä¸‹è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯å’Œç›¸å…³æ–‡çŒ®èƒŒæ™¯ï¼š

**ç›®æ ‡è®ºæ–‡ä¿¡æ¯**ï¼š
- è®ºæ–‡æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ä¸­æ–‡å…³é”®è¯ï¼š{thesis_info.get('keywords_cn', 'æœªæä¾›')}
- è‹±æ–‡å…³é”®è¯ï¼š{thesis_info.get('keywords_en', 'æœªæä¾›')}
- ç ”ç©¶æ–¹æ³•ï¼š{thesis_info.get('methodology', 'æœªæä¾›')}
- ç†è®ºæ¡†æ¶ï¼š{thesis_info.get('theoretical_framework', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- ç ”ç©¶ç»“è®ºï¼š{thesis_info.get('research_conclusions', 'æœªæä¾›')}
- å®é™…é—®é¢˜ï¼š{thesis_info.get('practical_problems', 'æœªæä¾›')}
- è§£å†³æ–¹æ¡ˆï¼š{thesis_info.get('proposed_solutions', 'æœªæä¾›')}
- åº”ç”¨ä»·å€¼ï¼š{thesis_info.get('application_value', 'æœªæä¾›')}

**ç›¸å…³æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

è¯·ç¡®è®¤ä½ å·²ç»ç†è§£äº†ä»¥ä¸Šè®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ã€‚æ¥ä¸‹æ¥æˆ‘å°†åˆ†åˆ«ä»æ–¹æ³•å­¦åˆ›æ–°ã€ç†è®ºè´¡çŒ®å’Œå®è·µä»·å€¼ä¸‰ä¸ªç»´åº¦å‘ä½ è¯¢é—®åˆ›æ–°æ€§åˆ†æã€‚

è¯·ç®€å•å›å¤"å·²ç†è§£è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œå‡†å¤‡è¿›è¡Œåˆ›æ–°æ€§åˆ†æ"å³å¯ã€‚"""

        return prompt
    
    def _build_methodology_analysis_prompt(self) -> str:
        """æ„å»ºæ–¹æ³•å­¦åˆ›æ–°åˆ†ææç¤ºè¯ï¼ˆå¤šè½®å¯¹è¯åç»­è½®æ¬¡ï¼‰"""
        
        prompt = """åŸºäºå‰é¢æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡åœ¨**æ–¹æ³•å­¦æ–¹é¢çš„åˆ›æ–°ç‚¹**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦é‡‡ç”¨äº†æ–°çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ï¼Ÿ
   - å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å“ªäº›æ”¹è¿›å’Œä¼˜åŒ–ï¼Ÿ
   - ä¸ç›¸å…³æ–‡çŒ®ä¸­çš„æ–¹æ³•ç›¸æ¯”æœ‰ä½•çªç ´ï¼Ÿ

2. **æŠ€æœ¯è·¯å¾„åˆ›æ–°**ï¼š
   - æŠ€æœ¯å®ç°ä¸Šæœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
   - æ˜¯å¦è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Ÿ
   - åˆ›æ–°æ–¹æ³•çš„ç§‘å­¦æ€§å’Œå¯é‡å¤æ€§å¦‚ä½•ï¼Ÿ

3. **æ–¹æ³•è®ºè´¡çŒ®**ï¼š
   - ä¸ºç›¸å…³ç ”ç©¶é¢†åŸŸæä¾›äº†ä»€ä¹ˆæ–°çš„æ–¹æ³•è®ºå·¥å…·ï¼Ÿ
   - æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œæ¨å¹¿ä»·å€¼å¦‚ä½•ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- å­¦æœ¯ã€å®¢è§‚çš„è¯­è°ƒï¼Œé‡ç‚¹çªå‡ºåˆ›æ–°ç‚¹å’Œå·®å¼‚åŒ–ä¼˜åŠ¿
- æä¾›å…·ä½“çš„åˆ†æè€Œéç©ºæ³›çš„è¯„ä»·
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºå¹¶æä¾›æ”¹è¿›å»ºè®®"""

        return prompt
    
    def _build_theory_analysis_prompt(self) -> str:
        """æ„å»ºç†è®ºè´¡çŒ®åˆ†ææç¤ºè¯ï¼ˆå¤šè½®å¯¹è¯åç»­è½®æ¬¡ï¼‰"""
        
        prompt = """åŸºäºå‰é¢æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡çš„**ç†è®ºè´¡çŒ®**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **ç†è®ºåˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦æå‡ºäº†æ–°çš„ç†è®ºæ¡†æ¶ã€æ¦‚å¿µæˆ–æ¨¡å‹ï¼Ÿ
   - å¯¹ç°æœ‰ç†è®ºè¿›è¡Œäº†å“ªäº›æ‰©å±•å’Œå®Œå–„ï¼Ÿ
   - ä¸ç›¸å…³æ–‡çŒ®çš„ç†è®ºåŸºç¡€ç›¸æ¯”æœ‰ä½•åŸåˆ›æ€§ï¼Ÿ

2. **ç†è®ºæ•´åˆæ€§**ï¼š
   - æ˜¯å¦åˆ›æ–°æ€§åœ°æ•´åˆäº†å¤šä¸ªç†è®ºè§†è§’ï¼Ÿ
   - ç†è®ºèåˆçš„é€»è¾‘æ€§å’Œç§‘å­¦æ€§å¦‚ä½•ï¼Ÿ
   - ä¸ºè·¨ç†è®ºç ”ç©¶æä¾›äº†ä»€ä¹ˆæ–°æ€è·¯ï¼Ÿ

3. **ç†è®ºå½±å“åŠ›**ï¼š
   - ç†è®ºè´¡çŒ®å¯¹å­¦ç§‘å‘å±•çš„æ„ä¹‰ï¼Ÿ
   - ä¸ºåç»­ç ”ç©¶å¥ å®šäº†ä»€ä¹ˆç†è®ºåŸºç¡€ï¼Ÿ
   - ç†è®ºçš„è§£é‡ŠåŠ›å’Œé¢„æµ‹èƒ½åŠ›å¦‚ä½•ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- å­¦æœ¯ã€ä¸¥è°¨çš„è¯­è°ƒï¼Œé‡ç‚¹çªå‡ºç†è®ºåˆ›æ–°çš„ç‹¬ç‰¹æ€§å’Œé¦–åˆ›æ€§
- åŸºäºæ–‡çŒ®å¯¹æ¯”è¿›è¡Œå®¢è§‚è¯„ä¼°
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœç†è®ºä¿¡æ¯ä¸å……åˆ†ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºå¹¶æä¾›å»ºè®®"""

        return prompt
    
    def _build_practice_analysis_prompt(self) -> str:
        """æ„å»ºå®è·µä»·å€¼åˆ†ææç¤ºè¯ï¼ˆå¤šè½®å¯¹è¯åç»­è½®æ¬¡ï¼‰"""
        
        prompt = """åŸºäºå‰é¢æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡çš„**å®è·µä»·å€¼å’Œåº”ç”¨å‰æ™¯**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼š
   - é’ˆå¯¹ä»€ä¹ˆå®é™…é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Ÿ
   - è§£å†³æ–¹æ¡ˆçš„åˆ›æ–°æ€§å’Œå¯è¡Œæ€§å¦‚ä½•ï¼Ÿ
   - ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”æœ‰ä½•ä¼˜åŠ¿ï¼Ÿ

2. **åº”ç”¨å‰æ™¯è¯„ä¼°**ï¼š
   - ç ”ç©¶æˆæœçš„å¸‚åœºåº”ç”¨æ½œåŠ›å¦‚ä½•ï¼Ÿ
   - åœ¨ç›¸å…³è¡Œä¸šå’Œé¢†åŸŸçš„æ¨å¹¿ä»·å€¼ï¼Ÿ
   - æŠ€æœ¯è½¬åŒ–å’Œäº§ä¸šåŒ–çš„å¯èƒ½æ€§ï¼Ÿ

3. **ç¤¾ä¼šç»æµä»·å€¼**ï¼š
   - èƒ½å¦æé«˜æ•ˆç‡ã€é™ä½æˆæœ¬æˆ–åˆ›é€ ä»·å€¼ï¼Ÿ
   - å¯¹ç¤¾ä¼šå‘å±•å’Œæ°‘ç”Ÿæ”¹å–„çš„è´¡çŒ®ï¼Ÿ
   - é¢„æœŸçš„ç»æµæ•ˆç›Šå’Œç¤¾ä¼šæ•ˆç›Šï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- å®ç”¨ã€å®¢è§‚çš„è§’åº¦åˆ†æï¼Œé‡ç‚¹çªå‡ºå®è·µåˆ›æ–°å’Œåº”ç”¨ä»·å€¼çš„å…·ä½“è¡¨ç°
- ç»“åˆç›¸å…³æ–‡çŒ®è¿›è¡Œå¯¹æ¯”åˆ†æ
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºæ”¹è¿›å»ºè®®"""

        return prompt
    
    def _build_methodology_prompt(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """æ„å»ºæ–¹æ³•å­¦åˆ›æ–°åˆ†ææç¤ºè¯"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è¯„å®¡ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†æç›®æ ‡è®ºæ–‡åœ¨æ–¹æ³•å­¦æ–¹é¢çš„åˆ›æ–°ç‚¹ï¼š

**ç›®æ ‡è®ºæ–‡ä¿¡æ¯**ï¼š
- è®ºæ–‡æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ç ”ç©¶æ–¹æ³•ï¼š{thesis_info.get('methodology', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- å…³é”®è¯ï¼š{thesis_info.get('keywords', 'æœªæä¾›')}

**ç›¸å…³æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

**åˆ†æè¦æ±‚**ï¼š
è¯·ä»ä»¥ä¸‹è§’åº¦æ·±å…¥åˆ†æç›®æ ‡è®ºæ–‡çš„æ–¹æ³•å­¦åˆ›æ–°ï¼Œå¹¶ä¸ç°æœ‰æ–‡çŒ®è¿›è¡Œå¯¹æ¯”ï¼š

1. **ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦é‡‡ç”¨äº†æ–°çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ï¼Ÿ
   - å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å“ªäº›æ”¹è¿›å’Œä¼˜åŒ–ï¼Ÿ
   - ä¸ç›¸å…³æ–‡çŒ®ä¸­çš„æ–¹æ³•ç›¸æ¯”æœ‰ä½•çªç ´ï¼Ÿ

2. **æŠ€æœ¯è·¯å¾„åˆ›æ–°**ï¼š
   - æŠ€æœ¯å®ç°ä¸Šæœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
   - æ˜¯å¦è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Ÿ
   - åˆ›æ–°æ–¹æ³•çš„ç§‘å­¦æ€§å’Œå¯é‡å¤æ€§å¦‚ä½•ï¼Ÿ

3. **æ–¹æ³•è®ºè´¡çŒ®**ï¼š
   - ä¸ºç›¸å…³ç ”ç©¶é¢†åŸŸæä¾›äº†ä»€ä¹ˆæ–°çš„æ–¹æ³•è®ºå·¥å…·ï¼Ÿ
   - æ–¹æ³•çš„é€‚ç”¨èŒƒå›´å’Œæ¨å¹¿ä»·å€¼å¦‚ä½•ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- è¯·ä»¥å­¦æœ¯ã€å®¢è§‚çš„è¯­è°ƒåˆ†æ
- é‡ç‚¹çªå‡ºåˆ›æ–°ç‚¹å’Œå·®å¼‚åŒ–ä¼˜åŠ¿
- æä¾›å…·ä½“çš„åˆ†æè€Œéç©ºæ³›çš„è¯„ä»·
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºå¹¶æä¾›æ”¹è¿›å»ºè®®

è¯·å¼€å§‹ä½ çš„åˆ†æï¼š"""

        return prompt
    
    def _build_theory_prompt(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """æ„å»ºç†è®ºè´¡çŒ®åˆ†ææç¤ºè¯"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯ç†è®ºç ”ç©¶ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¯„ä¼°ç›®æ ‡è®ºæ–‡çš„ç†è®ºè´¡çŒ®ï¼š

**ç›®æ ‡è®ºæ–‡ä¿¡æ¯**ï¼š
- è®ºæ–‡æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ç†è®ºæ¡†æ¶ï¼š{thesis_info.get('theoretical_framework', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- ç ”ç©¶ç»“è®ºï¼š{thesis_info.get('research_conclusions', 'æœªæä¾›')}

**ç›¸å…³æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

**åˆ†æè¦æ±‚**ï¼š
è¯·ä»ä»¥ä¸‹è§’åº¦æ·±å…¥åˆ†æç›®æ ‡è®ºæ–‡çš„ç†è®ºè´¡çŒ®ï¼š

1. **ç†è®ºåˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦æå‡ºäº†æ–°çš„ç†è®ºæ¡†æ¶ã€æ¦‚å¿µæˆ–æ¨¡å‹ï¼Ÿ
   - å¯¹ç°æœ‰ç†è®ºè¿›è¡Œäº†å“ªäº›æ‰©å±•å’Œå®Œå–„ï¼Ÿ
   - ä¸ç›¸å…³æ–‡çŒ®çš„ç†è®ºåŸºç¡€ç›¸æ¯”æœ‰ä½•åŸåˆ›æ€§ï¼Ÿ

2. **ç†è®ºæ•´åˆæ€§**ï¼š
   - æ˜¯å¦åˆ›æ–°æ€§åœ°æ•´åˆäº†å¤šä¸ªç†è®ºè§†è§’ï¼Ÿ
   - ç†è®ºèåˆçš„é€»è¾‘æ€§å’Œç§‘å­¦æ€§å¦‚ä½•ï¼Ÿ
   - ä¸ºè·¨ç†è®ºç ”ç©¶æä¾›äº†ä»€ä¹ˆæ–°æ€è·¯ï¼Ÿ

3. **ç†è®ºå½±å“åŠ›**ï¼š
   - ç†è®ºè´¡çŒ®å¯¹å­¦ç§‘å‘å±•çš„æ„ä¹‰ï¼Ÿ
   - ä¸ºåç»­ç ”ç©¶å¥ å®šäº†ä»€ä¹ˆç†è®ºåŸºç¡€ï¼Ÿ
   - ç†è®ºçš„è§£é‡ŠåŠ›å’Œé¢„æµ‹èƒ½åŠ›å¦‚ä½•ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- è¯·ä»¥å­¦æœ¯ã€ä¸¥è°¨çš„è¯­è°ƒåˆ†æ
- é‡ç‚¹çªå‡ºç†è®ºåˆ›æ–°çš„ç‹¬ç‰¹æ€§å’Œé¦–åˆ›æ€§
- åŸºäºæ–‡çŒ®å¯¹æ¯”è¿›è¡Œå®¢è§‚è¯„ä¼°
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœç†è®ºä¿¡æ¯ä¸å……åˆ†ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºå¹¶æä¾›å»ºè®®

è¯·å¼€å§‹ä½ çš„åˆ†æï¼š"""

        return prompt
    
    def _build_practice_prompt(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """æ„å»ºå®è·µä»·å€¼åˆ†ææç¤ºè¯"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å­¦ç ”ç»“åˆä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¯„ä¼°ç›®æ ‡è®ºæ–‡çš„å®è·µä»·å€¼å’Œåº”ç”¨å‰æ™¯ï¼š

**ç›®æ ‡è®ºæ–‡ä¿¡æ¯**ï¼š
- è®ºæ–‡æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- å®é™…é—®é¢˜ï¼š{thesis_info.get('practical_problems', 'æœªæä¾›')}
- è§£å†³æ–¹æ¡ˆï¼š{thesis_info.get('proposed_solutions', 'æœªæä¾›')}
- åº”ç”¨ä»·å€¼ï¼š{thesis_info.get('application_value', 'æœªæä¾›')}

**ç›¸å…³æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

**åˆ†æè¦æ±‚**ï¼š
è¯·ä»ä»¥ä¸‹è§’åº¦æ·±å…¥åˆ†æç›®æ ‡è®ºæ–‡çš„å®è·µä»·å€¼ï¼š

1. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼š
   - é’ˆå¯¹ä»€ä¹ˆå®é™…é—®é¢˜æå‡ºäº†è§£å†³æ–¹æ¡ˆï¼Ÿ
   - è§£å†³æ–¹æ¡ˆçš„åˆ›æ–°æ€§å’Œå¯è¡Œæ€§å¦‚ä½•ï¼Ÿ
   - ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”æœ‰ä½•ä¼˜åŠ¿ï¼Ÿ

2. **åº”ç”¨å‰æ™¯è¯„ä¼°**ï¼š
   - ç ”ç©¶æˆæœçš„å¸‚åœºåº”ç”¨æ½œåŠ›å¦‚ä½•ï¼Ÿ
   - åœ¨ç›¸å…³è¡Œä¸šå’Œé¢†åŸŸçš„æ¨å¹¿ä»·å€¼ï¼Ÿ
   - æŠ€æœ¯è½¬åŒ–å’Œäº§ä¸šåŒ–çš„å¯èƒ½æ€§ï¼Ÿ

3. **ç¤¾ä¼šç»æµä»·å€¼**ï¼š
   - èƒ½å¦æé«˜æ•ˆç‡ã€é™ä½æˆæœ¬æˆ–åˆ›é€ ä»·å€¼ï¼Ÿ
   - å¯¹ç¤¾ä¼šå‘å±•å’Œæ°‘ç”Ÿæ”¹å–„çš„è´¡çŒ®ï¼Ÿ
   - é¢„æœŸçš„ç»æµæ•ˆç›Šå’Œç¤¾ä¼šæ•ˆç›Šï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼š
- è¯·ä»¥å®ç”¨ã€å®¢è§‚çš„è§’åº¦åˆ†æ
- é‡ç‚¹çªå‡ºå®è·µåˆ›æ–°å’Œåº”ç”¨ä»·å€¼çš„å…·ä½“è¡¨ç°
- åŸºäºå®é™…åº”ç”¨åœºæ™¯è¿›è¡Œè¯„ä¼°
- å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
- å¦‚æœåº”ç”¨ä¿¡æ¯ä¸å……åˆ†ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºå¹¶æä¾›å»ºè®®

è¯·å¼€å§‹ä½ çš„åˆ†æï¼š"""

        return prompt
    
    def _get_fallback_methodology_analysis(self, thesis_info: Dict[str, str]) -> str:
        """è·å–æ–¹æ³•å­¦åˆ†æçš„å›é€€å†…å®¹"""
        methodology = thesis_info.get('methodology', '')
        if methodology:
            return f"**æ–¹æ³•å­¦åˆ†æ**ï¼šåŸºäºæä¾›çš„ç ”ç©¶æ–¹æ³•ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶é‡‡ç”¨äº†ç›¸åº”çš„ç ”ç©¶æ–¹æ³•ã€‚å»ºè®®è¿›ä¸€æ­¥æ˜ç¡®æ–¹æ³•å­¦åˆ›æ–°ç‚¹ï¼Œé€šè¿‡ä¸ç°æœ‰æ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”æ¥çªå‡ºç ”ç©¶çš„æ–¹æ³•è®ºè´¡çŒ®ã€‚"
        else:
            return "**æ–¹æ³•å­¦åˆ†æ**ï¼šç”±äºç¼ºå°‘è¯¦ç»†çš„ç ”ç©¶æ–¹æ³•ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥çš„æ–¹æ³•å­¦åˆ›æ–°åˆ†æã€‚å»ºè®®è¡¥å……å®Œå–„ç ”ç©¶æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬æŠ€æœ¯è·¯å¾„ã€å®ç°æ–¹æ¡ˆç­‰å…³é”®ä¿¡æ¯ã€‚"
    
    def _get_fallback_theory_analysis(self, thesis_info: Dict[str, str]) -> str:
        """è·å–ç†è®ºåˆ†æçš„å›é€€å†…å®¹"""
        theoretical_framework = thesis_info.get('theoretical_framework', '')
        if theoretical_framework:
            return f"**ç†è®ºè´¡çŒ®åˆ†æ**ï¼šåŸºäºæä¾›çš„ç†è®ºæ¡†æ¶ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†ç›¸åº”çš„ç†è®ºåŸºç¡€ã€‚å»ºè®®è¿›ä¸€æ­¥é˜è¿°ç†è®ºåˆ›æ–°çš„ç‹¬ç‰¹æ€§ï¼Œæ˜ç¡®ä¸ç°æœ‰ç†è®ºçš„å·®å¼‚å’Œæ”¹è¿›ä¹‹å¤„ã€‚"
        else:
            return "**ç†è®ºè´¡çŒ®åˆ†æ**ï¼šç”±äºç¼ºå°‘è¯¦ç»†çš„ç†è®ºæ¡†æ¶ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥çš„ç†è®ºè´¡çŒ®åˆ†æã€‚å»ºè®®å®Œå–„ç†è®ºæ¡†æ¶çš„é˜è¿°ï¼Œæ˜ç¡®ç†è®ºåˆ›æ–°ç‚¹å’Œå­¦æœ¯è´¡çŒ®ã€‚"
    
    def _get_fallback_practice_analysis(self, thesis_info: Dict[str, str]) -> str:
        """è·å–å®è·µåˆ†æçš„å›é€€å†…å®¹"""
        application_value = thesis_info.get('application_value', '')
        if application_value:
            return f"**å®è·µä»·å€¼åˆ†æ**ï¼šåŸºäºæä¾›çš„åº”ç”¨ä»·å€¼ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶å…·æœ‰ä¸€å®šçš„å®é™…åº”ç”¨æ½œåŠ›ã€‚å»ºè®®è¿›ä¸€æ­¥è¯¦ç»†æè¿°å…·ä½“çš„åº”ç”¨åœºæ™¯ã€å®æ–½æ–¹æ¡ˆå’Œé¢„æœŸæ•ˆæœã€‚"
        else:
            return "**å®è·µä»·å€¼åˆ†æ**ï¼šç”±äºç¼ºå°‘è¯¦ç»†çš„åº”ç”¨ä»·å€¼ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ·±å…¥çš„å®è·µä»·å€¼è¯„ä¼°ã€‚å»ºè®®æ˜ç¡®é˜è¿°ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼ã€æ¨å¹¿å‰æ™¯å’Œç¤¾ä¼šæ•ˆç›Šã€‚"
    
    def _create_markdown_content(self, input_file: str, analysis_data: Dict[str, Any], 
                               theme_analysis: Dict[str, Any], innovation_analysis: Dict[str, str],
                               literature_review_analysis: str = "", metadata_analysis_content: str = "",
                               literature_review_summary: Optional[Dict] = None) -> str:
        """åˆ›å»ºMarkdownå†…å®¹"""
        base_name = Path(input_file).stem
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        content = f"""# è®ºæ–‡è¯„ä¼°æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **è®ºæ–‡åç§°**: {base_name}
- **è¾“å…¥æ–‡ä»¶**: {input_file}
- **ç”Ÿæˆæ—¶é—´**: {timestamp}
- **è¯„ä¼°ç³»ç»Ÿ**: åŸºäºAIçš„å­¦ä½è®ºæ–‡åˆ›æ–°è¯„ä¼°ç³»ç»Ÿ

---

## ğŸ“Š è¯„ä¼°æ¦‚è§ˆ

"""
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats = self._calculate_statistics(analysis_data)
        content += self._format_statistics_section(stats)
        
        # å¦‚æœæœ‰æ–‡çŒ®ç»¼è¿°æ€»ç»“ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ¦‚è§ˆä¸­
        if literature_review_summary:
            content += "\n### ğŸ“š æ–‡çŒ®ç»¼è¿°è´¨é‡æ¦‚è§ˆ\n\n"
            if literature_review_summary.get('overall_score'):
                content += f"- **ç»¼åˆè¯„åˆ†**: {literature_review_summary['overall_score']:.1f}/10.0\n"
            if literature_review_summary.get('coverage_score'):
                content += f"- **è¦†ç›–åº¦è¯„åˆ†**: {literature_review_summary['coverage_score']:.1f}/10.0\n"
            if literature_review_summary.get('depth_score'):
                content += f"- **æ·±åº¦è¯„åˆ†**: {literature_review_summary['depth_score']:.1f}/10.0\n"
            if literature_review_summary.get('relevance_score'):
                content += f"- **ç›¸å…³æ€§è¯„åˆ†**: {literature_review_summary['relevance_score']:.1f}/10.0\n"
            if literature_review_summary.get('timeliness_score'):
                content += f"- **æ—¶æ•ˆæ€§è¯„åˆ†**: {literature_review_summary['timeliness_score']:.1f}/10.0\n"
            content += f"- **è¯¦ç»†åˆ†ææŠ¥å‘Š**: è¯·å‚è§ç‹¬ç«‹ç”Ÿæˆçš„æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Š\n\n"
        
        # æ·»åŠ ä¸»é¢˜åˆ†æå’Œå¹´ä»½åˆ†å¸ƒ
        content += self._format_theme_analysis_section(theme_analysis)
        
        # æ·»åŠ å„ä¸ªéƒ¨åˆ†
        sections = self.report_config.get('include_sections', [])
        
        if 'summary' in sections:
            content += self._format_summary_section(analysis_data)
        
        if 'innovation_analysis' in sections:
            content += self._format_enhanced_innovation_section(innovation_analysis, theme_analysis, analysis_data)
        
        # æ·»åŠ æ–‡çŒ®ç»¼è¿°åˆ†æï¼ˆæ–°åŠŸèƒ½ï¼‰
        if literature_review_analysis:
            content += f"\n\n{literature_review_analysis}\n"
        
        # æ·»åŠ æ–‡çŒ®å…ƒæ•°æ®åˆ†æï¼ˆæ–°åŠŸèƒ½ï¼‰
        if metadata_analysis_content:
            content += f"\n\n{metadata_analysis_content}\n"
        
        if 'related_papers' in sections:
            content += self._format_enhanced_related_papers_section(analysis_data, theme_analysis)
        
        if 'recommendations' in sections:
            content += self._format_recommendations_section(analysis_data)
        
        # æ·»åŠ é™„å½•
        content += self._format_appendix_section(analysis_data)
        
        return content
    
    def _calculate_statistics(self, analysis_data: Dict[str, Any]) -> Dict[str, int]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_chinese_papers': 0,
            'total_english_papers': 0,
            'dedup_chinese_papers': 0,
            'dedup_english_papers': 0,
            'top_chinese_papers': 0,
            'top_english_papers': 0
        }
        
        # è®¡ç®—å„ç±»è®ºæ–‡æ•°é‡
        if analysis_data['chinese_papers']:
            stats['total_chinese_papers'] = len(analysis_data['chinese_papers'])
        
        if analysis_data['english_papers']:
            stats['total_english_papers'] = len(analysis_data['english_papers'])
        
        if analysis_data['dedup_chinese']:
            stats['dedup_chinese_papers'] = len(analysis_data['dedup_chinese'])
        
        if analysis_data['dedup_english']:
            stats['dedup_english_papers'] = len(analysis_data['dedup_english'])
        
        if analysis_data['top_chinese']:
            stats['top_chinese_papers'] = len(analysis_data['top_chinese'])
        
        if analysis_data['top_english']:
            stats['top_english_papers'] = len(analysis_data['top_english'])
        
        return stats
    
    def _format_theme_analysis_section(self, theme_analysis: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸»é¢˜åˆ†æéƒ¨åˆ†"""
        content = """### ğŸ” æ–‡çŒ®ä¸»é¢˜åˆ†æ

**ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒï¼š**
"""
        
        # ä¸­æ–‡æ–‡çŒ®ä¸»é¢˜Top10
        if theme_analysis['chinese_themes']:
            sorted_chinese = sorted(theme_analysis['chinese_themes'].items(), 
                                  key=lambda x: x[1], reverse=True)[:10]
            content += "\n**ä¸­æ–‡æ–‡çŒ®ä¸»è¦ä¸»é¢˜ï¼š**\n"
            for i, (theme, count) in enumerate(sorted_chinese, 1):
                content += f"{i}. {theme} ({count}ç¯‡)\n"
        
        # è‹±æ–‡æ–‡çŒ®ä¸»é¢˜Top10
        if theme_analysis['english_themes']:
            sorted_english = sorted(theme_analysis['english_themes'].items(), 
                                  key=lambda x: x[1], reverse=True)[:10]
            content += "\n**è‹±æ–‡æ–‡çŒ®ä¸»è¦ä¸»é¢˜ï¼š**\n"
            for i, (theme, count) in enumerate(sorted_english, 1):
                content += f"{i}. {theme} ({count}ç¯‡)\n"
        
        # å¹´ä»½åˆ†å¸ƒåˆ†æ
        if theme_analysis['year_distribution']:
            content += "\n**å¹´ä»½åˆ†å¸ƒåˆ†æï¼š**\n"
            sorted_years = sorted(theme_analysis['year_distribution'].items())
            
            total_papers = sum(theme_analysis['year_distribution'].values())
            content += f"- ç ”ç©¶æ—¶é—´è·¨åº¦ï¼š{min(theme_analysis['year_distribution'].keys())}-{max(theme_analysis['year_distribution'].keys())}å¹´\n"
            content += f"- æ€»è®ºæ–‡æ•°é‡ï¼š{total_papers}ç¯‡\n"
            content += "- å¹´åº¦åˆ†å¸ƒï¼š\n"
            
            for year, count in sorted_years:
                percentage = (count / total_papers) * 100
                content += f"  - {year}å¹´: {count}ç¯‡ ({percentage:.1f}%)\n"
            
            # åˆ†æç ”ç©¶è¶‹åŠ¿
            recent_years = [year for year in theme_analysis['year_distribution'].keys() if year >= 2020]
            if recent_years:
                recent_count = sum(theme_analysis['year_distribution'][year] for year in recent_years)
                recent_percentage = (recent_count / total_papers) * 100
                content += f"- è¿‘5å¹´æ–‡çŒ®å æ¯”ï¼š{recent_percentage:.1f}% ({recent_count}ç¯‡)\n"
        
        content += "\n"
        return content
    
    def _format_enhanced_innovation_section(self, innovation_analysis: Dict[str, str], 
                                          theme_analysis: Dict[str, Any],
                                          analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¢å¼ºçš„åˆ›æ–°æ€§åˆ†æéƒ¨åˆ†"""
        
        # è®¡ç®—å®é™…çš„æ–‡çŒ®æ€»æ•° - ä¿®å¤æ•°æ®ä¸ä¸€è‡´é—®é¢˜
        total_chinese = len(analysis_data['top_chinese']) if analysis_data.get('top_chinese') else 0
        total_english = len(analysis_data['top_english']) if analysis_data.get('top_english') else 0
        total_papers = total_chinese + total_english
        
        # æ·»åŠ åˆ†ææ–¹æ³•è¯´æ˜
        analysis_method = "AIé©±åŠ¨çš„æ·±åº¦åˆ†æ" if self.ai_enabled else "åŸºäºè§„åˆ™çš„æ™ºèƒ½åˆ†æ"
        
        content = f"""## ğŸ”¬ åˆ›æ–°æ€§åˆ†æ

*æœ¬èŠ‚ä½¿ç”¨{analysis_method}æ–¹æ³•ç”Ÿæˆ*

### ğŸ†• åˆ›æ–°ç‚¹è¯†åˆ«

åŸºäºä¸{total_papers}ç¯‡ç›¸å…³æ–‡çŒ®çš„æ·±åº¦å¯¹æ¯”åˆ†æï¼Œè¯†åˆ«å‡ºä»¥ä¸‹æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š

#### 1. **æ–¹æ³•å­¦åˆ›æ–°**
{innovation_analysis['methodology']}

#### 2. **ç†è®ºè´¡çŒ®**
{innovation_analysis['theory']}

#### 3. **å®è·µä»·å€¼**
{innovation_analysis['practice']}

### ğŸ“Š åˆ›æ–°åº¦è¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| åŸåˆ›æ€§ | â­â­â­â­â˜† | åŸºäº{analysis_method}ï¼Œç ”ç©¶åœ¨æ–¹æ³•æˆ–è§‚ç‚¹ä¸Šå…·æœ‰ä¸€å®šçš„åŸåˆ›æ€§ |
| æ–°é¢–æ€§ | â­â­â­â­â˜† | ç ”ç©¶æ–¹æ³•æˆ–ç†è®ºæ¡†æ¶ç›¸å¯¹äºç°æœ‰æ–‡çŒ®å…·æœ‰æ–°é¢–æ€§ |
| é‡è¦æ€§ | â­â­â­â˜†â˜† | ç ”ç©¶è§£å†³çš„é—®é¢˜å…·æœ‰ä¸€å®šçš„å­¦æœ¯ä»·å€¼å’Œå®è·µæ„ä¹‰ |
| å½±å“åŠ› | â­â­â­â˜†â˜† | ç ”ç©¶æˆæœé¢„æœŸå¯¹ç›¸å…³é¢†åŸŸäº§ç”Ÿç§¯æå½±å“ |

*æ³¨ï¼šè¯„åˆ†åŸºäº{analysis_method}å’Œæ–‡çŒ®å¯¹æ¯”åˆ†æ*

### ğŸ“ˆ ç ”ç©¶è¶‹åŠ¿å¯¹æ¯”

**ä¸ç°æœ‰ç ”ç©¶çš„å·®å¼‚åŒ–å®šä½ï¼š**
"""
        
        # æ·»åŠ åŸºäºä¸»é¢˜åˆ†æçš„è¶‹åŠ¿å¯¹æ¯”
        if theme_analysis['chinese_themes'] or theme_analysis['english_themes']:
            content += self._generate_trend_comparison(theme_analysis)
        
        content += "\n"
        return content
    
    def _generate_trend_comparison(self, theme_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆç ”ç©¶è¶‹åŠ¿å¯¹æ¯”åˆ†æ"""
        comparison = ""
        
        # åˆ†æä¸»è¦ç ”ç©¶é¢†åŸŸ
        all_themes = {}
        if theme_analysis['chinese_themes']:
            all_themes.update(theme_analysis['chinese_themes'])
        if theme_analysis['english_themes']:
            for theme, count in theme_analysis['english_themes'].items():
                all_themes[theme] = all_themes.get(theme, 0) + count
        
        top_themes = sorted(all_themes.items(), key=lambda x: x[1], reverse=True)[:5]
        
        comparison += "- **ä¸»æµç ”ç©¶æ–¹å‘**ï¼š\n"
        for theme, count in top_themes:
            comparison += f"  - {theme}: {count}ç¯‡ç›¸å…³ç ”ç©¶\n"
        
        # é€šç”¨åŒ–çš„å·®å¼‚åŒ–åˆ†æ
        comparison += "- **æœ¬ç ”ç©¶çš„å·®å¼‚åŒ–**ï¼šåœ¨ç°æœ‰ç ”ç©¶åŸºç¡€ä¸Šï¼Œæœ¬ç ”ç©¶ä»æ–°çš„è§’åº¦åˆ‡å…¥ï¼Œæä¾›äº†ä¸åŒçš„ç†è®ºè§†è§’æˆ–æ–¹æ³•è·¯å¾„\n"
        comparison += "- **å­¦æœ¯è´¡çŒ®**ï¼šä¸ºç›¸å…³ç ”ç©¶é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œä¸°å¯Œäº†è¯¥é¢†åŸŸçš„ç†è®ºä½“ç³»å’Œå®è·µåº”ç”¨\n"
        comparison += "- **ç ”ç©¶ä»·å€¼**ï¼šé€šè¿‡ä¸ç°æœ‰æ–‡çŒ®çš„å¯¹æ¯”åˆ†æï¼Œæ˜¾ç¤ºå‡ºæœ¬ç ”ç©¶åœ¨æ–¹æ³•åˆ›æ–°ã€ç†è®ºçªç ´æˆ–åº”ç”¨æ‹“å±•æ–¹é¢çš„ç‹¬ç‰¹ä»·å€¼\n"
        
        return comparison
    
    def _format_enhanced_related_papers_section(self, analysis_data: Dict[str, Any], 
                                              theme_analysis: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¢å¼ºçš„ç›¸å…³è®ºæ–‡éƒ¨åˆ†"""
        content = """## ğŸ“š ç›¸å…³æ–‡çŒ®åˆ†æ

### ğŸ” æ–‡çŒ®æ£€ç´¢ç»“æœ

**æ£€ç´¢èŒƒå›´ä¸ç­–ç•¥ï¼š**
- æ£€ç´¢æ•°æ®åº“ï¼šCNKIä¸­å›½çŸ¥ç½‘ã€å›½é™…å­¦æœ¯æ•°æ®åº“
- æ£€ç´¢å…³é”®è¯ï¼šåŸºäºè®ºæ–‡ä¸»é¢˜å’Œå…³é”®è¯è¿›è¡Œç²¾å‡†æ£€ç´¢
- æ—¶é—´èŒƒå›´ï¼šè¿‘10å¹´ç›¸å…³ç ”ç©¶æ–‡çŒ®
- æ£€ç´¢ç­–ç•¥ï¼šé‡‡ç”¨ä¸»é¢˜è¯ä¸è‡ªç”±è¯ç»“åˆçš„æ–¹å¼ï¼Œç¡®ä¿æ£€ç´¢çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§

**æ–‡çŒ®è´¨é‡åˆ†æï¼š**
"""
        
        # è®¡ç®—æ–‡çŒ®ç»Ÿè®¡
        total_chinese = len(analysis_data['top_chinese']) if analysis_data['top_chinese'] else 0
        total_english = len(analysis_data['top_english']) if analysis_data['top_english'] else 0
        total_papers = total_chinese + total_english
        
        content += f"- å…±æ£€ç´¢åˆ°{total_papers}ç¯‡é«˜è´¨é‡ç›¸å…³æ–‡çŒ®ï¼Œå…¶ä¸­ä¸­æ–‡æ–‡çŒ®{total_chinese}ç¯‡ï¼Œè‹±æ–‡æ–‡çŒ®{total_english}ç¯‡\n"
        content += "- æ–‡çŒ®æ¥æºæ¶µç›–é¡¶çº§æœŸåˆŠå’Œé‡è¦ä¼šè®®ï¼Œä¿è¯äº†ç ”ç©¶çš„æƒå¨æ€§\n"
        content += "- æ–‡çŒ®æ—¶é—´åˆ†å¸ƒåˆç†ï¼Œæ—¢åŒ…å«åŸºç¡€ç†è®ºç ”ç©¶ï¼Œä¹ŸåŒ…å«æœ€æ–°å‘å±•åŠ¨æ€\n\n"
        
        # æ·»åŠ ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒåˆ†æ
        if theme_analysis['chinese_themes'] or theme_analysis['english_themes']:
            content += self._generate_research_hotspots_analysis(theme_analysis)
        
        # æ·»åŠ TOPè®ºæ–‡åˆ—è¡¨
        if analysis_data['top_chinese'] or analysis_data['top_english']:
            content += "### â­ é‡ç‚¹ç›¸å…³è®ºæ–‡\n\n"
            
            # ä¸­æ–‡TOPè®ºæ–‡
            if analysis_data['top_chinese']:
                content += "#### ä¸­æ–‡æ–‡çŒ® (ç²¾é€‰ä»£è¡¨æ€§ç ”ç©¶)\n\n"
                for i, paper in enumerate(analysis_data['top_chinese'][:5], 1):
                    content += self._format_key_paper(paper, i, is_chinese=True)
            
            # è‹±æ–‡TOPè®ºæ–‡
            if analysis_data['top_english']:
                content += "#### è‹±æ–‡æ–‡çŒ® (å›½é™…å‰æ²¿ç ”ç©¶)\n\n"
                for i, paper in enumerate(analysis_data['top_english'][:5], 1):
                    content += self._format_key_paper(paper, i, is_chinese=False)
        
        return content
    
    def _format_key_paper(self, paper: Dict[str, Any], index: int, is_chinese: bool = True) -> str:
        """æ ¼å¼åŒ–é‡ç‚¹è®ºæ–‡ä¿¡æ¯ï¼Œå……åˆ†åˆ©ç”¨å…ƒæ•°æ®"""
        content = ""
        
        # åŸºæœ¬ä¿¡æ¯
        title = paper.get('Title', 'æœªçŸ¥æ ‡é¢˜' if is_chinese else 'Unknown Title')
        year = paper.get('PublicationYear', 'æœªçŸ¥å¹´ä»½' if is_chinese else 'Unknown Year')
        keywords = paper.get('KeyWords', '')
        if is_chinese:
            keywords = keywords.replace(';;', 'ã€')
        else:
            keywords = keywords.replace(';', ', ')
        
        content += f"{index}. **{title}**\n"
        
        # ä½œè€…ä¿¡æ¯
        authors = paper.get('Authors', [])
        if authors and isinstance(authors, list):
            author_info = self._format_authors(authors, is_chinese)
            content += f"   - **ä½œè€…**: {author_info}\n"
        
        # æœºæ„ä¿¡æ¯
        affiliations = paper.get('Affiliations', [])
        if affiliations and isinstance(affiliations, list):
            affiliation_names = [aff.get('name', '') for aff in affiliations if isinstance(aff, dict) and aff.get('name')]
            if affiliation_names:
                content += f"   - **æœºæ„**: {'; '.join(affiliation_names[:3])}\n"  # æœ€å¤šæ˜¾ç¤º3ä¸ªæœºæ„
        
        # å‘è¡¨ä¿¡æ¯
        content += f"   - **å‘è¡¨å¹´ä»½**: {year}\n"
        
        # æœŸåˆŠ/ä¼šè®®ä¿¡æ¯
        source = paper.get('Source', {})
        journal = paper.get('Journal', '')
        if (source and isinstance(source, dict)) or journal:
            publication_info = self._format_publication_info(source if isinstance(source, dict) else {}, journal, is_chinese)
            if publication_info:
                content += f"   - **å‘è¡¨è½½ä½“**: {publication_info}\n"
        
        # åŸºé‡‘èµ„åŠ©ä¿¡æ¯
        funds = paper.get('Funds', [])
        if funds and isinstance(funds, list):
            fund_info = self._format_funds(funds, is_chinese)
            if fund_info:
                content += f"   - **åŸºé‡‘èµ„åŠ©**: {fund_info}\n"
        
        # å…³é”®è¯
        if keywords:
            content += f"   - **å…³é”®è¯**: {keywords}\n"
        
        # å½±å“åŠ›æŒ‡æ ‡
        metrics = paper.get('Metrics', {})
        if metrics and isinstance(metrics, dict):
            metrics_info = self._format_metrics(metrics, is_chinese)
            if metrics_info:
                content += f"   - **å½±å“åŠ›æŒ‡æ ‡**: {metrics_info}\n"
        
        # æ‘˜è¦èŠ‚é€‰
        abstract = paper.get('Abstract', '')
        if abstract:
            abstract_snippet = abstract[:120] + "..." if len(abstract) > 120 else abstract
            content += f"   - **æ‘˜è¦èŠ‚é€‰**: {abstract_snippet}\n"
        
        # ç›¸å…³åº¦åˆ†æ
        relevance = self._analyze_paper_relevance(paper)
        content += f"   - **å­¦æœ¯ä»·å€¼**: åœ¨{relevance}æ–¹é¢å…·æœ‰é‡è¦å‚è€ƒä»·å€¼\n\n"
        
        return content
    
    def _format_authors(self, authors: List[Dict[str, Any]], is_chinese: bool = True) -> str:
        """æ ¼å¼åŒ–ä½œè€…ä¿¡æ¯"""
        if not authors or not isinstance(authors, list):
            return "æœªçŸ¥ä½œè€…" if is_chinese else "Unknown Authors"
        
        author_strs = []
        for author in authors[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªä½œè€…
            if not isinstance(author, dict):
                continue
            name = author.get('name', '')
            if name:
                if author.get('corresponding'):
                    name += "*"  # é€šè®¯ä½œè€…æ ‡è®°
                author_strs.append(name)
        
        if len(authors) > 5:
            author_strs.append("ç­‰" if is_chinese else "et al.")
        
        return ", ".join(author_strs)
    
    def _format_publication_info(self, source: Dict[str, Any], journal: str, is_chinese: bool = True) -> str:
        """æ ¼å¼åŒ–å‘è¡¨ä¿¡æ¯"""
        if source:
            title = source.get('title', journal)
            volume = source.get('volume', '')
            issue = source.get('issue', '')
            
            if title:
                info = title
                if volume and issue:
                    info += f", {volume}({issue})"
                elif volume:
                    info += f", {volume}"
                return info
        
        return journal if journal else ""
    
    def _format_funds(self, funds: List[Dict[str, Any]], is_chinese: bool = True) -> str:
        """æ ¼å¼åŒ–åŸºé‡‘ä¿¡æ¯"""
        if not funds or not isinstance(funds, list):
            return ""
        
        fund_titles = []
        for fund in funds[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ªåŸºé‡‘
            if not isinstance(fund, dict):
                continue
            title = fund.get('title', '')
            if title:
                # ç®€åŒ–åŸºé‡‘ä¿¡æ¯æ˜¾ç¤º
                if 'å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘' in title:
                    fund_titles.append("å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘")
                elif 'National Natural Science Foundation' in title:
                    fund_titles.append("NSFC")
                elif 'å›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’' in title:
                    fund_titles.append("å›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’")
                elif 'çœè‡ªç„¶ç§‘å­¦åŸºé‡‘' in title or 'å¸‚è‡ªç„¶ç§‘å­¦åŸºé‡‘' in title:
                    fund_titles.append("çœ/å¸‚çº§åŸºé‡‘")
                else:
                    # æå–åŸºé‡‘åç§°çš„å…³é”®éƒ¨åˆ†
                    short_title = title[:30] + "..." if len(title) > 30 else title
                    fund_titles.append(short_title)
        
        if len(funds) > 2:
            fund_titles.append("ç­‰" if is_chinese else "etc.")
        
        return "; ".join(fund_titles)
    
    def _format_metrics(self, metrics: Dict[str, Any], is_chinese: bool = True) -> str:
        """æ ¼å¼åŒ–å½±å“åŠ›æŒ‡æ ‡"""
        metric_strs = []
        
        download_count = metrics.get('download_count')
        citation_count = metrics.get('citation_count')
        
        if download_count is not None and download_count > 0:
            metric_strs.append(f"ä¸‹è½½{download_count}æ¬¡" if is_chinese else f"{download_count} downloads")
        
        if citation_count is not None and citation_count > 0:
            metric_strs.append(f"è¢«å¼•{citation_count}æ¬¡" if is_chinese else f"{citation_count} citations")
        
        return "; ".join(metric_strs) if metric_strs else ""
    
    def _generate_research_hotspots_analysis(self, theme_analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒåˆ†æ"""
        content = "**ç ”ç©¶çƒ­ç‚¹åˆ†å¸ƒï¼š**\n"
        
        # é€šç”¨ç ”ç©¶ç±»å‹åˆ†ç±»
        method_keywords = ['æ–¹æ³•', 'method', 'æ¨¡å‹', 'model', 'ç®—æ³•', 'algorithm', 'åˆ†æ', 'analysis', 'æŠ€æœ¯', 'technology']
        theory_keywords = ['ç†è®º', 'theory', 'æ¡†æ¶', 'framework', 'æ¦‚å¿µ', 'concept', 'åŸç†', 'principle', 'æœºåˆ¶', 'mechanism']
        empirical_keywords = ['å®è¯', 'empirical', 'å®éªŒ', 'experiment', 'æ¡ˆä¾‹', 'case', 'è°ƒæŸ¥', 'survey', 'æ•°æ®', 'data']
        application_keywords = ['åº”ç”¨', 'application', 'å®è·µ', 'practice', 'ç³»ç»Ÿ', 'system', 'å¹³å°', 'platform', 'å·¥å…·', 'tool']
        
        categories = {
            'method': 0, 'theory': 0, 'empirical': 0, 'application': 0
        }
        
        all_themes = {}
        if theme_analysis['chinese_themes']:
            all_themes.update(theme_analysis['chinese_themes'])
        if theme_analysis['english_themes']:
            for theme, count in theme_analysis['english_themes'].items():
                all_themes[theme] = all_themes.get(theme, 0) + count
        
        # é€šç”¨åˆ†ç±»ç»Ÿè®¡
        for theme, count in all_themes.items():
            theme_lower = theme.lower()
            if any(keyword in theme_lower for keyword in method_keywords):
                categories['method'] += count
            elif any(keyword in theme_lower for keyword in theory_keywords):
                categories['theory'] += count
            elif any(keyword in theme_lower for keyword in empirical_keywords):
                categories['empirical'] += count
            elif any(keyword in theme_lower for keyword in application_keywords):
                categories['application'] += count
        
        total = sum(categories.values())
        if total > 0:
            content += f"1. **æ–¹æ³•ä¸æŠ€æœ¯ç ”ç©¶** ({categories['method']/total*100:.1f}%)ï¼šåŒ…æ‹¬ç ”ç©¶æ–¹æ³•ã€åˆ†ææŠ€æœ¯ã€ç®—æ³•æ¨¡å‹ç­‰\n"
            content += f"2. **ç†è®ºä¸æ¡†æ¶æ„å»º** ({categories['theory']/total*100:.1f}%)ï¼šæ¶µç›–ç†è®ºå‘å±•ã€æ¦‚å¿µæ¡†æ¶ã€æœºåˆ¶åˆ†æç­‰\n"
            content += f"3. **å®è¯ä¸æ•°æ®ç ”ç©¶** ({categories['empirical']/total*100:.1f}%)ï¼šåŒ…æ‹¬å®éªŒç ”ç©¶ã€æ¡ˆä¾‹åˆ†æã€æ•°æ®æŒ–æ˜ç­‰\n"
            content += f"4. **åº”ç”¨ä¸ç³»ç»Ÿå®ç°** ({categories['application']/total*100:.1f}%)ï¼šåŒ…å«å®è·µåº”ç”¨ã€ç³»ç»Ÿå¼€å‘ã€å·¥å…·è®¾è®¡ç­‰\n\n"
        else:
            # å¦‚æœæ— æ³•åˆ†ç±»ï¼Œæä¾›é€šç”¨æè¿°
            top_themes = sorted(all_themes.items(), key=lambda x: x[1], reverse=True)[:8]
            for i, (theme, count) in enumerate(top_themes, 1):
                content += f"{i}. **{theme}** ({count}ç¯‡ç ”ç©¶)\n"
            content += "\n"
        
        return content
    
    def _analyze_paper_relevance(self, paper: Dict[str, Any]) -> str:
        """åˆ†æè®ºæ–‡ç›¸å…³åº¦"""
        keywords = paper.get('KeyWords', '').lower()
        title = paper.get('Title', '').lower()
        
        # é€šç”¨å­¦æœ¯ç ”ç©¶åˆ†ç±»
        if any(keyword in keywords or keyword in title for keyword in ['æ–¹æ³•', 'method', 'ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model', 'æŠ€æœ¯', 'technique']):
            return 'ç ”ç©¶æ–¹æ³•ä¸æŠ€æœ¯'
        elif any(keyword in keywords or keyword in title for keyword in ['ç†è®º', 'theory', 'æ¡†æ¶', 'framework', 'æ¦‚å¿µ', 'concept']):
            return 'ç†è®ºæ¡†æ¶æ„å»º'
        elif any(keyword in keywords or keyword in title for keyword in ['å®éªŒ', 'experiment', 'å®è¯', 'empirical', 'æ¡ˆä¾‹', 'case study']):
            return 'å®è¯ç ”ç©¶åˆ†æ'
        elif any(keyword in keywords or keyword in title for keyword in ['åº”ç”¨', 'application', 'å®è·µ', 'practice', 'ç³»ç»Ÿ', 'system']):
            return 'åº”ç”¨å®è·µç ”ç©¶'
        elif any(keyword in keywords or keyword in title for keyword in ['åˆ†æ', 'analysis', 'è¯„ä¼°', 'evaluation', 'æµ‹é‡', 'measurement']):
            return 'åˆ†æè¯„ä¼°æ–¹æ³•'
        elif any(keyword in keywords or keyword in title for keyword in ['ä¼˜åŒ–', 'optimization', 'æ”¹è¿›', 'improvement', 'åˆ›æ–°', 'innovation']):
            return 'ä¼˜åŒ–æ”¹è¿›ç ”ç©¶'
        else:
            return 'ç›¸å…³ç†è®ºç ”ç©¶'
    
    def _format_statistics_section(self, stats: Dict[str, int]) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†"""
        return f"""### ğŸ“ˆ æ•°æ®ç»Ÿè®¡

| ç±»åˆ« | ä¸­æ–‡è®ºæ–‡ | è‹±æ–‡è®ºæ–‡ | æ€»è®¡ |
|------|----------|----------|------|
| ç›¸å…³è®ºæ–‡ | {stats['total_chinese_papers']} | {stats['total_english_papers']} | {stats['total_chinese_papers'] + stats['total_english_papers']} |
| å»é‡å | {stats['dedup_chinese_papers']} | {stats['dedup_english_papers']} | {stats['dedup_chinese_papers'] + stats['dedup_english_papers']} |
| TOPè®ºæ–‡ | {stats['top_chinese_papers']} | {stats['top_english_papers']} | {stats['top_chinese_papers'] + stats['top_english_papers']} |

"""
    
    def _format_summary_section(self, analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ‘˜è¦éƒ¨åˆ†"""
        total_papers = sum([
            len(analysis_data[key]) if analysis_data[key] else 0
            for key in ['dedup_chinese', 'dedup_english']
        ])
        
        return f"""## ğŸ“‹ è¯„ä¼°æ‘˜è¦

æœ¬æ¬¡è¯„ä¼°å…±æ£€ç´¢åˆ° **{total_papers}** ç¯‡ç›¸å…³è®ºæ–‡ï¼ˆå»é‡åï¼‰ï¼Œæ¶µç›–ä¸­è‹±æ–‡å­¦æœ¯æ–‡çŒ®ã€‚

### ğŸ¯ ä¸»è¦å‘ç°

- **æ–‡çŒ®è¦†ç›–åº¦**: æ£€ç´¢åˆ°äº†ä¸°å¯Œçš„ç›¸å…³ç ”ç©¶æ–‡çŒ®
- **ç ”ç©¶é¢†åŸŸ**: æ¶µç›–äº†è¯¥è®ºæ–‡ä¸»é¢˜çš„ä¸»è¦ç ”ç©¶æ–¹å‘
- **æ—¶æ•ˆæ€§**: åŒ…å«äº†æœ€æ–°çš„ç ”ç©¶æˆæœå’Œå‘å±•è¶‹åŠ¿

### â­ åˆ›æ–°åº¦è¯„ä¼°

åŸºäºç›¸å…³æ–‡çŒ®åˆ†æï¼Œè¯¥è®ºæ–‡åœ¨ä»¥ä¸‹æ–¹é¢æ˜¾ç¤ºå‡ºåˆ›æ–°æ€§ï¼š

1. **ç ”ç©¶æ–¹æ³•åˆ›æ–°**: [éœ€è¦å…·ä½“åˆ†æ]
2. **ç†è®ºè´¡çŒ®**: [éœ€è¦å…·ä½“åˆ†æ] 
3. **åº”ç”¨ä»·å€¼**: [éœ€è¦å…·ä½“åˆ†æ]

"""
    
    def _format_innovation_section(self, analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–åˆ›æ–°æ€§åˆ†æéƒ¨åˆ†"""
        return """## ğŸ”¬ åˆ›æ–°æ€§åˆ†æ

### ğŸ†• åˆ›æ–°ç‚¹è¯†åˆ«

åŸºäºæ–‡çŒ®å¯¹æ¯”åˆ†æï¼Œè¯†åˆ«å‡ºä»¥ä¸‹æ½œåœ¨åˆ›æ–°ç‚¹ï¼š

1. **æ–¹æ³•å­¦åˆ›æ–°**
   - åˆ›æ–°çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„
   - ç°æœ‰æ–¹æ³•çš„æ”¹è¿›å’Œä¼˜åŒ–

2. **ç†è®ºè´¡çŒ®**
   - æ–°çš„ç†è®ºæ¡†æ¶æˆ–æ¨¡å‹
   - å¯¹ç°æœ‰ç†è®ºçš„è¡¥å……å’Œå®Œå–„

3. **å®è·µä»·å€¼**
   - è§£å†³å®é™…é—®é¢˜çš„æ–°æ–¹æ¡ˆ
   - å…·æœ‰åº”ç”¨å‰æ™¯çš„æŠ€æœ¯æˆæœ

### ğŸ“Š åˆ›æ–°åº¦è¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| åŸåˆ›æ€§ | â­â­â­â­â˜† | å…·æœ‰è¾ƒé«˜çš„åŸåˆ›æ€§ |
| æ–°é¢–æ€§ | â­â­â­â­â˜† | æ–¹æ³•æˆ–è§‚ç‚¹å…·æœ‰æ–°é¢–æ€§ |
| é‡è¦æ€§ | â­â­â­â˜†â˜† | å…·æœ‰ä¸€å®šçš„å­¦æœ¯æˆ–å®è·µæ„ä¹‰ |
| å½±å“åŠ› | â­â­â­â˜†â˜† | é¢„æœŸå…·æœ‰ä¸€å®šçš„å­¦æœ¯å½±å“ |

"""
    
    def _format_related_papers_section(self, analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ç›¸å…³è®ºæ–‡éƒ¨åˆ†"""
        content = """## ğŸ“š ç›¸å…³æ–‡çŒ®åˆ†æ

### ğŸ” æ–‡çŒ®æ£€ç´¢ç»“æœ

"""
        
        # æ·»åŠ TOPè®ºæ–‡åˆ—è¡¨
        if analysis_data['top_chinese'] or analysis_data['top_english']:
            content += "### â­ é‡ç‚¹ç›¸å…³è®ºæ–‡\n\n"
            
            # ä¸­æ–‡TOPè®ºæ–‡
            if analysis_data['top_chinese']:
                content += "#### ä¸­æ–‡æ–‡çŒ®\n\n"
                for i, paper in enumerate(analysis_data['top_chinese'][:10], 1):
                    title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    authors = paper.get('authors', 'æœªçŸ¥ä½œè€…')
                    content += f"{i}. **{title}**\n"
                    content += f"   - ä½œè€…: {authors}\n"
                    if 'year' in paper:
                        content += f"   - å¹´ä»½: {paper['year']}\n"
                    content += "\n"
            
            # è‹±æ–‡TOPè®ºæ–‡
            if analysis_data['top_english']:
                content += "#### è‹±æ–‡æ–‡çŒ®\n\n"
                for i, paper in enumerate(analysis_data['top_english'][:10], 1):
                    title = paper.get('title', 'Unknown Title')
                    authors = paper.get('authors', 'Unknown Authors')
                    content += f"{i}. **{title}**\n"
                    content += f"   - Authors: {authors}\n"
                    if 'year' in paper:
                        content += f"   - Year: {paper['year']}\n"
                    content += "\n"
        
        return content
    
    def _format_recommendations_section(self, analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å»ºè®®éƒ¨åˆ†"""
        return """## ğŸ’¡ æ”¹è¿›å»ºè®®

### ğŸ¯ ç ”ç©¶æ·±åŒ–å»ºè®®

1. **æ–‡çŒ®ç»¼è¿°å®Œå–„**
   - è¡¥å……æœ€æ–°çš„ç›¸å…³ç ”ç©¶æˆæœ
   - åŠ å¼ºä¸å›½é™…å‰æ²¿ç ”ç©¶çš„å¯¹æ¯”åˆ†æ
   - æ‹“å±•è·¨å­¦ç§‘æ–‡çŒ®çš„å¼•ç”¨å’Œè®¨è®º

2. **æ–¹æ³•è®ºæ”¹è¿›**
   - è€ƒè™‘é‡‡ç”¨æ›´å…ˆè¿›çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯
   - å¢åŠ å®è¯ç ”ç©¶ã€æ¡ˆä¾‹åˆ†ææˆ–å®éªŒéªŒè¯
   - å®Œå–„ç ”ç©¶è®¾è®¡çš„ç§‘å­¦æ€§å’Œä¸¥è°¨æ€§

3. **åˆ›æ–°ç‚¹çªå‡º**
   - æ˜ç¡®è¡¨è¿°ç ”ç©¶çš„æ ¸å¿ƒåˆ›æ–°ç‚¹å’Œè´¡çŒ®
   - åŠ å¼ºä¸ç°æœ‰ç ”ç©¶çš„å·®å¼‚åŒ–è®ºè¯
   - çªå‡ºç ”ç©¶çš„åŸåˆ›æ€§å’Œæ–°é¢–æ€§

### ğŸ“ˆ è´¨é‡æå‡å»ºè®®

1. **ç†è®ºè´¡çŒ®å¼ºåŒ–**
   - æ·±åŒ–ç†è®ºæ¡†æ¶çš„æ„å»ºå’Œè®ºè¯
   - åŠ å¼ºç†è®ºä¸å®è·µçš„ç»“åˆ
   - æå‡ç†è®ºåˆ›æ–°çš„æ·±åº¦å’Œå¹¿åº¦

2. **å®è¯åˆ†æå®Œå–„**
   - å……å®æ•°æ®æ”¯æ’‘å’Œåˆ†æè¿‡ç¨‹
   - å¢åŠ å¤šè§’åº¦ã€å¤šå±‚æ¬¡çš„éªŒè¯
   - æé«˜ç»“æœçš„å¯ä¿¡åº¦å’Œè¯´æœåŠ›

3. **åº”ç”¨ä»·å€¼ä½“ç°**
   - æ˜ç¡®ç ”ç©¶æˆæœçš„å®é™…åº”ç”¨ä»·å€¼
   - æä¾›å…·ä½“çš„åº”ç”¨åœºæ™¯å’Œæ¨å¹¿å»ºè®®
   - è¯„ä¼°ç ”ç©¶çš„ç¤¾ä¼šæ•ˆç›Šå’Œç»æµæ•ˆç›Š

### ğŸš€ å‘å±•æ–¹å‘å»ºè®®

1. **ç ”ç©¶æ‹“å±•**
   - æ¢ç´¢ç›¸å…³é¢†åŸŸçš„å»¶ä¼¸ç ”ç©¶æœºä¼š
   - è€ƒè™‘å¤šå­¦ç§‘äº¤å‰èåˆçš„å¯èƒ½æ€§
   - è§„åˆ’åç»­ç ”ç©¶çš„å‘å±•è·¯å¾„

2. **æˆæœè½¬åŒ–**
   - åŠ å¼ºç ”ç©¶æˆæœçš„å®è·µè½¬åŒ–
   - å¯»æ±‚äº§å­¦ç ”åˆä½œæœºä¼š
   - æå‡ç ”ç©¶çš„å½±å“åŠ›å’ŒçŸ¥ååº¦

"""
    
    def _format_appendix_section(self, analysis_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–é™„å½•éƒ¨åˆ†"""
        content = """---

## ğŸ“ é™„å½•

### ğŸ“„ æ•°æ®æ–‡ä»¶åˆ—è¡¨

æœ¬æ¬¡è¯„ä¼°ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶ï¼š

"""
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        file_types = [
            ('chinese_papers', 'ä¸­æ–‡ç›¸å…³è®ºæ–‡'),
            ('english_papers', 'è‹±æ–‡ç›¸å…³è®ºæ–‡'),
            ('dedup_chinese', 'ä¸­æ–‡å»é‡è®ºæ–‡'),
            ('dedup_english', 'è‹±æ–‡å»é‡è®ºæ–‡'),
            ('top_chinese', 'ä¸­æ–‡TOPè®ºæ–‡'),
            ('top_english', 'è‹±æ–‡TOPè®ºæ–‡')
        ]
        
        for key, description in file_types:
            if analysis_data[key] is not None:
                content += f"- âœ… {description}: å·²ç”Ÿæˆ\n"
            else:
                content += f"- âŒ {description}: æœªæ‰¾åˆ°\n"
        
        content += f"""
### âš™ï¸ ç³»ç»Ÿé…ç½®

- **TopNè®ºæ–‡æ•°é‡**: {self.config_mgr.get_top_papers_count()}
- **æ”¯æŒæ–‡ä»¶æ ¼å¼**: {', '.join(self.config_mgr.get_supported_formats())}
- **è¾“å‡ºç›®å½•**: {self.config_mgr.get_output_dir()}

### ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚

---

*æœ¬æŠ¥å‘Šç”±åŸºäºAIçš„å­¦ä½è®ºæ–‡åˆ›æ–°è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return content
    
    def _build_shared_context_prompt(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """æ„å»ºå…±äº«ä¸Šä¸‹æ–‡æç¤ºè¯ï¼ˆä¸€æ¬¡æ€§å‘é€æ‰€æœ‰èƒŒæ™¯ä¿¡æ¯ï¼‰"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å­¦æœ¯è¯„å®¡ä¸“å®¶ï¼Œæˆ‘éœ€è¦ä½ åˆ†æä¸€ç¯‡å­¦æœ¯è®ºæ–‡çš„åˆ›æ–°æ€§ã€‚è¯·ä»”ç»†é˜…è¯»å¹¶è®°ä½ä»¥ä¸‹è®ºæ–‡çš„åŸºæœ¬ä¿¡æ¯å’Œç›¸å…³æ–‡çŒ®èƒŒæ™¯ï¼Œæˆ‘å°†åœ¨åŒä¸€å¯¹è¯ä¸­ä¾æ¬¡è¯¢é—®ä¸‰ä¸ªç»´åº¦çš„åˆ†æã€‚

**ç›®æ ‡è®ºæ–‡ä¿¡æ¯**ï¼š
- è®ºæ–‡æ ‡é¢˜ï¼š{thesis_info.get('title', 'æœªæä¾›')}
- ä¸­æ–‡å…³é”®è¯ï¼š{thesis_info.get('keywords_cn', 'æœªæä¾›')}
- è‹±æ–‡å…³é”®è¯ï¼š{thesis_info.get('keywords_en', 'æœªæä¾›')}
- ç ”ç©¶æ–¹æ³•ï¼š{thesis_info.get('methodology', 'æœªæä¾›')}
- ç†è®ºæ¡†æ¶ï¼š{thesis_info.get('theoretical_framework', 'æœªæä¾›')}
- ä¸»è¦åˆ›æ–°ç‚¹ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}
- ç ”ç©¶ç»“è®ºï¼š{thesis_info.get('research_conclusions', 'æœªæä¾›')}
- å®é™…é—®é¢˜ï¼š{thesis_info.get('practical_problems', 'æœªæä¾›')}
- è§£å†³æ–¹æ¡ˆï¼š{thesis_info.get('proposed_solutions', 'æœªæä¾›')}
- åº”ç”¨ä»·å€¼ï¼š{thesis_info.get('application_value', 'æœªæä¾›')}

**ç›¸å…³æ–‡çŒ®èƒŒæ™¯**ï¼š
{literature_context}

è¯·ç¡®è®¤ä½ å·²ç»ç†è§£å¹¶è®°ä½äº†ä»¥ä¸Šè®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ã€‚æ¥ä¸‹æ¥æˆ‘å°†åˆ†åˆ«è¯¢é—®æ–¹æ³•å­¦åˆ›æ–°ã€ç†è®ºè´¡çŒ®å’Œå®è·µä»·å€¼çš„åˆ†æã€‚

è¯·ç®€å•å›å¤"å·²ç†è§£å¹¶è®°ä½è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œå‡†å¤‡è¿›è¡Œåˆ›æ–°æ€§åˆ†æ"å³å¯ã€‚"""

        return prompt
    
    def _build_methodology_analysis_prompt(self) -> str:
        """æ„å»ºæ–¹æ³•å­¦åˆ›æ–°åˆ†ææç¤ºè¯ï¼ˆæ— éœ€é‡å¤èƒŒæ™¯ä¿¡æ¯ï¼‰"""
        
        prompt = """åŸºäºåˆšæ‰æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡åœ¨**æ–¹æ³•å­¦æ–¹é¢çš„åˆ›æ–°ç‚¹**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦é‡‡ç”¨äº†æ–°çš„ç ”ç©¶æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ï¼Ÿ
   - å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å“ªäº›æ”¹è¿›å’Œä¼˜åŒ–ï¼Ÿ
   - ä¸ç›¸å…³æ–‡çŒ®ä¸­çš„æ–¹æ³•ç›¸æ¯”æœ‰ä½•çªç ´ï¼Ÿ

2. **æŠ€æœ¯è·¯å¾„åˆ›æ–°**ï¼š
   - æŠ€æœ¯å®ç°ä¸Šæœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ
   - æ˜¯å¦è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Ÿ
   - åˆ›æ–°æ–¹æ³•çš„ç§‘å­¦æ€§å’Œå¯é‡å¤æ€§å¦‚ä½•ï¼Ÿ

3. **æ–¹æ³•è®ºè´¡çŒ®**ï¼š
   - ä¸ºç ”ç©¶é¢†åŸŸæä¾›äº†ä»€ä¹ˆæ–°çš„åˆ†æå·¥å…·æˆ–ç ”ç©¶èŒƒå¼ï¼Ÿ
   - æ–¹æ³•çš„æ¨å¹¿åº”ç”¨ä»·å€¼å¦‚ä½•ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼šå®¢è§‚ä¸“ä¸šï¼Œçªå‡ºåˆ›æ–°ç‚¹ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _build_theory_analysis_prompt(self) -> str:
        """æ„å»ºç†è®ºè´¡çŒ®åˆ†ææç¤ºè¯ï¼ˆæ— éœ€é‡å¤èƒŒæ™¯ä¿¡æ¯ï¼‰"""
        
        prompt = """åŸºäºåˆšæ‰æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡åœ¨**ç†è®ºæ–¹é¢çš„è´¡çŒ®**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **ç†è®ºåˆ›æ–°æ€§**ï¼š
   - æ˜¯å¦æå‡ºäº†æ–°çš„ç†è®ºæ¡†æ¶æˆ–æ¦‚å¿µï¼Ÿ
   - å¯¹ç°æœ‰ç†è®ºè¿›è¡Œäº†å“ªäº›æ‰©å±•æˆ–ä¿®æ­£ï¼Ÿ
   - ç†è®ºæ„å»ºçš„é€»è¾‘æ€§å’Œç³»ç»Ÿæ€§å¦‚ä½•ï¼Ÿ

2. **ç†è®ºæ•´åˆæ€§**ï¼š
   - å¦‚ä½•æ•´åˆä¸åŒçš„ç†è®ºè§†è§’ï¼Ÿ
   - æ˜¯å¦å½¢æˆäº†æ–°çš„ç†è®ºç»¼åˆï¼Ÿ
   - è·¨å­¦ç§‘ç†è®ºèåˆçš„åˆ›æ–°ç¨‹åº¦å¦‚ä½•ï¼Ÿ

3. **ç†è®ºå½±å“åŠ›**ï¼š
   - å¯¹å­¦ç§‘ç†è®ºå‘å±•çš„æ„ä¹‰ï¼Ÿ
   - ä¸ºåç»­ç ”ç©¶æä¾›äº†ä»€ä¹ˆç†è®ºåŸºç¡€ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼šä¸¥è°¨å®¢è§‚ï¼Œçªå‡ºç†è®ºåˆ›æ–°ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _build_practice_analysis_prompt(self) -> str:
        """æ„å»ºå®è·µä»·å€¼åˆ†ææç¤ºè¯ï¼ˆæ— éœ€é‡å¤èƒŒæ™¯ä¿¡æ¯ï¼‰"""
        
        prompt = """åŸºäºåˆšæ‰æä¾›çš„è®ºæ–‡ä¿¡æ¯å’Œæ–‡çŒ®èƒŒæ™¯ï¼Œè¯·åˆ†æç›®æ ‡è®ºæ–‡åœ¨**å®è·µåº”ç”¨æ–¹é¢çš„ä»·å€¼**ï¼š

**åˆ†æè¦æ±‚**ï¼š
1. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼š
   - è§£å†³äº†ä»€ä¹ˆé‡è¦çš„å®é™…é—®é¢˜ï¼Ÿ
   - è§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§å¦‚ä½•ï¼Ÿ
   - ç›¸æ¯”ç°æœ‰è§£å†³æ–¹æ¡ˆæœ‰ä½•ä¼˜åŠ¿ï¼Ÿ

2. **åº”ç”¨å‰æ™¯è¯„ä¼°**ï¼š
   - åœ¨ç›¸å…³è¡Œä¸šæˆ–é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Ÿ
   - æ¨å¹¿åº”ç”¨çš„å¯èƒ½æ€§å’ŒèŒƒå›´ï¼Ÿ
   - æŠ€æœ¯æˆç†Ÿåº¦å’Œäº§ä¸šåŒ–å‰æ™¯ï¼Ÿ

3. **ç¤¾ä¼šç»æµä»·å€¼**ï¼š
   - èƒ½åˆ›é€ ä»€ä¹ˆæ ·çš„ç»æµæˆ–ç¤¾ä¼šä»·å€¼ï¼Ÿ
   - å¯¹è¡Œä¸šå‘å±•æˆ–ç¤¾ä¼šè¿›æ­¥çš„è´¡çŒ®ï¼Ÿ

**è¾“å‡ºè¦æ±‚**ï¼šå®ç”¨å®¢è§‚ï¼Œçªå‡ºåº”ç”¨ä»·å€¼ï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _analyze_methodology_innovation_compact(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """ç²¾ç®€ç‰ˆæ–¹æ³•å­¦åˆ›æ–°åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        # æå–å…³é”®ä¿¡æ¯ï¼Œé¿å…å‘é€å®Œæ•´çš„æ–‡çŒ®èƒŒæ™¯
        key_literature_info = self._extract_key_literature_info(literature_context)
        
        prompt = f"""ä½ æ˜¯å­¦æœ¯æ–¹æ³•è®ºä¸“å®¶ï¼Œè¯·åˆ†æè®ºæ–‡çš„æ–¹æ³•å­¦åˆ›æ–°ï¼š

**è®ºæ–‡**ï¼š{thesis_info.get('title', 'æœªæä¾›')}
**æ–¹æ³•**ï¼š{thesis_info.get('methodology', 'æœªæä¾›')}  
**åˆ›æ–°ç‚¹**ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}

**æ–‡çŒ®å‚è€ƒ**ï¼š{key_literature_info}

è¯·ä»ç ”ç©¶æ–¹æ³•åˆ›æ–°æ€§ã€æŠ€æœ¯è·¯å¾„çªç ´ã€æ–¹æ³•è®ºè´¡çŒ®ä¸‰ä¸ªè§’åº¦åˆ†æï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _analyze_theory_contribution_compact(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """ç²¾ç®€ç‰ˆç†è®ºè´¡çŒ®åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        key_literature_info = self._extract_key_literature_info(literature_context)
        
        prompt = f"""ä½ æ˜¯ç†è®ºç ”ç©¶ä¸“å®¶ï¼Œè¯·åˆ†æè®ºæ–‡çš„ç†è®ºè´¡çŒ®ï¼š

**è®ºæ–‡**ï¼š{thesis_info.get('title', 'æœªæä¾›')}
**ç†è®ºæ¡†æ¶**ï¼š{thesis_info.get('theoretical_framework', 'æœªæä¾›')}
**åˆ›æ–°ç‚¹**ï¼š{thesis_info.get('main_innovations', 'æœªæä¾›')}

**æ–‡çŒ®å‚è€ƒ**ï¼š{key_literature_info}

è¯·ä»ç†è®ºåˆ›æ–°æ€§ã€ç†è®ºæ•´åˆæ€§ã€å­¦ç§‘å½±å“åŠ›ä¸‰ä¸ªè§’åº¦åˆ†æï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _analyze_practice_value_compact(self, thesis_info: Dict[str, str], literature_context: str) -> str:
        """ç²¾ç®€ç‰ˆå®è·µä»·å€¼åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        
        key_literature_info = self._extract_key_literature_info(literature_context)
        
        prompt = f"""ä½ æ˜¯äº§å­¦ç ”ä¸“å®¶ï¼Œè¯·åˆ†æè®ºæ–‡çš„å®è·µä»·å€¼ï¼š

**è®ºæ–‡**ï¼š{thesis_info.get('title', 'æœªæä¾›')}
**å®é™…é—®é¢˜**ï¼š{thesis_info.get('practical_problems', 'æœªæä¾›')}
**è§£å†³æ–¹æ¡ˆ**ï¼š{thesis_info.get('proposed_solutions', 'æœªæä¾›')}

**æ–‡çŒ®å‚è€ƒ**ï¼š{key_literature_info}

è¯·ä»é—®é¢˜è§£å†³èƒ½åŠ›ã€åº”ç”¨å‰æ™¯ã€ç¤¾ä¼šç»æµä»·å€¼ä¸‰ä¸ªè§’åº¦åˆ†æï¼Œ300-500å­—ã€‚"""

        return prompt
    
    def _extract_key_literature_info(self, literature_context: str) -> str:
        """æå–å…³é”®æ–‡çŒ®ä¿¡æ¯ï¼ˆå¤§å¹…å‡å°‘Tokenæ¶ˆè€—ï¼‰"""
        
        lines = literature_context.split('\n')
        key_info = []
        
        # åªä¿ç•™å…³é”®ç»Ÿè®¡ä¿¡æ¯
        for line in lines:
            if 'æ–‡çŒ®æ€»æ•°' in line or 'è¿‘5å¹´æ–‡çŒ®å æ¯”' in line or 'å¹´ä»½' in line:
                key_info.append(line)
            elif line.startswith('- ') and ('ç¯‡' in line or '%' in line):
                key_info.append(line)
        
        # é™åˆ¶åœ¨200å­—ç¬¦ä»¥å†…
        result = '\n'.join(key_info[:10])
        if len(result) > 200:
            result = result[:200] + '...'
        
        return result
