#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½å­¦ä½è®ºæ–‡ç›®å½•æŠ½å–å™¨
ä¸“é—¨æ”¯æŒWordæ–‡æ¡£(.docx)æ ¼å¼çš„ç›®å½•æ™ºèƒ½æŠ½å–
ä½¿ç”¨AIæŠ€æœ¯è¯†åˆ«å’Œè§£æå¤æ‚çš„è®ºæ–‡ç›®å½•ç»“æ„
ä»…æ”¯æŒä¸­æ–‡è®ºæ–‡ï¼Œä¸æ”¯æŒè—è¯­æˆ–å…¶ä»–è¯­è¨€è®ºæ–‡
"""

import re
import json
import docx
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import logging
from datetime import datetime
import xml.etree.ElementTree as ET
from docx.oxml.ns import qn

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def is_chinese_text(text: str, min_chinese_ratio: float = 0.3) -> bool:
    """
    æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºä¸­æ–‡
    
    Args:
        text: å¾…æ£€æµ‹æ–‡æœ¬
        min_chinese_ratio: æœ€å°ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹é˜ˆå€¼
    
    Returns:
        bool: æ˜¯å¦ä¸ºä¸­æ–‡æ–‡æœ¬
    """
    if not text or not text.strip():
        return False
    
    # ç§»é™¤ç©ºç™½å­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—
    clean_text = re.sub(r'[\s\d\W]', '', text)
    
    if len(clean_text) == 0:
        return False
    
    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ï¼‰
    chinese_chars = re.findall(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', clean_text)
    chinese_ratio = len(chinese_chars) / len(clean_text)
    
    return chinese_ratio >= min_chinese_ratio

def detect_non_chinese_content(text: str, sample_lines: int = 100) -> Tuple[bool, str]:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å¤§é‡éä¸­æ–‡å†…å®¹ï¼ˆå¦‚è—è¯­ã€è‹±è¯­ç­‰ï¼‰
    
    Args:
        text: å¾…æ£€æµ‹æ–‡æœ¬
        sample_lines: é‡‡æ ·è¡Œæ•°
    
    Returns:
        Tuple[bool, str]: (æ˜¯å¦åŒ…å«å¤§é‡éä¸­æ–‡å†…å®¹, æ£€æµ‹åˆ°çš„è¯­è¨€ç±»å‹)
    """
    lines = text.split('\n')[:sample_lines]
    non_chinese_lines = 0
    tibetan_lines = 0
    total_content_lines = 0
    detected_language = "unknown"
    
    for line in lines:
        line = line.strip()
        if not line or len(line) < 3:  # è·³è¿‡ç©ºè¡Œå’Œè¿‡çŸ­çš„è¡Œ
            continue
            
        total_content_lines += 1
            
        # æ£€æµ‹è—è¯­ï¼ˆè—æ–‡UnicodeèŒƒå›´ï¼‰- æ›´ä¸¥æ ¼çš„æ£€æµ‹
        tibetan_chars = re.findall(r'[\u0f00-\u0fff]', line)
        if len(tibetan_chars) > 2:  # å¦‚æœä¸€è¡Œä¸­æœ‰è¶…è¿‡2ä¸ªè—æ–‡å­—ç¬¦
            tibetan_lines += 1
            non_chinese_lines += 1
            detected_language = "tibetan"
            continue
        
        # æ£€æµ‹è‹±è¯­ï¼ˆå¤§é‡è‹±æ–‡å•è¯ï¼‰
        english_words = re.findall(r'\b[a-zA-Z]{4,}\b', line)
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', line)
        if len(english_words) > 2 and len(chinese_chars) < 3:
            non_chinese_lines += 1
            detected_language = "english"
            continue
        
        # æ£€æµ‹å…¶ä»–è¯­è¨€ï¼ˆå¤§é‡éä¸­æ–‡å­—ç¬¦ï¼‰
        if not is_chinese_text(line, min_chinese_ratio=0.15):
            non_chinese_lines += 1
            if detected_language == "unknown":
                detected_language = "other"
    
    if total_content_lines == 0:
        return False, "unknown"
    
    # ç‰¹åˆ«ä¸¥æ ¼æ£€æµ‹è—æ–‡
    tibetan_ratio = tibetan_lines / total_content_lines
    non_chinese_ratio = non_chinese_lines / total_content_lines
    
    # å¦‚æœè—æ–‡è¡Œæ¯”ä¾‹è¶…è¿‡20%ï¼Œåˆ™è®¤ä¸ºæ˜¯è—æ–‡æ–‡æ¡£
    if tibetan_ratio > 0.2:
        return True, "tibetan"
    
    # å¦‚æœéä¸­æ–‡è¡Œæ¯”ä¾‹è¶…è¿‡40%ï¼Œåˆ™è®¤ä¸ºæ˜¯éä¸­æ–‡æ–‡æ¡£
    return non_chinese_ratio > 0.4, detected_language

@dataclass
class TocEntry:
    """ç›®å½•æ¡ç›®æ•°æ®ç»“æ„"""
    level: int              # ç« èŠ‚å±‚çº§ (1=ä¸»ç« èŠ‚, 2=äºŒçº§ç« èŠ‚, 3=ä¸‰çº§ç« èŠ‚)
    number: str             # ç« èŠ‚ç¼–å· (å¦‚ "1", "1.1", "1.1.1")
    title: str              # ç« èŠ‚æ ‡é¢˜
    page: Optional[int]     # é¡µç 
    line_number: int        # åœ¨åŸæ–‡æ¡£ä¸­çš„è¡Œå·
    confidence: float       # AIè¯†åˆ«ç½®ä¿¡åº¦ (0-1)
    section_type: str       # ç« èŠ‚ç±»å‹ (chapter, section, subsection, etc.)

@dataclass
class ThesisToc:
    """è®ºæ–‡ç›®å½•ç»“æ„"""
    title: str                    # è®ºæ–‡æ ‡é¢˜
    author: str                   # ä½œè€…
    entries: List[TocEntry]       # ç›®å½•æ¡ç›®åˆ—è¡¨
    total_entries: int           # æ€»æ¡ç›®æ•°
    max_level: int               # æœ€å¤§å±‚çº§æ·±åº¦
    extraction_method: str        # æŠ½å–æ–¹æ³•
    confidence_score: float      # æ•´ä½“ç½®ä¿¡åº¦
    toc_content: str             # ç›®å½•åŸå§‹å†…å®¹

class DocumentParser(ABC):
    """æ–‡æ¡£è§£æå™¨æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def parse(self, file_path: str) -> str:
        """è§£ææ–‡æ¡£å†…å®¹"""
        pass
    
    @abstractmethod
    def get_lines(self, file_path: str) -> List[str]:
        """è·å–æ–‡æ¡£è¡Œåˆ—è¡¨"""
        pass
    
    @abstractmethod
    def extract_toc_boundary(self, file_path: str) -> Tuple[str, int, int]:
        """æå–ç›®å½•è¾¹ç•Œå’Œå†…å®¹"""
        pass

class WordParser(DocumentParser):
    """Wordæ–‡æ¡£è§£æå™¨ï¼Œä¸“é—¨å¤„ç†.docxæ ¼å¼"""
    
    def parse(self, file_path: str) -> str:
        """è§£æWordæ–‡æ¡£å†…å®¹"""
        try:
            doc = docx.Document(file_path)
            content = ""
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            return content
        except Exception as e:
            logger.error(f"è§£æWordæ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def extract_toc_from_bookmarks(self, doc_path: str, detected_lang: str = "chinese") -> Dict:
        """é€šè¿‡XMLä¹¦ç­¾æå–å®Œæ•´çš„TOCç»“æ„ï¼Œæ”¯æŒä¸­è‹±æ–‡ä¸åŒæˆªå–ç­–ç•¥"""
        try:
            import re
            document = docx.Document(doc_path)
            document_xml = document._element.xml
            
            # æŸ¥æ‰¾æ‰€æœ‰TOCä¹¦ç­¾å’Œå¯¹åº”çš„æ–‡æœ¬
            bookmark_start_pattern = r'<w:bookmarkStart[^>]*w:name="(_Toc\d+)"[^>]*/>'
            bookmark_starts = re.finditer(bookmark_start_pattern, document_xml)
            
            bookmark_positions = []
            
            # æ”¶é›†æ‰€æœ‰ä¹¦ç­¾ä½ç½®
            for match in bookmark_starts:
                bookmark_name = match.group(1)
                start_pos = match.end()
                bookmark_positions.append((bookmark_name, start_pos))
            
            logger.info(f"æ‰¾åˆ° {len(bookmark_positions)} ä¸ªTOCä¹¦ç­¾ä½ç½®")
            
            if not bookmark_positions:
                return {}
            
            toc_entries = []
            
            # ä¸ºæ¯ä¸ªä¹¦ç­¾æå–åç»­æ–‡æœ¬å†…å®¹
            for i, (bookmark_name, start_pos) in enumerate(bookmark_positions):
                # ç¡®å®šæå–æ–‡æœ¬çš„ç»“æŸä½ç½® - å¢åŠ æå–èŒƒå›´
                if i + 1 < len(bookmark_positions):
                    end_pos = bookmark_positions[i + 1][1]
                else:
                    end_pos = start_pos + 2000  # æœ€åä¸€ä¸ªä¹¦ç­¾ï¼Œæå–æ›´å¤šå†…å®¹
                
                # æå–ä¹¦ç­¾åçš„XMLç‰‡æ®µ
                xml_fragment = document_xml[start_pos:end_pos]
                
                # ä»XMLä¸­æå–æ–‡æœ¬å†…å®¹ - æ”¹è¿›çš„æ–‡æœ¬æå–é€»è¾‘
                text_pattern = r'<w:t[^>]*>(.*?)</w:t>'
                text_matches = re.findall(text_pattern, xml_fragment, re.DOTALL)
                
                if text_matches:
                    # åˆå¹¶æ‰€æœ‰ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œè€Œä¸æ˜¯åªå–ç¬¬ä¸€ä¸ª
                    text_parts = []
                    for text in text_matches:
                        clean_text = text.strip()
                        if clean_text and not clean_text.isspace():
                            text_parts.append(clean_text)
                        
                        # å¦‚æœå·²ç»æ”¶é›†åˆ°è¶³å¤Ÿçš„æ–‡æœ¬å†…å®¹ï¼Œåœæ­¢æ”¶é›†
                        if len(' '.join(text_parts)) > 100:
                            break
                    
                    if text_parts:
                        # åˆå¹¶æ–‡æœ¬ç‰‡æ®µï¼Œå»é™¤é‡å¤å’Œæ— å…³å†…å®¹
                        text_content = ' '.join(text_parts)
                        
                        # æ¸…ç†æ–‡æœ¬å†…å®¹
                        text_content = self._clean_extracted_text(text_content)
                        
                        if text_content:
                            toc_entries.append((bookmark_name, text_content))
            
            # è¿‡æ»¤å‡ºçœŸæ­£çš„ç›®å½•æ¡ç›®ï¼ˆé€šå¸¸ä»"æ‘˜è¦"ã€"ç›®å½•"ã€"ç»ªè®º"ç­‰å¼€å§‹ï¼‰
            start_keywords = ['æ‘˜è¦', 'æ‘˜ è¦', 'ABSTRACT', 'ç›®å½•', 'ç»ªè®º', 'ç¬¬ä¸€ç« ', 'ç¬¬1ç« ', '1.', '1 ']
            start_index = -1
            
            for i, (bookmark, text) in enumerate(toc_entries):
                for keyword in start_keywords:
                    if keyword in text:
                        start_index = i
                        break
                if start_index >= 0:
                    break
            
            if start_index >= 0:
                filtered_toc = toc_entries[start_index:]
                logger.info(f"é€šè¿‡ä¹¦ç­¾æå–åˆ° {len(filtered_toc)} ä¸ªæœ‰æ•ˆç›®å½•é¡¹")
                
                # è¯¦ç»†æ‰“å°æœ‰æ•ˆç›®å½•é¡¹
                print(f"\nğŸ“‹ è¯¦ç»†åˆ†æï¼š{len(filtered_toc)}ä¸ªæœ‰æ•ˆç›®å½•é¡¹å†…å®¹")
                print("=" * 80)
                for i, (bookmark, text) in enumerate(filtered_toc):
                    print(f"[{i+1:2d}] ä¹¦ç­¾: {bookmark} | æ–‡æœ¬: {text}")
                print("=" * 80)
                
                # æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆæ ¹æ®è¯­è¨€ç±»å‹æˆªå–æ–‡æœ¬ï¼‰
                toc_content = ""
                truncated_entries = []
                for bookmark, text in filtered_toc:
                    # æ ¹æ®è¯­è¨€ç±»å‹æˆªå–æ–‡æœ¬
                    truncated_text = self._truncate_text_by_language(text, detected_lang)
                    
                    # æ¨¡å¼åŒ¹é…å¤„ç†ï¼šè§„èŒƒåŒ–ç« èŠ‚æ ‡é¢˜æ ¼å¼
                    truncated_text = self._normalize_chapter_title(truncated_text)
                    
                    toc_content += truncated_text + "\n"
                    truncated_entries.append((bookmark, truncated_text))
                
                # è®°å½•æˆªå–åçš„ç›®å½•å†…å®¹
                lang_desc = "å‰30å­—ç¬¦" if detected_lang == "chinese" else "å‰20ä¸ªå•è¯"
                print(f"\næˆªå–åçš„ç›®å½•å†…å®¹ï¼ˆ{lang_desc}ï¼Œä¼ é€’ç»™AIåˆ†æï¼‰:")
                print("-" * 60)
                for i, (bookmark, truncated_text) in enumerate(truncated_entries):
                    print(f"[{i+1:2d}] {truncated_text}")
                print("-" * 60)
                
                return {
                    'content': toc_content,
                    'entries': filtered_toc,
                    'method': 'xml_bookmarks'
                }
            else:
                logger.warning("æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®å½•èµ·å§‹ç‚¹")
                return {}
                
        except Exception as e:
            logger.error(f"é€šè¿‡ä¹¦ç­¾æå–TOCå¤±è´¥: {e}")
            return {}
    
    def _clean_extracted_text(self, text: str) -> str:
        """æ¸…ç†ä»XMLæå–çš„æ–‡æœ¬å†…å®¹"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ç§»é™¤å¸¸è§çš„æ— å…³å†…å®¹
        unwanted_patterns = [
            r'HYPERLINK.*?"',  # Wordè¶…é“¾æ¥
            r'_Toc\d+',        # TOCå¼•ç”¨
            r'PAGEREF.*?"',    # é¡µé¢å¼•ç”¨
            r'REF.*?"',        # ä¸€èˆ¬å¼•ç”¨
            r'\\"',            # è½¬ä¹‰å¼•å·
        ]
        
        for pattern in unwanted_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼å¹¶è¿”å›
        return re.sub(r'\s+', ' ', text.strip())
    
    def _truncate_text_by_language(self, text: str, detected_lang: str) -> str:
        """æ ¹æ®è¯­è¨€ç±»å‹æˆªå–æ–‡æœ¬ï¼šä¸­æ–‡æˆªå–30å­—ç¬¦ï¼Œè‹±æ–‡æˆªå–20ä¸ªå•è¯"""
        if not text:
            return text
        
        if detected_lang == "chinese":
            # ä¸­æ–‡ï¼šæˆªå–å‰30å­—ç¬¦
            truncated = text[:30].strip()
            if len(text) > 30:
                truncated += "..."
        else:
            # è‹±æ–‡ï¼šæˆªå–å‰20ä¸ªå•è¯
            words = text.split()
            if len(words) <= 20:
                truncated = text.strip()
            else:
                truncated = ' '.join(words[:20]) + "..."
        
        return truncated
    
    def _normalize_chapter_title(self, text: str) -> str:
        """è§„èŒƒåŒ–ç« èŠ‚æ ‡é¢˜æ ¼å¼ï¼Œå»é™¤å¤šä½™ç©ºæ ¼"""
        if not text:
            return text
        
        import re
        
        # ä¸­æ–‡ç« èŠ‚æ ‡é¢˜è§„èŒƒåŒ–ï¼šå»é™¤"ç¬¬"å’Œ"ç« "ä¹‹é—´çš„ç©ºæ ¼
        # "ç¬¬ ä¸€ ç« " -> "ç¬¬ä¸€ç« "
        # "ç¬¬ äºŒ ç« " -> "ç¬¬äºŒç« "
        # "ç¬¬ ä¸‰ ç« " -> "ç¬¬ä¸‰ç« "  
        # "ç¬¬ å›› ç« " -> "ç¬¬å››ç« "
        # "ç¬¬ äº” ç« " -> "ç¬¬äº”ç« "
        # "ç¬¬ å…­ ç« " -> "ç¬¬å…­ç« "
        text = re.sub(r'ç¬¬\s+([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s+ç« ', r'ç¬¬\1ç« ', text)
        
        # é˜¿æ‹‰ä¼¯æ•°å­—ç« èŠ‚æ ‡é¢˜è§„èŒƒåŒ–
        # "ç¬¬ 1 ç« " -> "ç¬¬1ç« "
        # "ç¬¬ 2 ç« " -> "ç¬¬2ç« "
        text = re.sub(r'ç¬¬\s+(\d+)\s+ç« ', r'ç¬¬\1ç« ', text)
        
        return text

    def extract_toc_fields(self, doc_path: str) -> Dict:
        """ä½¿ç”¨XMLè§£ææå–Wordæ–‡æ¡£ä¸­çš„TOCå­—æ®µ"""
        try:
            document = docx.Document(doc_path)
            toc_data = {}
            
            # æœç´¢æ–‡æ¡£ä¸­çš„TOCç›¸å…³å†…å®¹
            for paragraph in document.paragraphs:
                if paragraph._element is not None:
                    # ç®€åŒ–çš„TOCå­—æ®µæœç´¢
                    para_xml = paragraph._element.xml
                    if 'TOC' in para_xml or 'fldChar' in para_xml:
                        text = paragraph.text.strip()
                        if text:
                            logger.info(f"å‘ç°æ½œåœ¨TOCæ®µè½: {text}")
                            if 'content' not in toc_data:
                                toc_data['content'] = ""
                            toc_data['content'] += text + "\n"
                                    
            return toc_data
        except Exception as e:
            logger.error(f"æå–TOCå­—æ®µå¤±è´¥: {e}")
            return {}
    
    def _extract_toc_content_from_field(self, element) -> str:
        """ä»TOCå­—æ®µå…ƒç´ ä¸­æå–å†…å®¹"""
        try:
            toc_content = ""
            # æŸ¥æ‰¾TOCå­—æ®µçš„å†…å®¹éƒ¨åˆ†
            for node in element.iter():
                if hasattr(node, 'text') and node.text:
                    text = node.text.strip()
                    if text and not text.startswith('TOC'):
                        toc_content += text + "\n"
            return toc_content
        except Exception as e:
            logger.error(f"æå–TOCå†…å®¹å¤±è´¥: {e}")
            return ""
    
    def extract_by_styles(self, doc_path: str) -> str:
        """åŸºäºæ ·å¼æå–TOCå†…å®¹"""
        try:
            document = docx.Document(doc_path)
            toc_content = ""
            
            # TOCç›¸å…³æ ·å¼åç§°
            toc_styles = [
                'TOC 1', 'TOC 2', 'TOC 3', 'TOC 4', 'TOC 5',
                'toc 1', 'toc 2', 'toc 3', 'toc 4', 'toc 5',
                'Toc1', 'Toc2', 'Toc3', 'Toc4', 'Toc5',
                'ç›®å½• 1', 'ç›®å½• 2', 'ç›®å½• 3', 'ç›®å½• 4', 'ç›®å½• 5',
                'Heading 1', 'Heading 2', 'Heading 3'  # æœ‰æ—¶TOCä½¿ç”¨æ ‡é¢˜æ ·å¼
            ]
            
            for paragraph in document.paragraphs:
                if paragraph.style and paragraph.style.name in toc_styles:
                    text = paragraph.text.strip()
                    if text:
                        logger.info(f"å‘ç°TOCæ ·å¼æ®µè½: {paragraph.style.name} - {text}")
                        toc_content += text + "\n"
                        
            return toc_content
        except Exception as e:
            logger.error(f"åŸºäºæ ·å¼æå–TOCå¤±è´¥: {e}")
            return ""
    
    def extract_enhanced_toc(self, file_path: str, detected_lang: str = "chinese") -> str:
        """å¢å¼ºçš„TOCæå–æ–¹æ³•ï¼Œä¼˜å…ˆä½¿ç”¨ä¹¦ç­¾æå–ï¼Œæ”¯æŒä¸­è‹±æ–‡ä¸åŒæˆªå–ç­–ç•¥"""
        try:
            # æ–¹æ³•1ï¼šå°è¯•é€šè¿‡XMLä¹¦ç­¾æå–ï¼ˆæœ€å¯é ï¼‰
            bookmark_result = self.extract_toc_from_bookmarks(file_path, detected_lang)
            if bookmark_result and 'content' in bookmark_result and bookmark_result['content'].strip():
                logger.info("ä½¿ç”¨XMLä¹¦ç­¾æå–æˆåŠŸè·å–TOC")
                return bookmark_result['content']
            
            # æ–¹æ³•2ï¼šå°è¯•å­—æ®µæå–
            toc_fields = self.extract_toc_fields(file_path)
            if toc_fields and 'content' in toc_fields and toc_fields['content'].strip():
                logger.info("ä½¿ç”¨å­—æ®µæå–æˆåŠŸè·å–TOC")
                return toc_fields['content']
            
            # æ–¹æ³•3ï¼šå°è¯•æ ·å¼æå–
            style_toc = self.extract_by_styles(file_path)
            if style_toc.strip():
                logger.info("ä½¿ç”¨æ ·å¼æå–æˆåŠŸè·å–TOC")
                return style_toc
            
            # æ–¹æ³•4ï¼šå›é€€åˆ°åŸæœ‰çš„è¾¹ç•Œæå–æ–¹æ³•
            logger.info("ä½¿ç”¨è¾¹ç•Œæå–æ–¹æ³•ä½œä¸ºåå¤‡")
            _, start, end = self.extract_toc_boundary(file_path)
            if start != -1 and end != -1:
                lines = self.get_lines(file_path)
                toc_lines = lines[start:end]
                return "\n".join(toc_lines)
            
            return ""
        except Exception as e:
            logger.error(f"å¢å¼ºTOCæå–å¤±è´¥: {e}")
            return ""
    
    def get_lines(self, file_path: str) -> List[str]:
        """è·å–Wordæ–‡æ¡£è¡Œåˆ—è¡¨"""
        try:
            doc = docx.Document(file_path)
            lines = []
            for paragraph in doc.paragraphs:
                lines.append(paragraph.text)
            return lines
        except Exception as e:
            logger.error(f"è·å–Wordæ–‡æ¡£è¡Œå¤±è´¥: {e}")
            raise
    
    def extract_toc_boundary(self, file_path: str) -> Tuple[str, int, int]:
        """æå–Wordæ–‡æ¡£ä¸­çš„ç›®å½•è¾¹ç•Œ"""
        try:
            doc = docx.Document(file_path)
            lines = []
            for paragraph in doc.paragraphs:
                lines.append(paragraph.text)
            
            # æŸ¥æ‰¾ç›®å½•èµ·å§‹ä½ç½®
            toc_start = -1
            toc_end = -1
            
            # ç›®å½•å¼€å§‹æ ‡è¯†ï¼ˆå¿…é¡»æ˜¯å•ç‹¬çš„è¡Œï¼‰
            toc_indicators = [
                r'^ç›®\s*å½•\s*$',
                r'^CONTENTS?\s*$',
                r'^TABLE\s+OF\s+CONTENTS?\s*$',
                r'^Contents\s*$',
                r'^ç›®\s*æ¬¡\s*$'
            ]
            
            # æ–¹æ³•1ï¼šå¯»æ‰¾æ˜ç¡®çš„ç›®å½•æ ‡é¢˜
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # æŸ¥æ‰¾ç›®å½•å¼€å§‹ - å¿…é¡»æ˜¯ç‹¬ç«‹çš„è¡Œ
                if toc_start == -1:
                    for indicator in toc_indicators:
                        if re.match(indicator, line, re.IGNORECASE):
                            toc_start = i
                            logger.info(f"æ‰¾åˆ°ç›®å½•å¼€å§‹ä½ç½®: ç¬¬{i+1}è¡Œ - {line}")
                            break
                
                # æŸ¥æ‰¾ç›®å½•ç»“æŸï¼ˆåœ¨ç›®å½•å¼€å§‹åï¼‰
                elif toc_start != -1 and toc_end == -1:
                    # è·³è¿‡ç›®å½•æ ‡é¢˜è¡Œï¼Œå¼€å§‹å¯»æ‰¾çœŸæ­£çš„ç›®å½•å†…å®¹
                    if i == toc_start:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„ç›®å½•æ¡ç›®
                    if self._is_toc_entry(line):
                        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›®å½•æ¡ç›®ï¼Œç»§ç»­å‘ä¸‹æ‰¾åˆ°ç›®å½•ç»“æŸ
                        continue
                    
                    # ç›®å½•ç»“æŸçš„æ ‡è¯† - åªæœ‰æ˜ç¡®çš„æ­£æ–‡ç« èŠ‚å¼€å§‹æ‰ç®—ç»“æŸ
                    end_indicators = [
                        r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s+\S+',  # å¿…é¡»æœ‰ç« èŠ‚æ ‡é¢˜çš„æ­£æ–‡ç« èŠ‚
                        r'^\d+\s+[^.\d\s]\S+',  # æ•°å­—å¼€å¤´çš„æ­£æ–‡ç« èŠ‚ï¼Œå¿…é¡»æœ‰æ ‡é¢˜
                        r'^Chapter\s+\d+\s+\S+',  # è‹±æ–‡æ­£æ–‡ç« èŠ‚å¿…é¡»æœ‰æ ‡é¢˜
                        r'^å¼•\s*è¨€\s*$',
                        r'^ç»ª\s*è®º\s*$',
                        r'^Introduction\s*$'
                    ]
                    
                    # åªæœ‰åŒ¹é…åˆ°æ˜ç¡®çš„æ­£æ–‡å¼€å§‹æ‰ç»“æŸç›®å½•æœç´¢
                    for indicator in end_indicators:
                        if re.match(indicator, line, re.IGNORECASE):
                            # ç¡®è®¤ä¸æ˜¯ç›®å½•æ¡ç›®
                            if not self._is_toc_entry(line):
                                toc_end = i
                                logger.info(f"æ‰¾åˆ°ç›®å½•ç»“æŸä½ç½®: ç¬¬{i+1}è¡Œ - {line}")
                                break
                    
                    if toc_end != -1:
                        break
                    
                    # å¦‚æœè·ç¦»ç›®å½•å¼€å§‹è¿‡è¿œä¸”è¿ç»­å¤šè¡Œéç›®å½•å†…å®¹ï¼Œè®¤ä¸ºç›®å½•ç»“æŸ
                    if i - toc_start > 100:  # å¢åŠ åˆ°100è¡Œ
                        non_toc_count = 0
                        # æ£€æŸ¥åç»­10è¡Œæ˜¯å¦éƒ½ä¸æ˜¯ç›®å½•æ¡ç›®
                        for j in range(i, min(i+10, len(lines))):
                            if not self._is_toc_entry(lines[j].strip()):
                                non_toc_count += 1
                        
                        if non_toc_count >= 8:  # å¦‚æœ10è¡Œä¸­æœ‰8è¡Œéƒ½ä¸æ˜¯ç›®å½•æ¡ç›®
                            toc_end = i
                            logger.info(f"è·ç¦»ç›®å½•å¼€å§‹è¿‡è¿œä¸”è¿ç»­éç›®å½•å†…å®¹ï¼Œè®¾å®šç»“æŸä½ç½®: ç¬¬{i+1}è¡Œ")
                            break
            
            # æ–¹æ³•2ï¼šå¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®çš„ç›®å½•æ ‡é¢˜ï¼Œå°è¯•å¯»æ‰¾æ‘˜è¦åçš„ç›®å½•åŒºåŸŸ
            if toc_start == -1:
                logger.info("æœªæ‰¾åˆ°æ˜ç¡®ç›®å½•æ ‡é¢˜ï¼Œå°è¯•å¯»æ‰¾æ‘˜è¦åçš„ç›®å½•åŒºåŸŸ")
                abstract_end = -1
                
                # å¯»æ‰¾æ‘˜è¦ç»“æŸä½ç½®
                for i, line in enumerate(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ£€æµ‹æ‘˜è¦ã€ABSTRACTç»“æŸ
                    if re.match(r'^(æ‘˜\s*è¦|Abstract|ABSTRACT)\s*$', line, re.IGNORECASE):
                        # å‘åæœç´¢æ‰¾åˆ°æ‘˜è¦å†…å®¹ç»“æŸ
                        for j in range(i+1, min(i+50, len(lines))):
                            next_line = lines[j].strip()
                            if not next_line:
                                continue
                            
                            # å¦‚æœé‡åˆ°ç›®å½•å†…å®¹ç‰¹å¾ï¼Œè¯´æ˜æ‘˜è¦ç»“æŸäº†
                            if (re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', next_line) or
                                re.match(r'^\d+\.?\d*\s+.+\d+$', next_line) or
                                re.match(r'^ç›®\s*å½•', next_line)):
                                abstract_end = j - 1
                                break
                        
                        if abstract_end != -1:
                            logger.info(f"æ‰¾åˆ°æ‘˜è¦ç»“æŸä½ç½®: ç¬¬{abstract_end+1}è¡Œ")
                            break
                
                # å¦‚æœæ‰¾åˆ°æ‘˜è¦ç»“æŸä½ç½®ï¼Œä»è¯¥ä½ç½®å¼€å§‹å¯»æ‰¾ç›®å½•
                if abstract_end != -1:
                    # ä»æ‘˜è¦ç»“æŸåå¼€å§‹å¯»æ‰¾ç›®å½•å†…å®¹
                    potential_toc_start = abstract_end + 1
                    
                    # å¯»æ‰¾è¿ç»­çš„ç›®å½•æ¡ç›®
                    consecutive_toc_entries = 0
                    for i in range(potential_toc_start, min(potential_toc_start + 100, len(lines))):
                        line = lines[i].strip()
                        if not line:
                            continue
                        
                        if self._is_toc_entry(line) or re.match(r'^ç›®\s*å½•\s*$', line):
                            consecutive_toc_entries += 1
                            if consecutive_toc_entries >= 3:  # æ‰¾åˆ°è‡³å°‘3ä¸ªè¿ç»­çš„ç›®å½•æ¡ç›®
                                toc_start = max(0, i - consecutive_toc_entries)
                                logger.info(f"é€šè¿‡æ‘˜è¦åæ£€æµ‹æ‰¾åˆ°ç›®å½•å¼€å§‹ä½ç½®: ç¬¬{toc_start+1}è¡Œ")
                                break
                        else:
                            consecutive_toc_entries = 0
            
            if toc_start == -1:
                logger.warning("æœªæ‰¾åˆ°ç›®å½•å¼€å§‹ä½ç½®ï¼Œå°è¯•å…¨æ–‡æœç´¢ç›®å½•å†…å®¹")
                return self._fallback_toc_extraction(lines)
            
            # å¯»æ‰¾ç›®å½•ç»“æŸä½ç½®
            if toc_end == -1:
                # å¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®ç»“æŸï¼Œå‘åæœç´¢æ›´å¤šå†…å®¹
                for i in range(toc_start + 1, min(toc_start + 150, len(lines))):  # å¢åŠ æœç´¢èŒƒå›´åˆ°150è¡Œ
                    line = lines[i].strip()
                    if line and self._is_toc_entry(line):
                        continue
                    elif line and not self._is_toc_entry(line):
                        # æ‰¾åˆ°éç›®å½•å†…å®¹ï¼Œéœ€è¦ä»”ç»†åˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯æ­£æ–‡å¼€å§‹
                        end_indicators = [
                            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s*ç»ª\s*è®º',  # ç¬¬ä¸€ç« ç»ªè®º
                            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s+\S+',  # æ­£æ–‡ç« èŠ‚
                            r'^\d+\s*\.\s*\d+\s*[é€‰é¢˜èƒŒæ™¯|ç ”ç©¶æ„ä¹‰|å›½å†…å¤–]',  # 1.1é€‰é¢˜èƒŒæ™¯åŠç ”ç©¶æ„ä¹‰
                            r'^\d+\s+[^.\d\s]\S+',
                            r'^Chapter\s+\d+\s+\S+',
                            r'^å¼•\s*è¨€\s*$',
                            r'^ç»ª\s*è®º\s*$',
                            r'^Introduction\s*$'
                        ]
                        
                        # åªæœ‰æ˜ç¡®åŒ¹é…åˆ°æ­£æ–‡å¼€å§‹æ ‡å¿—æ‰åœæ­¢
                        for indicator in end_indicators:
                            if re.match(indicator, line, re.IGNORECASE):
                                toc_end = i
                                logger.info(f"é€šè¿‡å†…å®¹åŒ¹é…æ‰¾åˆ°ç›®å½•ç»“æŸä½ç½®: ç¬¬{i+1}è¡Œ - {line}")
                                break
                        
                        if toc_end != -1:
                            break
                
                if toc_end == -1:
                    toc_end = min(toc_start + 150, len(lines))  # å‡è®¾ç›®å½•ä¸è¶…è¿‡150è¡Œ
                    logger.info(f"ä½¿ç”¨å¯å‘å¼æ–¹æ³•ç¡®å®šç›®å½•ç»“æŸä½ç½®: ç¬¬{toc_end}è¡Œ")
            
            # æå–ç›®å½•å†…å®¹
            if lines[toc_start].strip() and re.match(r'^ç›®\s*å½•\s*$', lines[toc_start].strip()):
                # å¦‚æœtoc_startæ˜¯"ç›®å½•"æ ‡é¢˜è¡Œï¼Œè·³è¿‡å®ƒ
                toc_lines = lines[toc_start + 1:toc_end]
            else:
                # å¦‚æœtoc_startå°±æ˜¯ç›®å½•å†…å®¹å¼€å§‹ï¼Œä¸è·³è¿‡
                toc_lines = lines[toc_start:toc_end]
            
            toc_content = "\n".join(toc_lines)
            
            logger.info(f"ç›®å½•è¾¹ç•Œç¡®å®š: ç¬¬{toc_start+1}è¡Œåˆ°ç¬¬{toc_end}è¡Œï¼Œå…±{len(toc_lines)}è¡Œ")
            
            return toc_content, toc_start, toc_end
            
            if toc_end == -1:
                # å¦‚æœæ²¡æ‰¾åˆ°æ˜ç¡®ç»“æŸï¼Œå‘åæœç´¢æ›´å¤šå†…å®¹
                for i in range(toc_start + 1, min(toc_start + 150, len(lines))):  # å¢åŠ æœç´¢èŒƒå›´åˆ°150è¡Œ
                    line = lines[i].strip()
                    if line and self._is_toc_entry(line):
                        continue
                    elif line and not self._is_toc_entry(line):
                        # æ‰¾åˆ°éç›®å½•å†…å®¹ï¼Œä½†éœ€è¦æ›´ä»”ç»†åˆ¤æ–­æ˜¯å¦çœŸçš„æ˜¯æ­£æ–‡å¼€å§‹
                        end_indicators = [
                            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s+\S+',  # æ­£æ–‡ç« èŠ‚
                            r'^\d+\s+[^.\d\s]\S+',
                            r'^Chapter\s+\d+\s+\S+',
                            r'^æ‘˜\s*è¦\s*$',
                            r'^Abstract\s*$',
                            r'^å¼•\s*è¨€\s*$',
                            r'^ç»ª\s*è®º\s*$'
                        ]
                        
                        # åªæœ‰æ˜ç¡®åŒ¹é…åˆ°æ­£æ–‡å¼€å§‹æ ‡å¿—æ‰åœæ­¢
                        for indicator in end_indicators:
                            if re.match(indicator, line, re.IGNORECASE):
                                toc_end = i
                                logger.info(f"é€šè¿‡å†…å®¹åŒ¹é…æ‰¾åˆ°ç›®å½•ç»“æŸä½ç½®: ç¬¬{i+1}è¡Œ - {line}")
                                break
                        
                        if toc_end != -1:
                            break
                
                if toc_end == -1:
                    toc_end = min(toc_start + 150, len(lines))  # å‡è®¾ç›®å½•ä¸è¶…è¿‡150è¡Œ
                    logger.info(f"ä½¿ç”¨å¯å‘å¼æ–¹æ³•ç¡®å®šç›®å½•ç»“æŸä½ç½®: ç¬¬{toc_end}è¡Œ")
            
            # æå–ç›®å½•å†…å®¹
            toc_lines = lines[toc_start + 1:toc_end]  # è·³è¿‡"ç›®å½•"æ ‡é¢˜è¡Œ
            toc_content = "\n".join(toc_lines)
            
            logger.info(f"ç›®å½•è¾¹ç•Œç¡®å®š: ç¬¬{toc_start+2}è¡Œåˆ°ç¬¬{toc_end}è¡Œï¼Œå…±{len(toc_lines)}è¡Œ")
            
            return toc_content, toc_start + 1, toc_end
            
        except Exception as e:
            logger.error(f"æå–Wordæ–‡æ¡£ç›®å½•è¾¹ç•Œå¤±è´¥: {e}")
            raise
    
    def _is_toc_entry(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç›®å½•æ¡ç›®"""
        if not line.strip():
            return False
        
        line = line.strip()
        
        # ç›®å½•æ¡ç›®ç‰¹å¾ - ä¼˜åŒ–åŒ¹é…æ¡ä»¶ä»¥åŒ…å«æ›´å¤šç±»å‹
        toc_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .+\d+$',  # ç¬¬Xç« ...é¡µç 
            r'^\d+\.?\d*\s+.+\d+$',  # 1.1 æ ‡é¢˜...é¡µç 
            r'^[A-Z][a-z]+\s+\d+.+\d+$',  # Chapter 1...é¡µç 
            r'^æ‘˜\s*è¦.+[IVX\d]+$',  # æ‘˜è¦...é¡µç 
            r'^Abstract.+[IVX\d]+$',  # Abstract...é¡µç 
            r'^ç›®\s*å½•.+[IVX\d]+$',  # ç›®å½•...é¡µç 
            r'^å‚\s*è€ƒ\s*æ–‡\s*çŒ®.+\d+$',  # å‚è€ƒæ–‡çŒ®...é¡µç 
            r'^References.+\d+$',  # References...é¡µç 
            r'^æ”»è¯».+å­¦ä½.+æœŸé—´.+æˆæœ.+\d+$',  # æ”»è¯»å­¦ä½æœŸé—´æˆæœ...é¡µç 
            r'^ç ”ç©¶æˆæœ.+\d+$',  # ç ”ç©¶æˆæœ...é¡µç 
            r'^å­¦æœ¯æˆæœ.+\d+$',  # å­¦æœ¯æˆæœ...é¡µç 
            r'^Publications.+\d+$',  # Publications...é¡µç 
            r'^è‡´\s*è°¢.+\d+$',  # è‡´è°¢...é¡µç 
            r'^Acknowledgment.+\d+$',  # Acknowledgment...é¡µç 
            r'^ä½œè€…ç®€ä»‹.+\d+$',  # ä½œè€…ç®€ä»‹...é¡µç 
            r'^ä¸ªäººç®€å†.+\d+$',  # ä¸ªäººç®€å†...é¡µç 
            r'^å\s*è®°.+\d+$',  # åè®°...é¡µç 
            r'^ç»“\s*è¯­.+\d+$',  # ç»“è¯­...é¡µç 
            r'^Epilogue.+\d+$',  # Epilogue...é¡µç 
            r'^Author.+Profile.+\d+$',  # Author Profile...é¡µç 
            r'^Biography.+\d+$',  # Biography...é¡µç 
            r'^é™„\s*å½•.+\d+$',  # é™„å½•...é¡µç 
            r'^Appendix.+\d+$',  # Appendix...é¡µç 
            r'^å£°\s*æ˜.+\d+$',  # å£°æ˜...é¡µç 
            r'^Declaration.+\d+$',  # Declaration...é¡µç 
            # ç‰¹æ®Šæ ¼å¼ï¼šæ ‡é¢˜å’Œé¡µç ä¹‹é—´ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”
            r'^.+\t+[IVX\d]+$',  # æ ‡é¢˜TABé¡µç 
            # é€šç”¨æ ¼å¼ï¼šæ ‡é¢˜åé¢è·Ÿé¡µç 
            r'^[^\d]+\s+[IVX\d]+$',  # éæ•°å­—å¼€å¤´çš„æ ‡é¢˜ é¡µç 
        ]
        
        for pattern in toc_patterns:
            if re.match(pattern, line):
                return True
        
        # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœåŒ…å«æ˜æ˜¾çš„ç›®å½•å…³é”®è¯å’Œé¡µç 
        toc_keywords = [
            'æ”»è¯»', 'å­¦ä½', 'æœŸé—´', 'æˆæœ', 'ç ”ç©¶æˆæœ', 'å­¦æœ¯æˆæœ',
            'è‡´è°¢', 'ä½œè€…ç®€ä»‹', 'ä¸ªäººç®€å†', 'å£°æ˜', 'ç‰ˆæƒ', 'åè®°', 'ç»“è¯­',
            'Publications', 'Acknowledgment', 'Biography', 'CV', 'Declaration', 'Epilogue'
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ä¸”ä»¥æ•°å­—ç»“å°¾ï¼ˆé¡µç ï¼‰
        if any(keyword in line for keyword in toc_keywords) and re.search(r'[IVX\d]+$', line):
            return True
        
        # æ£€æŸ¥ç« èŠ‚ç¼–å·æ¨¡å¼ï¼ˆç‰¹åˆ«é’ˆå¯¹1.1, 2.1è¿™ç§æ ¼å¼ï¼‰
        if re.match(r'^\d+\.\d+', line) and not re.search(r'[ã€‚.].{10,}', line):
            return True
            
        # æ£€æŸ¥ç¬¬Xç« æ¨¡å¼
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', line):
            return True

        return False
    
    def _extract_toc_from_content(self, content: str) -> List[str]:
        """
        ä»æ­£æ–‡å†…å®¹ä¸­æ™ºèƒ½æå–ç›®å½•ç»“æ„
        é€‚ç”¨äºç›®å½•åŸŸæ— æ³•è¯»å–çš„æƒ…å†µ
        """
        lines = content.split('\n')
        toc_lines = []
        
        # æ ‡å‡†ç›®å½•æ¨¡å¼
        chapter_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« )\s*(.+?)(?:\s+(\d+))?$',  # ç¬¬Xç«  æ ‡é¢˜ é¡µç 
            r'^(\d+\.)\s*(.+?)(?:\s+(\d+))?$',                           # 1. æ ‡é¢˜ é¡µç 
            r'^(\d+\.\d+)\s*(.+?)(?:\s+(\d+))?$',                       # 1.1 æ ‡é¢˜ é¡µç 
            r'^(\d+\.\d+\.\d+)\s*(.+?)(?:\s+(\d+))?$',                  # 1.1.1 æ ‡é¢˜ é¡µç 
            r'^([A-Z]+)\s*(.+?)(?:\s+(\d+))?$',                         # ABSTRACT æ ‡é¢˜ é¡µç 
            r'^(æ‘˜\s*è¦|ç›®\s*å½•|å‚è€ƒæ–‡çŒ®|è‡´\s*è°¢|æ”»è¯»|é™„\s*å½•)\s*(.*)(?:\s+(\d+))?$'  # ç‰¹æ®Šç« èŠ‚
        ]
        
        for line in lines:
            line = line.strip()
            if not line or len(line) < 2:
                continue
                
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›®å½•æ¨¡å¼
            for pattern in chapter_patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    # ç¡®ä¿è¿™æ˜¯ç›®å½•è¡Œè€Œä¸æ˜¯æ­£æ–‡æ ‡é¢˜
                    if self._is_likely_toc_line(line):
                        toc_lines.append(line)
                        break
        
        logger.info(f"ä»æ­£æ–‡å†…å®¹ä¸­æå–åˆ° {len(toc_lines)} è¡Œå¯èƒ½çš„ç›®å½•å†…å®¹")
        return toc_lines
    
    def _is_likely_toc_line(self, line: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯ç›®å½•è¡Œ
        """
        line = line.strip()
        
        # æ’é™¤æ˜æ˜¾ä¸æ˜¯ç›®å½•çš„è¡Œ
        exclude_patterns = [
            r'å›¾\s*\d+',  # å›¾X-X
            r'è¡¨\s*\d+',  # è¡¨X-X
            r'å…¬å¼\s*\d+',  # å…¬å¼X-X
            r'ç®—æ³•\s*\d+',  # ç®—æ³•X-X
            r'æœ¬ç« å°ç»“',   # ç« èŠ‚ç»“å°¾
            r'å¦‚å›¾\s*\d+',  # å¦‚å›¾Xæ‰€ç¤º
            r'è§è¡¨\s*\d+',  # è§è¡¨X
            r'^æ ¹æ®',     # æ ¹æ®...
            r'^åŸºäº',     # åŸºäº...
            r'^é€šè¿‡',     # é€šè¿‡...
            r'^å¦‚æœ',     # å¦‚æœ...
            r'^å› æ­¤',     # å› æ­¤...
            r'^ç”±äº',     # ç”±äº...
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, line):
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„ç›®å½•ç‰¹å¾
        toc_indicators = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',  # ç« èŠ‚
            r'^\d+\.\s*[^\d]',              # æ•°å­—ç¼–å·å¼€å¤´
            r'^\d+\.\d+\s*[^\d]',           # äºŒçº§ç¼–å·
            r'æ‘˜\s*è¦|ç›®\s*å½•|å‚è€ƒæ–‡çŒ®|è‡´\s*è°¢|æ”»è¯»|é™„\s*å½•|ç»ª\s*è®º|æ€»ç»“|å±•æœ›|ç»“è®º',  # ç‰¹æ®Šç« èŠ‚
            r'ABSTRACT|CONCLUSION|REFERENCE',  # è‹±æ–‡ç« èŠ‚
        ]
        
        for pattern in toc_indicators:
            if re.search(pattern, line, re.IGNORECASE):
                return True
        
        return False
    
    def _fallback_toc_extraction(self, lines: List[str]) -> Tuple[str, int, int]:
        """å¤‡ç”¨ç›®å½•æå–æ–¹æ³•"""
        logger.info("ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æœç´¢ç›®å½•å†…å®¹")
        
        # å¯»æ‰¾åŒ…å«ç›®å½•æ¡ç›®çš„åŒºåŸŸ
        toc_lines = []
        toc_start = -1
        toc_end = -1
        
        for i, line in enumerate(lines):
            if self._is_toc_entry(line.strip()):
                if toc_start == -1:
                    toc_start = i
                toc_lines.append(line)
                toc_end = i + 1
        
        if toc_lines:
            toc_content = "\n".join(toc_lines)
            logger.info(f"å¤‡ç”¨æ–¹æ³•æ‰¾åˆ° {len(toc_lines)} è¡Œç›®å½•å†…å®¹")
            return toc_content, toc_start, toc_end
        
        return "", 0, 0

class AITocExtractor:
    """AIæ™ºèƒ½ç›®å½•æŠ½å–å™¨ - ä¸“é—¨å¤„ç†.docxæ ¼å¼è®ºæ–‡"""
    
    def __init__(self):
        self.setup_patterns()
        self.parsers = {
            '.docx': WordParser()
        }
        self.ai_client = None  # å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
    
    def setup_patterns(self):
        """è®¾ç½®AIè¯†åˆ«æ¨¡å¼"""
        
        # ä¸»ç« èŠ‚æ¨¡å¼ (é«˜ç½®ä¿¡åº¦)
        self.main_chapter_patterns = [
            {
                'pattern': r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« )\s*([^\n\r\t\d]*)',
                'confidence': 0.95,
                'level': 1,
                'type': 'traditional_chapter'
            },
            {
                'pattern': r'^(\d+)\s+([^\n\r\t\d].{3,})',
                'confidence': 0.90,
                'level': 1,
                'type': 'numeric_chapter'
            },
            {
                'pattern': r'^(Chapter\s+\d+)\s+([^\n\r\t]+)',
                'confidence': 0.90,
                'level': 1,
                'type': 'english_chapter'
            },
            {
                'pattern': r'^(CHAPTER\s+\d+)\s+([^\n\r\t]+)',
                'confidence': 0.90,
                'level': 1,
                'type': 'english_chapter_caps'
            }
        ]
        
        # å­ç« èŠ‚æ¨¡å¼ (ä¸­ç­‰ç½®ä¿¡åº¦)
        self.subsection_patterns = [
            {
                'pattern': r'^(\d+\.\d+)\s+([^\n\r\t\d].{2,})',
                'confidence': 0.85,
                'level': 2,
                'type': 'level2_section'
            },
            {
                'pattern': r'^(\d+\.\d+\.\d+)\s+([^\n\r\t\d].{2,})',
                'confidence': 0.80,
                'level': 3,
                'type': 'level3_section'
            },
            {
                'pattern': r'^(\d+\.\d+\.\d+\.\d+)\s+([^\n\r\t\d].{2,})',
                'confidence': 0.75,
                'level': 4,
                'type': 'level4_section'
            }
        ]
        
        # ç‰¹æ®Šç« èŠ‚æ¨¡å¼ (ä¸“é—¨å¤„ç†ç‰¹æ®Šæ ¼å¼)
        self.special_patterns = [
            {
                'pattern': r'^(æ‘˜\s*è¦|Abstract|ABSTRACT)',
                'confidence': 0.95,
                'level': 0,
                'type': 'abstract'
            },
            {
                'pattern': r'^(ç»“\s*è®º|Conclusion|æ€»ç»“|Summary)',
                'confidence': 0.90,
                'level': 1,
                'type': 'conclusion'
            },
            {
                'pattern': r'^(å‚\s*è€ƒ\s*æ–‡\s*çŒ®|References|REFERENCES)',
                'confidence': 0.95,
                'level': 1,
                'type': 'references'
            },
            {
                'pattern': r'^(æ”»è¯».*å­¦ä½.*æœŸé—´.*æˆæœ|ç ”ç©¶æˆæœ|å­¦æœ¯æˆæœ|Publications|Academic Achievements)',
                'confidence': 0.90,
                'level': 1,
                'type': 'achievements'
            },
            {
                'pattern': r'^(è‡´\s*è°¢|Acknowledgment|Acknowledgement|ACKNOWLEDGMENTS?)',
                'confidence': 0.90,
                'level': 1,
                'type': 'acknowledgment'
            },
            {
                'pattern': r'^(ä½œè€…ç®€ä»‹|ä¸ªäººç®€å†|Author.*Profile|Biography|CV)',
                'confidence': 0.90,
                'level': 1,
                'type': 'author_profile'
            },
            {
                'pattern': r'^(é™„\s*å½•|Appendix|APPENDIX)',
                'confidence': 0.85,
                'level': 1,
                'type': 'appendix'
            },
            {
                'pattern': r'^(å£°\s*æ˜|Declaration|Statement)',
                'confidence': 0.85,
                'level': 1,
                'type': 'declaration'
            },
            {
                'pattern': r'^(ç‰ˆæƒå£°æ˜|Copyright|License)',
                'confidence': 0.85,
                'level': 1,
                'type': 'copyright'
            }
        ]
        
        # è®ºæ–‡ä¿¡æ¯æŠ½å–æ¨¡å¼
        self.meta_patterns = {
            'title': [
                r'è®ºæ–‡é¢˜ç›®[ï¼š:]\s*([^\n\r]+)',
                r'æ ‡é¢˜[ï¼š:]\s*([^\n\r]+)',
                r'Title[ï¼š:]\s*([^\n\r]+)',
            ],
            'author': [
                r'ä½œè€…å§“å[ï¼š:]\s*([^\n\r]+)',
                r'ä½œè€…[ï¼š:]\s*([^\n\r]+)',
                r'Author[ï¼š:]\s*([^\n\r]+)',
                r'å§“å[ï¼š:]\s*([^\n\r]+)',
            ]
        }
    
    def init_ai_client(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
        if self.ai_client is None:
            try:
                # å°è¯•å¯¼å…¥å’Œåˆå§‹åŒ–AIå®¢æˆ·ç«¯
                import sys
                import os
                
                # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
                project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                if project_root not in sys.path:
                    sys.path.append(project_root)
                
                # å°è¯•å¯¼å…¥AIå®¢æˆ·ç«¯
                try:
                    from .ai_client import get_ai_client
                    self.ai_client = get_ai_client()
                    logger.info("AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                except ImportError as e:
                    # åˆ›å»ºä¸€ä¸ªç®€å•çš„Mock AIå®¢æˆ·ç«¯
                    class MockAIClientImpl:
                        def send_message(self, prompt):
                            # åˆ›å»ºæ¨¡æ‹Ÿçš„AIResponseå¯¹è±¡
                            class MockAIResponse:
                                def __init__(self, content):
                                    self.content = content
                                    self.metadata = {}
                                    self.session_id = 'mock'
                                    self.timestamp = 0.0
                                    self.model_type = 'mock'
                            
                            return MockAIResponse('{"entries": []}')
                    
                    self.ai_client = MockAIClientImpl()
                    logger.warning(f"AIå®¢æˆ·ç«¯æ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨Mockå®¢æˆ·ç«¯: {e}")
                    
            except Exception as e:
                logger.warning(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                # åˆ›å»ºMockå®¢æˆ·ç«¯ä½œä¸ºfallback
                class MockAIClientFallback:
                    def send_message(self, prompt):
                        # åˆ›å»ºæ¨¡æ‹Ÿçš„AIResponseå¯¹è±¡
                        class MockAIResponse:
                            def __init__(self, content):
                                self.content = content
                                self.metadata = {}
                                self.session_id = 'mock'
                                self.timestamp = 0.0
                                self.model_type = 'mock'
                        
                        return MockAIResponse('{"entries": []}')
                
                self.ai_client = MockAIClientFallback()
    
    def extract_toc(self, file_path: str) -> ThesisToc:
        """æ™ºèƒ½æŠ½å–è®ºæ–‡ç›®å½•"""
        file_path_obj = Path(file_path)
        
        if not file_path_obj.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è§£æå™¨
        suffix = file_path_obj.suffix.lower()
        if suffix not in self.parsers:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
        
        parser = self.parsers[suffix]
        
        logger.info(f"å¼€å§‹æŠ½å–ç›®å½•: {file_path_obj.name}")
        
        # è§£ææ–‡æ¡£å†…å®¹
        content = parser.parse(file_path)
        
        # æ£€æµ‹æ–‡æ¡£è¯­è¨€ç±»å‹ï¼ˆä»…æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼‰
        is_non_chinese, detected_lang = detect_non_chinese_content(content)
        if is_non_chinese:
            if detected_lang == "english":
                logger.info(f"âœ… æ£€æµ‹é€šè¿‡ï¼šæ–‡æ¡£ä¸ºè‹±æ–‡è®ºæ–‡")
            else:
                logger.warning(f"æ£€æµ‹åˆ°ä¸æ”¯æŒçš„è¯­è¨€ç±»å‹: {detected_lang}")
                raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€ç±»å‹: {detected_lang}ã€‚æ­¤å·¥å…·ä»…æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡è®ºæ–‡ã€‚")
        else:
            # è¿›ä¸€æ­¥éªŒè¯ä¸­æ–‡å†…å®¹
            if not is_chinese_text(content[:5000], min_chinese_ratio=0.2):
                logger.warning("æ–‡æ¡£ä¸­ä¸­æ–‡å†…å®¹è¿‡å°‘ï¼Œå¯èƒ½ä¸æ˜¯ä¸­æ–‡è®ºæ–‡")
                # å°è¯•æ£€æµ‹æ˜¯å¦ä¸ºè‹±æ–‡
                english_ratio = len([w for w in content[:5000].split() if w.isascii()]) / max(len(content[:5000].split()), 1)
                if english_ratio > 0.5:
                    detected_lang = "english"
                    logger.info("âœ… æ£€æµ‹é€šè¿‡ï¼šæ–‡æ¡£ä¸ºè‹±æ–‡è®ºæ–‡")
                else:
                    detected_lang = "unknown"
                    raise ValueError("æ— æ³•ç¡®å®šæ–‡æ¡£è¯­è¨€ç±»å‹ã€‚æ­¤å·¥å…·ä»…æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡è®ºæ–‡ã€‚")
            else:
                logger.info("âœ… æ£€æµ‹é€šè¿‡ï¼šæ–‡æ¡£ä¸ºä¸­æ–‡è®ºæ–‡")
                detected_lang = "chinese"
        
        # æå–ç›®å½•è¾¹ç•Œå’Œå†…å®¹
        if hasattr(parser, 'extract_enhanced_toc'):
            # ä½¿ç”¨å¢å¼ºçš„TOCæå–æ–¹æ³•
            toc_content = parser.extract_enhanced_toc(file_path, detected_lang)
            toc_start = 0
            toc_end = len(toc_content.split('\n')) if toc_content else 0
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„è¾¹ç•Œæå–æ–¹æ³•
            toc_content, toc_start, toc_end = parser.extract_toc_boundary(file_path)

        if not toc_content:
            logger.warning("æœªæ‰¾åˆ°ç›®å½•å†…å®¹ï¼Œå°è¯•ä»æ­£æ–‡å†…å®¹ä¸­æ™ºèƒ½æå–ç›®å½•")
            # å°è¯•ä»æ­£æ–‡å†…å®¹ä¸­æå–ç›®å½•ç»“æ„
            if hasattr(parser, '_extract_toc_from_content'):
                extracted_toc_lines = parser._extract_toc_from_content(content)
                if extracted_toc_lines:
                    toc_content = '\n'.join(extracted_toc_lines)
                    logger.info(f"ä»æ­£æ–‡å†…å®¹ä¸­æˆåŠŸæå– {len(extracted_toc_lines)} è¡Œç›®å½•å†…å®¹")
                else:
                    logger.warning("ä»æ­£æ–‡å†…å®¹ä¸­ä¹Ÿæœªèƒ½æå–åˆ°ç›®å½•ï¼Œä½¿ç”¨å…¨æ–‡åˆ†æ")
                    toc_content = content
            else:
                logger.warning("è§£æå™¨ä¸æ”¯æŒå†…å®¹æå–ï¼Œä½¿ç”¨å…¨æ–‡åˆ†æ")
                toc_content = content
        
        # æŠ½å–è®ºæ–‡å…ƒä¿¡æ¯
        title, author = self._extract_meta_info(content)
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.init_ai_client()
        
        # AIæ™ºèƒ½è¯†åˆ«ç›®å½•æ¡ç›®
        if self.ai_client and hasattr(self.ai_client, 'send_message'):
            entries = self._ai_extract_entries_with_llm(toc_content)
        else:
            entries = self._ai_extract_entries_traditional(toc_content.split('\n'))
        
        # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        confidence_score = self._calculate_overall_confidence(entries)
        
        # æ„å»ºç›®å½•ç»“æ„
        toc = ThesisToc(
            title=title,
            author=author,
            entries=entries,
            total_entries=len(entries),
            max_level=max([e.level for e in entries]) if entries else 0,
            extraction_method="AI_Smart_Extraction_with_LLM" if (self.ai_client and hasattr(self.ai_client, 'send_message')) else "Traditional_Pattern_Matching",
            confidence_score=confidence_score,
            toc_content=toc_content
        )
        
        logger.info(f"ç›®å½•æŠ½å–å®Œæˆ: {len(entries)} ä¸ªæ¡ç›®, ç½®ä¿¡åº¦: {confidence_score:.2f}")
        
        return toc
    
    def _normalize_chapter_title(self, text: str) -> str:
        """è§„èŒƒåŒ–ç« èŠ‚æ ‡é¢˜æ ¼å¼ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼Œæ”¯æŒä¸­è‹±æ–‡"""
        if not text:
            return text
        
        import re
        
        # ä¸­æ–‡ç« èŠ‚æ ‡é¢˜è§„èŒƒåŒ–ï¼šå»é™¤"ç¬¬"å’Œ"ç« "ä¹‹é—´çš„ç©ºæ ¼
        # "ç¬¬ ä¸€ ç« " -> "ç¬¬ä¸€ç« "
        # "ç¬¬ äºŒ ç« " -> "ç¬¬äºŒç« "
        # "ç¬¬ ä¸‰ ç« " -> "ç¬¬ä¸‰ç« "  
        # "ç¬¬ å›› ç« " -> "ç¬¬å››ç« "
        # "ç¬¬ äº” ç« " -> "ç¬¬äº”ç« "
        # "ç¬¬ å…­ ç« " -> "ç¬¬å…­ç« "
        text = re.sub(r'ç¬¬\s+([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s+ç« ', r'ç¬¬\1ç« ', text)
        
        # é˜¿æ‹‰ä¼¯æ•°å­—ç« èŠ‚æ ‡é¢˜è§„èŒƒåŒ–
        # "ç¬¬ 1 ç« " -> "ç¬¬1ç« "
        # "ç¬¬ 2 ç« " -> "ç¬¬2ç« "
        text = re.sub(r'ç¬¬\s+(\d+)\s+ç« ', r'ç¬¬\1ç« ', text)
        
        # è‹±æ–‡ç« èŠ‚æ ‡é¢˜è§„èŒƒåŒ–ï¼šå»é™¤Chapterå’Œæ•°å­—ä¹‹é—´çš„å¤šä½™ç©ºæ ¼
        # "Chapter  1" -> "Chapter 1"
        # "CHAPTER   2" -> "CHAPTER 2"
        text = re.sub(r'(Chapter|CHAPTER)\s+(\d+)', r'\1 \2', text, flags=re.IGNORECASE)
        
        return text
    
    def _ai_extract_entries_with_llm(self, toc_content: str) -> List[TocEntry]:
        """ä½¿ç”¨LLM AIæ™ºèƒ½æŠ½å–ç›®å½•æ¡ç›®ï¼Œç„¶åç¨‹åºè¿‡æ»¤level=1æ¡ç›®"""
        
        if not self.ai_client or not hasattr(self.ai_client, 'send_message'):
            logger.warning("AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return self._ai_extract_entries_traditional(toc_content.split('\n'))
        
        prompt = f"""
ğŸ¯ è¯·ä»”ç»†åˆ†æä»¥ä¸‹ç›®å½•å†…å®¹ï¼Œæå–æ‰€æœ‰ç« èŠ‚å’Œæ¡ç›®ä¿¡æ¯ã€‚ç‰¹åˆ«æ³¨æ„ä¸»ç« èŠ‚çš„è¯†åˆ«ï¼

âš ï¸ ã€å…³é”®è¦æ±‚ - å¿…é¡»éµå®ˆã€‘ï¼š
1. ã€è¯†åˆ«æ‰€æœ‰ä¸»ç« èŠ‚æ ¼å¼ã€‘ï¼š
   **ä¸­æ–‡æ ¼å¼ï¼š**
   - "ç¬¬ä¸€ç« "ã€"ç¬¬äºŒç« "ã€"ç¬¬ä¸‰ç« " 
   - "ç¬¬ ä¸€ ç« "ã€"ç¬¬ äºŒ ç« "ã€"ç¬¬ ä¸‰ ç« "ï¼ˆæ³¨æ„ï¼šå¸¦ç©ºæ ¼çš„ä¹Ÿæ˜¯ä¸»ç« èŠ‚ï¼ï¼‰
   - "ç¬¬ å›› ç« "ã€"ç¬¬ äº” ç« "ã€"ç¬¬ å…­ ç« "ï¼ˆè¿™äº›éƒ½å¿…é¡»è¯†åˆ«ä¸ºlevel=1ï¼ï¼‰
   
   **è‹±æ–‡æ ¼å¼ï¼š**
   - "Chapter 1"ã€"Chapter 2"ã€"Chapter 3"
   - "CHAPTER 1"ã€"CHAPTER 2"ã€"CHAPTER 3"
   - "Chapter  1"ï¼ˆå¤šç©ºæ ¼ä¹Ÿè¦æ­£ç¡®è¯†åˆ«ï¼‰

   **ç‰¹æ®Šä¸»ç« èŠ‚ï¼ˆå¿…é¡»è¯†åˆ«ä¸ºlevel=1ï¼‰ï¼š**
   - "ç»“è®º"ã€"ç»“ è®º"ï¼ˆç»“è®ºç« èŠ‚ï¼Œlevel=1ï¼‰
   - "ä½œè€…ç®€ä»‹"ã€"ä½œè€… ç®€ä»‹"ï¼ˆä½œè€…ç®€ä»‹ç« èŠ‚ï¼Œlevel=1ï¼‰
   - "å‚è€ƒæ–‡çŒ®"ã€"è‡´è°¢"ã€"æ‘˜è¦"ç­‰ï¼ˆå‡ä¸ºlevel=1ï¼‰
   - "æ”»è¯»å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ"ç­‰å­¦æœ¯æˆæœç« èŠ‚ï¼ˆlevel=1ï¼‰

2. ã€å®Œæ•´æ€§æ£€æŸ¥ã€‘ï¼šç¡®ä¿ALLä¸»ç« èŠ‚éƒ½è¢«æå–ï¼ŒåŒ…æ‹¬ç»“è®ºå’Œä½œè€…ç®€ä»‹
3. ã€æ ‡é¢˜æ¸…ç†ã€‘ï¼šå¯¹äºç‰¹æ®Šç« èŠ‚ï¼Œåªä¿ç•™æ ¸å¿ƒæ ‡é¢˜

ğŸ”‘ è¯†åˆ«è§„åˆ™ï¼š
1. ä»»ä½•å½¢å¦‚"ç¬¬Xç« "ã€"Chapter X"ç­‰æ ¼å¼çš„éƒ½æ˜¯level=1ä¸»ç« èŠ‚
2. æ•°å­—æ ¼å¼å¦‚"X.Y"çš„éƒ½æ˜¯level=2æˆ–æ›´ä½å±‚çº§çš„å­ç« èŠ‚
3. åœ¨æ–‡æœ¬ä¸­çœ‹åˆ°ä¸»ç« èŠ‚æ ‡é¢˜è¡Œæ—¶ï¼Œä¼˜å…ˆè¯†åˆ«å®ƒè€Œä¸æ˜¯ä¸‹é¢çš„å­ç« èŠ‚

ç›®å½•å†…å®¹:
{toc_content}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›æ‰€æœ‰ç›®å½•æ¡ç›®ï¼š
{{
    "entries": [
        {{
            "level": 1,
            "number": "ç¬¬ä¸€ç« ",
            "title": "ç»ªè®º", 
            "page": 1,
            "confidence": 0.95,
            "section_type": "chapter"
        }},
        {{
            "level": 2,
            "number": "1.1",
            "title": "ç ”ç©¶èƒŒæ™¯",
            "page": 2,
            "confidence": 0.90,
            "section_type": "section"
        }},
        {{
            "level": 1,
            "number": "",
            "title": "å‚è€ƒæ–‡çŒ®",
            "page": 89,
            "confidence": 0.95,
            "section_type": "references"
        }}
    ]
}}

å±‚çº§åˆ¤æ–­è§„åˆ™ï¼š
- level=1: ä¸»ç« èŠ‚ï¼ˆç¬¬Xç« ã€Chapter Xã€X.æ ¼å¼ã€ç»“è®ºã€å‚è€ƒæ–‡çŒ®ã€è‡´è°¢ã€ä½œè€…ç®€ä»‹ç­‰æ‰€æœ‰ç‹¬ç«‹ç« èŠ‚ï¼‰
- level=2: äºŒçº§ç« èŠ‚ï¼ˆX.Xæ ¼å¼ï¼‰  
- level=3: ä¸‰çº§ç« èŠ‚ï¼ˆX.X.Xæ ¼å¼ï¼‰
- level=4: å››çº§ç« èŠ‚ï¼ˆX.X.X.Xæ ¼å¼ï¼‰

ğŸ”´ ã€é‡è¦ã€‘ç‰¹æ®Šç« èŠ‚å¿…é¡»æ ‡è®°ä¸ºlevel=1ï¼š
- "ç»“è®º"ã€"ç»“ è®º" â†’ level=1, section_type="conclusion"
- "ä½œè€…ç®€ä»‹"ã€"ä½œè€… ç®€ä»‹" â†’ level=1, section_type="author_profile"
- "å‚è€ƒæ–‡çŒ®" â†’ level=1, section_type="references"
- "è‡´è°¢" â†’ level=1, section_type="acknowledgment"
- "æ‘˜è¦" â†’ level=1, section_type="abstract"
- "æ”»è¯»å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ" â†’ level=1, section_type="achievements"

ç« èŠ‚ç±»å‹ï¼ˆsection_typeï¼‰ï¼š
- chapter: æ­£æ–‡ç« èŠ‚ï¼ˆç¬¬ä¸€ç« ã€ç¬¬äºŒç« ã€Chapter 1ã€Chapter 2ã€X.æ ¼å¼ç­‰ï¼‰
- abstract: æ‘˜è¦
- conclusion: ç»“è®º
- references: å‚è€ƒæ–‡çŒ®
- acknowledgment: è‡´è°¢
- achievements: å­¦æœ¯æˆæœ/æ”»è¯»å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ
- author_profile: ä½œè€…ç®€ä»‹
- appendix: é™„å½•

æ ‡é¢˜æ¸…ç†è§„åˆ™ï¼š
- å¯¹äºæ­£æ–‡ç« èŠ‚ï¼šä¿ç•™"ç¬¬Xç« "å’Œä¸»è¦æ ‡é¢˜
- å¯¹äºç‰¹æ®Šç« èŠ‚ï¼šåªä¿ç•™æ ¸å¿ƒåç§°
  * "æ‘˜è¦" ä¸è¦åŒ…å«å…·ä½“å†…å®¹
  * "å‚è€ƒæ–‡çŒ®" ä¸è¦åŒ…å«å…·ä½“æ–‡çŒ®å†…å®¹
  * "è‡´è°¢" ä¸è¦åŒ…å«è‡´è°¢æ­£æ–‡
  * "æ”»è¯»å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ" å¯ç®€åŒ–ä¸ºè¯¥æ ‡é¢˜
  * "ä½œè€…ç®€ä»‹" ä¸è¦åŒ…å«ä¸ªäººä¿¡æ¯

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰è¯†åˆ«åˆ°çš„ç›®å½•æ¡ç›®ï¼Œç‰¹åˆ«æ˜¯æ‰€æœ‰ä¸»ç« èŠ‚ã€‚
"""
        
        try:
            response = self.ai_client.send_message(prompt)
            
            # è·å–å“åº”å†…å®¹ï¼ˆç°åœ¨æ‰€æœ‰å®¢æˆ·ç«¯éƒ½è¿”å›ç±»ä¼¼AIResponseçš„å¯¹è±¡ï¼‰
            response_text = str(response.content)
            
            # è®°å½•AIçš„åŸå§‹å“åº”
            print(f"\nAI LLMåŸå§‹å“åº”:")
            print("-" * 60)
            print(response_text)
            print("-" * 60)
            
            # å°è¯•è§£æJSONå“åº”
            import json
            result_data = json.loads(response_text)
            
            all_entries = []
            for i, entry_data in enumerate(result_data.get('entries', [])):
                entry = TocEntry(
                    level=entry_data.get('level', 1),
                    number=entry_data.get('number', ''),
                    title=entry_data.get('title', ''),
                    page=entry_data.get('page'),
                    line_number=i + 1,
                    confidence=entry_data.get('confidence', 0.8),
                    section_type=entry_data.get('section_type', 'unknown')
                )
                all_entries.append(entry)
            
            # è¯¦ç»†åˆ†æAI LLMçš„æ‰€æœ‰æå–ç»“æœ
            print(f"\nAI LLMæå–æ‰€æœ‰ç»“æœï¼š{len(all_entries)}ä¸ªæ¡ç›®")
            print("=" * 80)
            for i, entry in enumerate(all_entries):
                print(f"[{i+1:2d}] å±‚çº§:{entry.level} | ç¼–å·:{entry.number} | æ ‡é¢˜:{entry.title} | é¡µç :{entry.page} | ç±»å‹:{entry.section_type}")
            print("=" * 80)
            
            # ç¨‹åºé€»è¾‘è¿‡æ»¤ï¼šåªä¿ç•™level=1çš„æ¡ç›®
            level1_entries = [entry for entry in all_entries if entry.level == 1]
            
            # æ¸…ç†ç‰¹æ®Šç« èŠ‚æ ‡é¢˜
            level1_entries = self._clean_special_section_titles(level1_entries)
            
            print(f"\nï¿½ ç¨‹åºè¿‡æ»¤ç»“æœï¼šä»{len(all_entries)}ä¸ªæ¡ç›®ä¸­ç­›é€‰å‡º{len(level1_entries)}ä¸ªlevel=1æ¡ç›®")
            print("=" * 80)
            for i, entry in enumerate(level1_entries):
                print(f"[{i+1:2d}] ç¼–å·:{entry.number} | æ ‡é¢˜:{entry.title} | é¡µç :{entry.page} | ç±»å‹:{entry.section_type}")
            print("=" * 80)
            
            # å¯¹æ¯”åˆ†æç»Ÿè®¡
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡åˆ†æ:")
            print(f"   åŸå§‹ä¹¦ç­¾æå–: 62ä¸ªæœ‰æ•ˆç›®å½•é¡¹")
            print(f"   AI LLMå®Œæ•´æå–: {len(all_entries)}ä¸ªç›®å½•æ¡ç›®")
            print(f"   ç¨‹åºè¿‡æ»¤level=1: {len(level1_entries)}ä¸ªä¸»ç« èŠ‚")
            print(f"   æœ€ç»ˆä¿ç•™æ¯”ä¾‹: {len(level1_entries)/len(all_entries)*100:.1f}%")
            
            logger.info(f"AI LLMæå–äº† {len(all_entries)} ä¸ªç›®å½•æ¡ç›®ï¼Œè¿‡æ»¤åä¿ç•™ {len(level1_entries)} ä¸ªlevel=1æ¡ç›®")
            return level1_entries
            
        except Exception as e:
            logger.error(f"AI LLMæå–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•: {e}")
            return self._ai_extract_entries_traditional(toc_content.split('\n'))
    
    def _clean_special_section_titles(self, entries: List[TocEntry]) -> List[TocEntry]:
        """æ¸…ç†ç‰¹æ®Šç« èŠ‚çš„æ ‡é¢˜ï¼Œå»é™¤å¤šä½™çš„æ­£æ–‡å†…å®¹"""
        for entry in entries:
            if entry.section_type == 'references':
                # å‚è€ƒæ–‡çŒ®ï¼šåªä¿ç•™"å‚è€ƒæ–‡çŒ®"
                entry.title = "å‚è€ƒæ–‡çŒ®"
            elif entry.section_type == 'acknowledgment':
                # è‡´è°¢ï¼šåªä¿ç•™"è‡´è°¢"
                entry.title = "è‡´è°¢"
            elif entry.section_type == 'achievements':
                # å­¦æœ¯æˆæœï¼šæ¸…ç†ä¸ºç®€æ´æ ‡é¢˜
                if 'æ”»è¯»' in entry.title and 'å­¦ä½' in entry.title:
                    entry.title = "æ”»è¯»å­¦ä½æœŸé—´å–å¾—çš„ç ”ç©¶æˆæœ"
                elif 'ç ”ç©¶æˆæœ' in entry.title:
                    entry.title = "ç ”ç©¶æˆæœ"
                elif 'å­¦æœ¯æˆæœ' in entry.title:
                    entry.title = "å­¦æœ¯æˆæœ"
            elif entry.section_type == 'author_profile':
                # ä½œè€…ç®€ä»‹ï¼šåªä¿ç•™"ä½œè€…ç®€ä»‹"
                if 'ä½œè€…ç®€ä»‹' in entry.title:
                    entry.title = "ä½œè€…ç®€ä»‹"
                elif 'ä¸ªäººç®€å†' in entry.title:
                    entry.title = "ä¸ªäººç®€å†"
        
        return entries
    
    def _extract_meta_info(self, content: str) -> Tuple[str, str]:
        """æŠ½å–è®ºæ–‡å…ƒä¿¡æ¯"""
        title = ""
        author = ""
        
        # æŠ½å–æ ‡é¢˜
        for pattern in self.meta_patterns['title']:
            match = re.search(pattern, content, re.MULTILINE | re.IGNORECASE)
            if match:
                title = match.group(1).strip()
                break
        
        # æŠ½å–ä½œè€…
        for pattern in self.meta_patterns['author']:
            match = re.search(pattern, content, re.MULTILINE | re.IGNORECASE)
            if match:
                author = match.group(1).strip()
                break
        
        return title, author
    
    def _ai_extract_entries_traditional(self, lines: List[str]) -> List[TocEntry]:
        """ä¼ ç»Ÿæ–¹æ³•AIæ™ºèƒ½æŠ½å–ç›®å½•æ¡ç›®"""
        entries = []
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # å°è¯•åŒ¹é…å„ç§æ¨¡å¼
            entry = self._match_patterns(line, line_num)
            if entry:
                entries.append(entry)
        
        # AIåå¤„ç†: éªŒè¯å’Œä¿®æ­£
        entries = self._ai_post_process(entries)
        
        return entries
    
    def _match_patterns(self, line: str, line_num: int) -> Optional[TocEntry]:
        """æ¨¡å¼åŒ¹é…"""
        
        # ä¼˜å…ˆåŒ¹é…ç‰¹æ®Šç« èŠ‚
        for pattern_info in self.special_patterns:
            match = re.search(pattern_info['pattern'], line, re.IGNORECASE)
            if match:
                return TocEntry(
                    level=pattern_info['level'],
                    number="",
                    title=match.group(1).strip(),
                    page=self._extract_page_number(line),
                    line_number=line_num + 1,
                    confidence=pattern_info['confidence'],
                    section_type=pattern_info['type']
                )
        
        # åŒ¹é…ä¸»ç« èŠ‚
        for pattern_info in self.main_chapter_patterns:
            match = re.match(pattern_info['pattern'], line)
            if match:
                number = match.group(1)
                title = match.group(2).strip() if len(match.groups()) > 1 else ""
                return TocEntry(
                    level=pattern_info['level'],
                    number=number,
                    title=title,
                    page=self._extract_page_number(line),
                    line_number=line_num + 1,
                    confidence=pattern_info['confidence'],
                    section_type=pattern_info['type']
                )
        
        # åŒ¹é…å­ç« èŠ‚
        for pattern_info in self.subsection_patterns:
            match = re.match(pattern_info['pattern'], line)
            if match:
                number = match.group(1)
                title = match.group(2).strip() if len(match.groups()) > 1 else ""
                return TocEntry(
                    level=pattern_info['level'],
                    number=number,
                    title=title,
                    page=self._extract_page_number(line),
                    line_number=line_num + 1,
                    confidence=pattern_info['confidence'],
                    section_type=pattern_info['type']
                )
        
        return None
    
    def _extract_page_number(self, line: str) -> Optional[int]:
        """ä»è¡Œä¸­æŠ½å–é¡µç """
        # å¯»æ‰¾è¡Œæœ«çš„æ•°å­—
        page_match = re.search(r'\.+\s*(\d+)\s*$', line)  # ç‚¹å·åè·Ÿæ•°å­—
        if page_match:
            return int(page_match.group(1))
        
        # å¯»æ‰¾åˆ¶è¡¨ç¬¦åçš„æ•°å­—
        page_match = re.search(r'\t+(\d+)\s*$', line)
        if page_match:
            return int(page_match.group(1))
        
        # å¯»æ‰¾ç©ºæ ¼åçš„æ•°å­—
        page_match = re.search(r'\s+(\d+)\s*$', line)
        if page_match:
            return int(page_match.group(1))
        
        return None
    
    def _ai_post_process(self, entries: List[TocEntry]) -> List[TocEntry]:
        """AIåå¤„ç†: æ™ºèƒ½éªŒè¯å’Œä¿®æ­£"""
        
        # 1. å»é™¤é‡å¤æ¡ç›®
        entries = self._remove_duplicates(entries)
        
        # 2. ä¿®æ­£ç« èŠ‚å±‚çº§
        entries = self._fix_levels(entries)
        
        # 3. æ™ºèƒ½æ ‡é¢˜è¡¥å…¨
        entries = self._complete_titles(entries)
        
        # 4. æŒ‰ç« èŠ‚é¡ºåºæ’åº
        entries = self._sort_entries(entries)
        
        return entries
    
    def _remove_duplicates(self, entries: List[TocEntry]) -> List[TocEntry]:
        """å»é™¤é‡å¤æ¡ç›®"""
        seen = set()
        unique_entries = []
        
        for entry in entries:
            key = (entry.number, entry.title)
            if key not in seen:
                seen.add(key)
                unique_entries.append(entry)
        
        return unique_entries
    
    def _fix_levels(self, entries: List[TocEntry]) -> List[TocEntry]:
        """ä¿®æ­£ç« èŠ‚å±‚çº§"""
        for entry in entries:
            if entry.number:
                # æ ¹æ®ç¼–å·ç‚¹çš„æ•°é‡ç¡®å®šå±‚çº§
                dot_count = entry.number.count('.')
                if dot_count == 0:  # å¦‚ "1" æˆ– "ç¬¬ä¸€ç« "
                    entry.level = 1
                elif dot_count == 1:  # å¦‚ "1.1"
                    entry.level = 2
                elif dot_count == 2:  # å¦‚ "1.1.1"
                    entry.level = 3
                elif dot_count == 3:  # å¦‚ "1.1.1.1"
                    entry.level = 4
        
        return entries
    
    def _complete_titles(self, entries: List[TocEntry]) -> List[TocEntry]:
        """æ™ºèƒ½æ ‡é¢˜è¡¥å…¨"""
        for entry in entries:
            if not entry.title and entry.number:
                # ä¸ºç¼ºå¤±æ ‡é¢˜çš„æ¡ç›®ç”Ÿæˆé»˜è®¤æ ‡é¢˜
                if entry.level == 1:
                    entry.title = f"ç¬¬{entry.number}ç« "
                else:
                    entry.title = f"ç¬¬{entry.number}èŠ‚"
        
        return entries
    
    def _sort_entries(self, entries: List[TocEntry]) -> List[TocEntry]:
        """æŒ‰ç« èŠ‚é¡ºåºæ’åº"""
        def sort_key(entry: TocEntry) -> tuple:
            if not entry.number:
                return (999, 0, 0, 0)  # ç‰¹æ®Šç« èŠ‚æ’åœ¨æœ€å
            
            # è§£æç¼–å·
            parts = entry.number.replace('ç¬¬', '').replace('ç« ', '').split('.')
            numbers = []
            for part in parts:
                try:
                    # å¤„ç†ä¸­æ–‡æ•°å­—
                    if part in ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å']:
                        cn_nums = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 
                                  'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10}
                        numbers.append(cn_nums.get(part, 0))
                    else:
                        numbers.append(int(part))
                except ValueError:
                    numbers.append(0)
            
            # å¡«å……åˆ°4ä½æ•°å­—
            while len(numbers) < 4:
                numbers.append(0)
            
            return tuple(numbers[:4])
        
        return sorted(entries, key=sort_key)
    
    def _calculate_overall_confidence(self, entries: List[TocEntry]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦"""
        if not entries:
            return 0.0
        
        total_confidence = sum(entry.confidence for entry in entries)
        return total_confidence / len(entries)
    
    def save_toc_json(self, toc: ThesisToc, output_path: str):
        """ä¿å­˜ç›®å½•ä¸ºç»“æ„åŒ–JSON"""
        output_path_obj = Path(output_path)
        
        # æ„å»ºç»“æ„åŒ–JSON
        toc_json = {
            "metadata": {
                "title": toc.title,
                "author": toc.author,
                "total_entries": toc.total_entries,
                "max_level": toc.max_level,
                "extraction_method": toc.extraction_method,
                "confidence_score": toc.confidence_score,
                "extracted_at": datetime.now().isoformat()
            },
            "toc_structure": {
                "chapters": [],
                "special_sections": [],
                "post_references": []  # å‚è€ƒæ–‡çŒ®åçš„ç« èŠ‚
            },
            "raw_entries": []
        }
        
        # ç»„ç»‡ç« èŠ‚ç»“æ„
        current_chapter = None
        references_found = False  # æ ‡è®°æ˜¯å¦å·²ç»æ‰¾åˆ°å‚è€ƒæ–‡çŒ®
        
        for entry in toc.entries:
            # æ·»åŠ åŸå§‹æ¡ç›®
            toc_json["raw_entries"].append({
                "level": entry.level,
                "number": entry.number,
                "title": entry.title,
                "page": entry.page,
                "line_number": entry.line_number,
                "confidence": entry.confidence,
                "section_type": entry.section_type
            })
            
            # æ£€æŸ¥æ˜¯å¦åˆ°äº†å‚è€ƒæ–‡çŒ®
            if entry.section_type == 'references':
                references_found = True
            
            # ç»„ç»‡å±‚æ¬¡ç»“æ„
            if entry.level == 1 and entry.section_type in ['traditional_chapter', 'numeric_chapter', 'english_chapter', 'chapter']:
                current_chapter = {
                    "number": entry.number,
                    "title": entry.title,
                    "page": entry.page,
                    "sections": []
                }
                toc_json["toc_structure"]["chapters"].append(current_chapter)
            elif entry.level > 1 and current_chapter and not references_found:
                current_chapter["sections"].append({
                    "level": entry.level,
                    "number": entry.number,
                    "title": entry.title,
                    "page": entry.page
                })
            elif entry.section_type in ['abstract', 'conclusion', 'references']:
                toc_json["toc_structure"]["special_sections"].append({
                    "type": entry.section_type,
                    "title": entry.title,
                    "page": entry.page
                })
            elif references_found and entry.section_type in ['achievements', 'acknowledgment', 'author_profile', 'appendix', 'declaration', 'copyright']:
                # å‚è€ƒæ–‡çŒ®åçš„ç« èŠ‚
                toc_json["toc_structure"]["post_references"].append({
                    "type": entry.section_type,
                    "title": entry.title,
                    "page": entry.page
                })
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(output_path_obj, 'w', encoding='utf-8') as f:
            json.dump(toc_json, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»“æ„åŒ–JSONå·²ä¿å­˜åˆ°: {output_path_obj}")
        
        return toc_json

def test_with_word_documents():
    """æµ‹è¯•Wordæ–‡æ¡£"""
    extractor = AITocExtractor()
    
    # æµ‹è¯•data/inputæ–‡ä»¶å¤¹ä¸­çš„Wordæ–‡æ¡£
    input_dir = Path("data/input")
    if not input_dir.exists():
        logger.error(f"æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    word_files = list(input_dir.glob("*.docx"))
    if not word_files:
        logger.warning(f"åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°Wordæ–‡æ¡£")
        return
    
    for word_file in word_files:
        logger.info(f"æµ‹è¯•Wordæ–‡æ¡£: {word_file.name}")
        
        try:
            # æŠ½å–ç›®å½•
            toc = extractor.extract_toc(str(word_file))
            
            # ä¿å­˜ç»“æ„åŒ–JSON
            output_file = f"{word_file.stem}_toc_structured.json"
            toc_json = extractor.save_toc_json(toc, output_file)
            
            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\n{word_file.name} æŠ½å–ç»“æœ:")
            print(f"   æ ‡é¢˜: {toc.title}")
            print(f"   ä½œè€…: {toc.author}")
            print(f"   æ€»æ¡ç›®: {toc.total_entries}")
            print(f"   ç½®ä¿¡åº¦: {toc.confidence_score:.2f}")
            print(f"   æ–¹æ³•: {toc.extraction_method}")
            print(f"   è¾“å‡º: {output_file}")
            
        except Exception as e:
            logger.error(f"å¤„ç† {word_file.name} å¤±è´¥: {e}")

if __name__ == "__main__":
    test_with_word_documents()
