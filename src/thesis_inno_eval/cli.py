#!/usr/bin/env python3
"""
è®ºæ–‡è¯„ä»·ç³»ç»Ÿå‘½ä»¤è¡Œæ¥å£
æ”¯æŒå¹¶è¡Œå¤„ç†å¤šç¯‡è®ºæ–‡
"""

import click
from pathlib import Path
import json
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# é¦–å…ˆåˆå§‹åŒ–æ—¥å¿—é…ç½®
from .logging_config import setup_logging
setup_logging()

import logging
logger = logging.getLogger(__name__)

from .config_manager import get_config_manager

def validate_file_format(file_path):
    """éªŒè¯æ–‡ä»¶æ ¼å¼æ˜¯å¦å—æ”¯æŒ"""
    config_mgr = get_config_manager()
    supported_formats = config_mgr.get_supported_formats()
    file_ext = Path(file_path).suffix.lower()
    
    if file_ext not in supported_formats:
        if file_ext == '.pdf':
            raise click.BadParameter(f"æš‚ä¸æ”¯æŒPDFæ ¼å¼æ–‡ä»¶ã€‚è¯·ä½¿ç”¨Wordæ–‡æ¡£(.docx)æ ¼å¼ã€‚")
        else:
            raise click.BadParameter(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: {', '.join(supported_formats)}")
    
    return True

def _detect_cached_search_results(base_name, output_dir):
    """æ£€æµ‹å¹¶åŠ è½½ç¼“å­˜çš„æ–‡çŒ®æ£€ç´¢ç»“æœ"""
    cached_results = {
        'papers_by_lang': {},
        'thesis_extracted_info': None,
        'literature_metadata_analysis': None
    }
    
    # æ£€æµ‹æ–‡çŒ®æ£€ç´¢ç»“æœæ–‡ä»¶
    languages = ['Chinese', 'English']  # æ”¯æŒçš„è¯­è¨€
    papers_found = False
    
    for lang in languages:
        # æ£€æŸ¥TOPç›¸å…³æ–‡çŒ®æ–‡ä»¶
        top_papers_file = output_dir / f"{base_name}_TOP30PAPERS_{lang}.json"
        if not top_papers_file.exists():
            top_papers_file = output_dir / f"{base_name}_TOP20PAPERS_{lang}.json"
        if not top_papers_file.exists():
            top_papers_file = output_dir / f"{base_name}_TOP50PAPERS_{lang}.json"
        
        if top_papers_file.exists():
            try:
                with open(top_papers_file, 'r', encoding='utf-8') as f:
                    papers_data = json.load(f)
                    cached_results['papers_by_lang'][lang] = papers_data
                    papers_found = True
            except Exception as e:
                click.echo(f"âš ï¸ è¯»å–ç¼“å­˜æ–‡çŒ®æ–‡ä»¶å¤±è´¥ {top_papers_file}: {e}")
    
    return cached_results if papers_found else None

@click.group()
def cli():
    """è®ºæ–‡åˆ›æ–°åº¦è¯„ä»·ç³»ç»Ÿ - ä½¿ç”¨AIå’Œæ–‡çŒ®æ•°æ®åº“åˆ†æè®ºæ–‡åˆ›æ–°æ€§"""
    pass

@cli.command()
def info():
    """æ˜¾ç¤ºç³»ç»Ÿé…ç½®ä¿¡æ¯"""
    try:
        config_mgr = get_config_manager()
        
        # è·å–æ¨¡å‹é…ç½®
        openai_config = config_mgr.get_ai_model_config('openai')
        gemini_config = config_mgr.get_ai_model_config('gemini')
        
        click.echo("ğŸ“‹ ç³»ç»Ÿé…ç½®ä¿¡æ¯:")
        click.echo(f"  ğŸ¤– OpenAIæ¨¡å‹: {openai_config.get('model_name', 'N/A')}")
        click.echo(f"  ğŸ¤– OpenAI API Base: {openai_config.get('api_base', 'N/A')}")
        click.echo(f"  ğŸ¤– OpenAI max_tokens: {openai_config.get('max_tokens'):,}")
        click.echo(f"  ğŸ¤– Geminiæ¨¡å‹: {gemini_config.get('model_name', 'N/A')}")
        click.echo(f"  ğŸ¤– Gemini max_tokens: {gemini_config.get('max_tokens'):,}")
        
        # æ£€æŸ¥å¹¶å‘å¤„ç†é…ç½®
        click.echo(f"  âš¡ æå–å¹¶å‘æ•°: æœ€å¤§4ä¸ª")
        click.echo(f"  âš¡ è¯„ä¼°å¹¶å‘æ•°: æœ€å¤§3ä¸ª")
        
        # æ£€æŸ¥æ–‡æ¡£ç›¸å…³ä¿¡æ¯
        max_tokens = openai_config.get('max_tokens', 0)
        estimated_chars = max_tokens * 3  # å¤§è‡´ä¼°ç®—
        click.echo(f"  ğŸ“„ æœ€å¤§å¤„ç†å­—ç¬¦æ•°: ~{estimated_chars:,} å­—ç¬¦")
        
        # æ£€æµ‹å½“å‰APIç±»å‹
        from .ai_client import get_ai_client
        try:
            ai_client = get_ai_client()
            if hasattr(ai_client, 'connection_pool'):
                api_type = ai_client.connection_pool._detected_api_type
                click.echo(f"  ğŸ”Œ å½“å‰APIç±»å‹: {api_type}")
        except Exception as e:
            click.echo(f"  âš ï¸ APIæ£€æµ‹å¤±è´¥: {e}")
        
    except Exception as e:
        click.echo(f"âŒ è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {e}", err=True)

@cli.command()
@click.argument('files', nargs=-1, required=True)
@click.option('--format', 'output_format', default='markdown',
              type=click.Choice(['markdown', 'json']),
              help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: markdown)')
@click.option('--force-search', is_flag=True, default=False,
              help='å¼ºåˆ¶é‡æ–°è¿›è¡Œæ–‡çŒ®æ£€ç´¢ï¼Œå¿½ç•¥ç¼“å­˜')
@click.option('--force-extract', is_flag=True, default=False,
              help='å¼ºåˆ¶é‡æ–°æå–è®ºæ–‡ä¿¡æ¯ï¼Œå¿½ç•¥ç¼“å­˜')
@click.option('--check-cache', is_flag=True, default=False,
              help='ä»…æ£€æŸ¥ç¼“å­˜çŠ¶æ€ï¼Œä¸æ‰§è¡Œè¯„ä¼°')
def eval_cached(files, output_format, force_search, force_extract, check_cache):
    """åŸºäºç¼“å­˜æ•°æ®å¿«é€Ÿè¯„ä¼°è®ºæ–‡ (æ¨èä½¿ç”¨)
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .docx (Wordæ–‡æ¡£)
    
    æ³¨æ„: æš‚ä¸æ”¯æŒPDFæ ¼å¼æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨Wordæ–‡æ¡£æ ¼å¼ã€‚
    """
    from .cached_evaluator import create_cached_evaluator
    from .ai_client import get_ai_client
    
    # éªŒè¯æ‰€æœ‰æ–‡ä»¶æ ¼å¼
    for file_path in files:
        validate_file_format(file_path)
    
    click.echo(f"ğŸš€ åŸºäºç¼“å­˜çš„è®ºæ–‡è¯„ä¼° ({len(files)} ä¸ªæ–‡ä»¶)")
    click.echo(f"ğŸ“Š è¾“å‡ºæ ¼å¼: {output_format}")
    if force_search:
        click.echo("ğŸ”§ å¼ºåˆ¶é‡æ–°æœç´¢æ¨¡å¼")
    if force_extract:
        click.echo("ğŸ”§ å¼ºåˆ¶é‡æ–°æå–æ¨¡å¼")
    
    try:
        config_mgr = get_config_manager()
        evaluator = create_cached_evaluator(config_mgr)
        
        success_count = 0
        failed_count = 0
        
        for file_path in files:
            click.echo(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
            
            # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
            cache_status = evaluator.get_cache_status(file_path)
            
            if check_cache:
                click.echo("ğŸ“Š ç¼“å­˜çŠ¶æ€æ£€æŸ¥:")
                click.echo(f"   è®ºæ–‡ä¿¡æ¯ç¼“å­˜: {'âœ… å·²ç¼“å­˜' if cache_status['thesis_info_cached'] else 'âŒ æœªç¼“å­˜'}")
                click.echo(f"   æ–‡çŒ®æœç´¢ç¼“å­˜: {'âœ… å·²ç¼“å­˜' if cache_status['search_results_cached'] else 'âŒ æœªç¼“å­˜'}")
                click.echo(f"   ç¼“å­˜æ–‡ä»¶æ•°é‡: {len(cache_status['cache_files'])} ä¸ª")
                for cache_file in cache_status['cache_files']:
                    size_mb = cache_file['size'] / (1024 * 1024)
                    click.echo(f"     ğŸ“ {cache_file['type']}: {size_mb:.2f} MB")
                continue
            
            # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
            click.echo(f"ğŸ’¾ è®ºæ–‡ä¿¡æ¯ç¼“å­˜: {'âœ…' if cache_status['thesis_info_cached'] else 'âŒ'}")
            click.echo(f"ğŸ’¾ æ–‡çŒ®æœç´¢ç¼“å­˜: {'âœ…' if cache_status['search_results_cached'] else 'âŒ'}")
            
            # æ‰§è¡Œè¯„ä¼°
            try:
                ai_client = get_ai_client()
                session_id = f"cached_eval_{int(time.time())}"
                
                result = evaluator.evaluate_with_cache(
                    file_path, ai_client, session_id,
                    force_search=force_search,
                    force_extract=force_extract,
                    output_format=output_format
                )
                
                if result['success']:
                    success_count += 1
                    click.echo("âœ… è¯„ä¼°å®Œæˆ")
                    if result.get('report_path'):
                        click.echo(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {result['report_path']}")
                    click.echo(f"ğŸ’¡ æ¶ˆæ¯: {result.get('message', '')}")
                else:
                    failed_count += 1
                    click.echo(f"âŒ è¯„ä¼°å¤±è´¥: {result['error']}")
                    if 'suggestions' in result:
                        click.echo("ğŸ’¡ å»ºè®®:")
                        for suggestion in result['suggestions']:
                            click.echo(f"   â€¢ {suggestion}")
                            
            except Exception as e:
                failed_count += 1
                click.echo(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
        
        if not check_cache:
            # æ˜¾ç¤ºæ€»ç»“
            click.echo(f"\nğŸ‰ æ‰¹é‡è¯„ä¼°å®Œæˆ!")
            click.echo(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
            if failed_count > 0:
                click.echo(f"âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        click.echo(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}", err=True)


@cli.command()
@click.argument('files', nargs=-1, required=True)
@click.option('--format', 'output_format', default='markdown',
              type=click.Choice(['markdown', 'json']),
              help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: markdown)')
@click.option('--skip-search', is_flag=True, default=False,
              help='è·³è¿‡æ–‡çŒ®æ£€ç´¢ï¼Œä»…åŸºäºç°æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š')
@click.option('--force-extract', is_flag=True, default=False,
              help='å¼ºåˆ¶é‡æ–°æå–è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¿½ç•¥å·²æœ‰JSONæ–‡ä»¶')
@click.option('--use-cached-search/--no-use-cached-search', default=True,
              help='ä½¿ç”¨å·²æœ‰çš„æ–‡çŒ®æ£€ç´¢ç¼“å­˜ç»“æœ (é»˜è®¤: å¯ç”¨)')
@click.option('--force-search', is_flag=True, default=False,
              help='å¼ºåˆ¶é‡æ–°è¿›è¡Œæ–‡çŒ®æ£€ç´¢ï¼Œå¿½ç•¥å·²æœ‰æ£€ç´¢ç¼“å­˜')
@click.option('--extraction-mode', default='batch-sections',
              type=click.Choice(['batch-sections']),
              help='è®ºæ–‡ä¿¡æ¯æå–æ¨¡å¼: batch-sections=ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼ (é»˜è®¤: batch-sections)')
@click.option('--batch-size', default=10000, type=int,
              help='ç« èŠ‚æ‰¹æ¬¡å¤„ç†æ—¶æ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•° (é»˜è®¤: 10000)')
def evaluate(files, output_format, skip_search, force_extract, use_cached_search, force_search, extraction_mode, batch_size):
    """è¯„ä¼°è®ºæ–‡æ–‡ä»¶å¹¶ç”ŸæˆæŠ¥å‘Š (å®Œæ•´æµç¨‹)
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .docx (Wordæ–‡æ¡£)
    
    æ³¨æ„: æš‚ä¸æ”¯æŒPDFæ ¼å¼æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨Wordæ–‡æ¡£æ ¼å¼ã€‚
    """
    # éªŒè¯æ‰€æœ‰æ–‡ä»¶æ ¼å¼
    for file_path in files:
        validate_file_format(file_path)
        
    logger.info(f"å¼€å§‹è¯„ä¼° {len(files)} ä¸ªæ–‡ä»¶ï¼Œæå–æ¨¡å¼: {extraction_mode}")
    
    click.echo(f"ğŸ“š å¼€å§‹è¯„ä¼° {len(files)} ä¸ªæ–‡ä»¶:")
    click.echo(f"ğŸ”§ æå–æ¨¡å¼: {extraction_mode}")
    if extraction_mode == 'batch-sections':
        click.echo(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size:,} å­—ç¬¦/æ‰¹æ¬¡")
        logger.info(f"ä½¿ç”¨ç« èŠ‚æ‰¹æ¬¡å¤„ç†æ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size} å­—ç¬¦")
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
    if len(files) == 1:
        return _evaluate_single_file(files[0], output_format, skip_search, force_extract, use_cached_search, force_search, extraction_mode, batch_size)
    
    # å¤šä¸ªæ–‡ä»¶æ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†
    click.echo(f"ğŸš€ å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ (æœ€å¤§{min(len(files), 3)}ä¸ªå¹¶å‘)")
    
    try:
        # å‡†å¤‡å¹¶è¡Œå¤„ç†ä»»åŠ¡
        success_count = 0
        failed_count = 0
        lock = threading.Lock()  # ç”¨äºä¿æŠ¤å…±äº«è®¡æ•°å™¨
        
        def process_file_with_session(file_info):
            """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ç‹¬ç«‹çš„AIä¼šè¯"""
            file_index, file_path = file_info
            session_id = f"eval_session_{file_index}_{int(time.time())}"
            
            try:
                result = _evaluate_single_file_parallel(
                    file_path, output_format, skip_search, force_extract, use_cached_search, force_search, session_id, extraction_mode, batch_size
                )
                return result
            except Exception as e:
                return {
                    'success': False,
                    'file_path': file_path,
                    'error': str(e),
                    'session_id': session_id
                }
        
        # æ‰§è¡Œå¹¶è¡Œå¤„ç†
        max_workers = min(len(files), 3)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•° (è¯„ä¼°æ¯”æå–æ›´èµ„æºå¯†é›†)
        file_list = list(enumerate(files))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(process_file_with_session, file_info): file_info[1] 
                for file_info in file_list
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    
                    with lock:
                        if result['success']:
                            success_count += 1
                        else:
                            failed_count += 1
                            click.echo(f"âŒ è¯„ä¼°æ–‡ä»¶å¤±è´¥ ({result['session_id']}): {result['error']}", err=True)
                            
                except Exception as exc:
                    with lock:
                        failed_count += 1
                        click.echo(f"âŒ è¯„ä¼°æ–‡ä»¶å¼‚å¸¸: {file_path} - {exc}", err=True)
        
        # æ˜¾ç¤ºæ€»ç»“
        click.echo(f"\nğŸ‰ å¹¶è¡Œè¯„ä¼°å®Œæˆ!")
        click.echo(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        if failed_count > 0:
            click.echo(f"âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        click.echo(f"ğŸ’¡ ä½¿ç”¨ 'thesis-eval files' æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶")
        
    except Exception as e:
        click.echo(f"âŒ å¹¶è¡Œè¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}", err=True)

def _evaluate_single_file(file_path, output_format, skip_search, force_extract=False, use_cached_search=True, force_search=False, extraction_mode='batch-sections', batch_size=10000):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰"""
    try:
        result = _evaluate_single_file_parallel(
            file_path, output_format, skip_search, force_extract, use_cached_search, force_search, session_id=None, extraction_mode=extraction_mode, batch_size=batch_size
        )
        
        if result['success']:
            click.echo(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")
            click.echo(f"ğŸ’¡ ä½¿ç”¨ 'thesis-eval files' æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶")
        else:
            click.echo(f"âŒ è¯„ä¼°å¤±è´¥: {result['error']}", err=True)
            
    except Exception as e:
        click.echo(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}", err=True)

def _evaluate_single_file_parallel(file_path, output_format, skip_search, force_extract, use_cached_search, force_search, session_id, extraction_mode='batch-sections', batch_size=10000):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰"""
    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}, ä¼šè¯ID: {session_id}, æå–æ¨¡å¼: {extraction_mode}")
    
    click.echo(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
    if session_id:
        click.echo(f"ğŸ”— ä¼šè¯ID: {session_id}")
    
    try:
        config_mgr = get_config_manager()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file_path).exists():
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
            error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            suggestions = []
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤æ ¼å¼é”™è¯¯
            if file_path in ['evaluate', 'extract', 'help', '--help', '-h']:
                error_msg = f"å‘½ä»¤æ ¼å¼é”™è¯¯: '{file_path}' ä¸æ˜¯æ–‡ä»¶è·¯å¾„"
                suggestions.extend([
                    "æ­£ç¡®æ ¼å¼: thesis-eval evaluate <æ–‡ä»¶è·¯å¾„>",
                    "æŸ¥çœ‹å¸®åŠ©: thesis-eval evaluate --help",
                    "æŸ¥çœ‹å¯ç”¨æ–‡ä»¶: thesis-eval files"
                ])
            else:
                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼
                if not file_path.startswith(('data/', './', '/', 'C:', 'D:')):
                    suggestions.append("æç¤º: æ–‡ä»¶è·¯å¾„å¯èƒ½éœ€è¦åŒ…å«ç›®å½•ï¼Œå¦‚ 'data/input/æ–‡ä»¶å.docx'")
                
                suggestions.extend([
                    "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                    "ä½¿ç”¨ 'thesis-eval files' æŸ¥çœ‹å¯ç”¨æ–‡ä»¶",
                    "ç¡®ä¿æ–‡ä»¶æ‰©å±•åä¸º .docx æˆ– .md (æš‚ä¸æ”¯æŒPDF)"
                ])
            
            return {
                'success': False,
                'file_path': file_path,
                'error': error_msg,
                'suggestions': suggestions,
                'session_id': session_id
            }
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        file_ext = Path(file_path).suffix
        supported_formats = config_mgr.get_supported_formats()
        if file_ext not in supported_formats:
            return {
                'success': False,
                'file_path': file_path,
                'error': f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}",
                'session_id': session_id
            }
        
        # å°è¯•è¯»å–ä¸“å®¶ç‰ˆç»“æ„åŒ–ä¿¡æ¯JSONæ–‡ä»¶
        thesis_extracted_info = None
        input_path = Path(file_path)
        base_name = input_path.stem
        output_dir = Path(config_mgr.get_output_dir())
        # åªè¯»å–ä¸“å®¶ç‰ˆæ–‡ä»¶
        extracted_info_file = output_dir / f"{base_name}_pro_extracted_info.json"
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨å·²æœ‰JSONæ–‡ä»¶
        if force_extract:
            click.echo("ğŸ”§ å¼ºåˆ¶é‡æ–°æå–æ¨¡å¼ï¼Œå°†å¿½ç•¥å·²æœ‰çš„ç»“æ„åŒ–ä¿¡æ¯æ–‡ä»¶")
            thesis_extracted_info = None
        elif extracted_info_file.exists():
            try:
                click.echo(f"ğŸ“– å‘ç°å·²æœ‰çš„ç»“æ„åŒ–ä¿¡æ¯æ–‡ä»¶: {extracted_info_file}")
                with open(extracted_info_file, 'r', encoding='utf-8') as f:
                    extracted_data = json.load(f)
                    thesis_extracted_info = extracted_data.get('extracted_info', None)
                    if thesis_extracted_info:
                        click.echo("âœ… æˆåŠŸè¯»å–å·²æœ‰çš„è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ï¼Œè·³è¿‡é‡å¤æå–")
                        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                        if 'title_cn' in thesis_extracted_info:
                            title = thesis_extracted_info['title_cn']
                            click.echo(f"ğŸ“‹ è®ºæ–‡æ ‡é¢˜: {title[:50]}...")
                        metadata = extracted_data.get('metadata', {})
                        if 'extraction_time' in metadata:
                            click.echo(f"â° åŸæå–æ—¶é—´: {metadata['extraction_time']}")
            except Exception as e:
                click.echo(f"âš ï¸ è¯»å–ç»“æ„åŒ–ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
                thesis_extracted_info = None
        else:
            thesis_extracted_info = None
        
        # æ£€æµ‹ç¼“å­˜çš„æ–‡çŒ®æ£€ç´¢ç»“æœ
        cached_search_results = None
        if not skip_search and not force_search and use_cached_search:
            cached_search_results = _detect_cached_search_results(base_name, output_dir)
            if cached_search_results:
                click.echo("ğŸ“š å‘ç°å·²æœ‰çš„æ–‡çŒ®æ£€ç´¢ç¼“å­˜ç»“æœï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®")
                click.echo(f"   ğŸ“‹ ç¼“å­˜åŒ…å«: {len(cached_search_results['papers_by_lang'])} ç§è¯­è¨€çš„æ£€ç´¢ç»“æœ")
                for lang, papers in cached_search_results['papers_by_lang'].items():
                    click.echo(f"   ğŸ“Š {lang}: {len(papers)} ç¯‡ç›¸å…³æ–‡çŒ®")
        elif force_search:
            click.echo("ğŸ”§ å¼ºåˆ¶é‡æ–°æ£€ç´¢æ¨¡å¼ï¼Œå°†å¿½ç•¥å·²æœ‰çš„æ–‡çŒ®æ£€ç´¢ç¼“å­˜")
        elif not use_cached_search:
            click.echo("ğŸ”§ ç¦ç”¨æ–‡çŒ®æ£€ç´¢ç¼“å­˜ï¼Œå°†é‡æ–°è¿›è¡Œæ–‡çŒ®æ£€ç´¢")
        
        # åˆå§‹åŒ–ä¸»è¦å˜é‡
        papers_by_lang = {}
        literature_metadata_analysis = None
        document_text = ""
        
        if not skip_search:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„æ£€ç´¢ç»“æœ
            if cached_search_results:
                click.echo("ğŸ”„ å¼€å§‹è®ºæ–‡è¯„ä¼°æµç¨‹ (ä½¿ç”¨ç¼“å­˜çš„æ–‡çŒ®æ£€ç´¢ç»“æœ)...")
                if thesis_extracted_info:
                    click.echo("   æ­¥éª¤1: âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ (å·²å­˜åœ¨)")
                elif force_extract:
                    click.echo("   æ­¥éª¤1: ğŸ”§ è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ (å¼ºåˆ¶é‡æ–°æå–)")
                else:
                    click.echo("   æ­¥éª¤1: ğŸ“„ è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ (éœ€è¦æå–)")
                click.echo("   æ­¥éª¤2: âœ… æ–‡çŒ®æ£€ç´¢ç»“æœ (ä½¿ç”¨ç¼“å­˜)")
                click.echo("   æ­¥éª¤3: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                
                # ä½¿ç”¨ç¼“å­˜çš„æ£€ç´¢ç»“æœ
                papers_by_lang = cached_search_results['papers_by_lang']
                literature_metadata_analysis = cached_search_results.get('literature_metadata_analysis', None)
                
                click.echo("âœ… ä½¿ç”¨ç¼“å­˜çš„æ–‡çŒ®æ£€ç´¢ç»“æœ")
                for lang, papers in papers_by_lang.items():
                    click.echo(f"   {lang}: ä½¿ç”¨ç¼“å­˜çš„ {len(papers)} ç¯‡ç›¸å…³æ–‡çŒ®")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æå–è®ºæ–‡ä¿¡æ¯
                if not thesis_extracted_info:
                    click.echo("ğŸ“„ å¼€å§‹æå–è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯...")
                    try:
                        # æå–æ–‡æ¡£æ–‡æœ¬ (ä»…æ”¯æŒWordæ–‡æ¡£)
                        from .extract_sections_with_ai import extract_text_from_word
                        click.echo("ğŸ” æ­£åœ¨æå–æ–‡æ¡£æ–‡æœ¬...")
                        
                        document_text = extract_text_from_word(file_path)
                        
                        click.echo(f"âœ… æ–‡æ¡£æ–‡æœ¬æå–å®Œæˆ ({len(document_text):,} å­—ç¬¦)")
                        
                        # ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼
                        click.echo(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯ (ä¸“å®¶ç­–ç•¥æ¨¡å¼)...")
                        
                        from .ai_client import get_ai_client
                        ai_client = get_ai_client()
                        
                        # ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼
                        from .extract_sections_with_ai import extract_sections_with_pro_strategy
                        extracted_info = extract_sections_with_pro_strategy(file_path=file_path, use_cache=True)
                        click.echo(f"   ğŸ“ ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼ (å¤šå­¦ç§‘æ”¯æŒ)")
                        
                        if extracted_info:
                            click.echo("âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯æå–æˆåŠŸ")
                            thesis_extracted_info = extracted_info
                            
                            # ä¿å­˜ä¸ºJSONæ–‡ä»¶
                            extracted_info_data = {
                                "metadata": {
                                    "extraction_time": datetime.now().isoformat(),
                                    "file_path": str(file_path),
                                    "document_length": len(document_text),
                                    "extraction_method": "AI + force_extract" if force_extract else "AI",
                                    "session_id": session_id
                                },
                                "extracted_info": extracted_info
                            }
                            
                            with open(extracted_info_file, 'w', encoding='utf-8') as f:
                                json.dump(extracted_info_data, f, ensure_ascii=False, indent=2)
                            
                            click.echo(f"ğŸ’¾ ç»“æ„åŒ–ä¿¡æ¯å·²ä¿å­˜: {extracted_info_file}")
                        else:
                            click.echo("âš ï¸ è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯æå–å¤±è´¥")
                            
                    except Exception as e:
                        click.echo(f"âŒ è®ºæ–‡ä¿¡æ¯æå–å¤±è´¥: {e}")
                        thesis_extracted_info = None
                
                if thesis_extracted_info:
                    click.echo("âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯å¯ç”¨")
                    
            else:
                # æ‰§è¡Œå®Œæ•´çš„è®ºæ–‡å¤„ç†æµç¨‹
                if thesis_extracted_info:
                    click.echo("ğŸ”„ å¼€å§‹è®ºæ–‡è¯„ä¼°æµç¨‹ (ä½¿ç”¨å·²æœ‰ç»“æ„åŒ–ä¿¡æ¯)...")
                    click.echo("   æ­¥éª¤1: âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯ (å·²å­˜åœ¨)")
                    click.echo("   æ­¥éª¤2: ç”Ÿæˆæ£€ç´¢ç­–ç•¥")
                    click.echo("   æ­¥éª¤3: æ‰§è¡Œæ–‡çŒ®æ£€ç´¢")
                    click.echo("   æ­¥éª¤4: è®¡ç®—æ–‡çŒ®ç›¸å…³æ€§")
                    click.echo("   æ­¥éª¤5: ç­›é€‰TOPç›¸å…³æ–‡çŒ®")
                    click.echo("   æ­¥éª¤6: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                else:
                    click.echo("ğŸ”„ å¼€å§‹å®Œæ•´çš„è®ºæ–‡åˆ†ææµç¨‹...")
                    click.echo("   æ­¥éª¤1: æå–è®ºæ–‡å†…å®¹")
                    click.echo("   æ­¥éª¤2: ä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯")
                    click.echo("   æ­¥éª¤3: ç”Ÿæˆæ£€ç´¢ç­–ç•¥")
                    click.echo("   æ­¥éª¤4: æ‰§è¡Œæ–‡çŒ®æ£€ç´¢")
                    click.echo("   æ­¥éª¤5: è®¡ç®—æ–‡çŒ®ç›¸å…³æ€§")
                    click.echo("   æ­¥éª¤6: ç­›é€‰TOPç›¸å…³æ–‡çŒ®")
                    click.echo("   æ­¥éª¤7: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
                
                try:
                    # è°ƒç”¨å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼ˆä¼ é€’session_idå’Œå·²æœ‰çš„ç»“æ„åŒ–ä¿¡æ¯ï¼‰
                    from .cnki_client_pool import cnki_auto_search
                    languages = config_mgr.get_supported_languages()
                    
                    # å¦‚æœæœ‰å·²æå–çš„ä¿¡æ¯ï¼Œä¼ é€’ç»™æ£€ç´¢å‡½æ•°
                    search_kwargs = {
                        'languages': languages, 
                        'session_id': session_id
                    }
                    if thesis_extracted_info:
                        search_kwargs['existing_thesis_info'] = thesis_extracted_info
                    
                    search_results = cnki_auto_search(
                        file_path, **search_kwargs
                    )
                    papers_by_lang = search_results['papers_by_lang']
                    # å¦‚æœæ²¡æœ‰ä¼ å…¥å·²æœ‰ä¿¡æ¯ï¼Œä½¿ç”¨æ£€ç´¢ç»“æœä¸­çš„ä¿¡æ¯
                    if not thesis_extracted_info:
                        thesis_extracted_info = search_results['thesis_extracted_info']
                    literature_metadata_analysis = search_results.get('literature_metadata_analysis', None)
                    
                    click.echo("âœ… è®ºæ–‡åˆ†æå’Œæ–‡çŒ®æ£€ç´¢å®Œæˆ")
                    for lang, papers in papers_by_lang.items():
                        click.echo(f"   {lang}: æ£€ç´¢åˆ° {len(papers)} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    if thesis_extracted_info:
                        click.echo("âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯å¯ç”¨")
                    
                except Exception as e:
                    return {
                        'success': False,
                        'file_path': file_path,
                        'error': f"å®Œæ•´æµç¨‹æ‰§è¡Œå¤±è´¥: {e}",
                        'session_id': session_id
                    }
        
        else:
            # è·³è¿‡æ£€ç´¢ï¼Œä½†å¦‚æœæ²¡æœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œä»éœ€æå–
            click.echo("â­ï¸ è·³è¿‡æ–‡çŒ®æ£€ç´¢ï¼ŒåŸºäºç°æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š...")
            
            # å¦‚æœæ²¡æœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œéœ€è¦å…ˆæå–
            if not thesis_extracted_info:
                try:
                    click.echo("ğŸ“„ æ²¡æœ‰ç°æœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¼€å§‹æå–è®ºæ–‡ä¿¡æ¯...")
                    click.echo("   æ­¥éª¤1: æå–è®ºæ–‡å†…å®¹")
                    click.echo("   æ­¥éª¤2: ä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯")
                    
                    # è·å–AIå®¢æˆ·ç«¯
                    from .ai_client import get_ai_client
                    from .extract_sections_with_ai import extract_sections_with_ai, extract_text_from_word
                    
                    ai_client = get_ai_client()
                    # ä½¿ç”¨ä¼ å…¥çš„session_idï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºæ–°çš„
                    ai_session_id = session_id if session_id else ai_client.create_session()
                    
                    # è·å–æ”¯æŒçš„è¯­è¨€
                    languages = config_mgr.get_supported_languages()
                    
                    # æå–æ–‡æ¡£æ–‡æœ¬ (ä»…æ”¯æŒWordæ–‡æ¡£)
                    click.echo("ğŸ” æ­£åœ¨æå–æ–‡æ¡£æ–‡æœ¬...")
                    
                    document_text = extract_text_from_word(file_path)
                    
                    click.echo(f"âœ… æ–‡æ¡£æ–‡æœ¬æå–å®Œæˆ ({len(document_text):,} å­—ç¬¦)")
                    
                    # ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼
                    click.echo(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯ (ä¸“å®¶ç­–ç•¥æ¨¡å¼)...")
                    
                    # ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼
                    from .extract_sections_with_ai import extract_sections_with_pro_strategy
                    thesis_extracted_info = extract_sections_with_pro_strategy(
                        file_path=file_path, 
                        use_cache=True
                    )
                    click.echo(f"   ğŸ“ ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼ (å¤šå­¦ç§‘æ”¯æŒ)")
                    
                    if thesis_extracted_info:
                        click.echo("âœ… è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯æå–å®Œæˆ")
                        # ä¿å­˜æå–çš„ä¿¡æ¯åˆ°ä¸“å®¶ç‰ˆJSONæ–‡ä»¶
                        extracted_info_file = output_dir / f"{base_name}_pro_extracted_info.json"
                        extracted_data = {
                            'extracted_info': thesis_extracted_info,
                            'metadata': {
                                'extraction_time': datetime.now().isoformat(),
                                'file_path': str(file_path),
                                'method': 'pro_strategy',
                                'extractor_version': '2.0',
                                'session_id': ai_session_id
                            }
                        }
                        with open(extracted_info_file, 'w', encoding='utf-8') as f:
                            json.dump(extracted_data, f, ensure_ascii=False, indent=2)
                        click.echo(f"ğŸ’¾ ä¸“å®¶ç‰ˆç»“æ„åŒ–ä¿¡æ¯å·²ä¿å­˜: {extracted_info_file}")
                    else:
                        click.echo("âš ï¸ è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯æå–å¤±è´¥")
                        
                except Exception as e:
                    click.echo(f"âŒ è®ºæ–‡ä¿¡æ¯æå–å¤±è´¥: {e}")
                    thesis_extracted_info = None
        
        # å¦‚æœè·³è¿‡æ£€ç´¢æˆ–ä½¿ç”¨ç¼“å­˜ï¼Œç¡®ä¿å˜é‡å·²åˆå§‹åŒ–
        if skip_search or cached_search_results:
            if not 'papers_by_lang' in locals():
                papers_by_lang = cached_search_results['papers_by_lang'] if cached_search_results else {}
            if not 'literature_metadata_analysis' in locals():
                literature_metadata_analysis = cached_search_results.get('literature_metadata_analysis', None) if cached_search_results else None
        
        if output_format == 'markdown':
            try:
                # ç”ŸæˆMarkdownæŠ¥å‘Š
                click.echo("ğŸ“ æ­£åœ¨ç”ŸæˆMarkdownè¯„ä¼°æŠ¥å‘Š...")
                from .report_generator import MarkdownReportGenerator
                
                report_generator = MarkdownReportGenerator()
                
                # å¦‚æœæœ‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¼ é€’ç»™æŠ¥å‘Šç”Ÿæˆå™¨
                if thesis_extracted_info:
                    report_file_path = report_generator.generate_evaluation_report(
                        file_path, 
                        thesis_extracted_info=thesis_extracted_info
                    )
                else:
                    report_file_path = report_generator.generate_evaluation_report(file_path)
                
                click.echo(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_file_path}")
                
                # ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†ææŠ¥å‘Š
                if thesis_extracted_info and papers_by_lang:
                    try:
                        click.echo("ğŸ“Š æ­£åœ¨ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†ææŠ¥å‘Š...")
                        from .literature_review_analyzer import LiteratureReviewAnalyzer
                        
                        literature_analyzer = LiteratureReviewAnalyzer()
                        literature_report_path = literature_analyzer.analyze_literature_review(
                            str(file_path),
                            thesis_extracted_info,
                            papers_by_lang,
                            str(output_dir)
                        )
                        click.echo(f"âœ… æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {literature_report_path}")
                        
                    except Exception as e:
                        click.echo(f"âš ï¸ æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                        # ä¸ä¸­æ–­ä¸»æµç¨‹ï¼Œåªæ˜¯è­¦å‘Š
                
            except Exception as e:
                return {
                    'success': False,
                    'file_path': file_path,
                    'error': f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}",
                    'session_id': session_id
                }
        
        else:
            # JSONæ ¼å¼è¾“å‡ºï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
            output_dir = config_mgr.get_output_dir()
            base_name = Path(file_path).stem
            
            # åˆ—å‡ºå¯èƒ½çš„è¾“å‡ºæ–‡ä»¶
            potential_files = [
                f"{base_name}_relevant_papers_Chinese.json",
                f"{base_name}_relevant_papers_English.json",
                f"{base_name}_TOP{config_mgr.get_top_papers_count()}PAPERS_Chinese.json",
                f"{base_name}_TOP{config_mgr.get_top_papers_count()}PAPERS_English.json"
            ]
            
            click.echo(f"ğŸ“Š JSONæ•°æ®æ–‡ä»¶ä½ç½®: {output_dir}")
            for filename in potential_files:
                file_path_full = Path(output_dir) / filename
                if file_path_full.exists():
                    click.echo(f"  âœ… {filename}")
                else:
                    click.echo(f"  â“ {filename} (å¾…ç”Ÿæˆ)")
        
        return {
            'success': True,
            'file_path': file_path,
            'session_id': session_id
        }
        
    except Exception as e:
        return {
            'success': False,
            'file_path': file_path,
            'error': str(e),
            'session_id': session_id
        }

@cli.command()
@click.argument('files', nargs=-1, required=True)
@click.option('--output-dir', '-o', 
              help='è¾“å‡ºç›®å½• (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¾“å‡ºç›®å½•)')
@click.option('--format', 'output_format', default='json',
              type=click.Choice(['json', 'markdown']),
              help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: json)')
@click.option('--extraction-mode', default='batch-sections',
              type=click.Choice(['batch-sections']),
              help='è®ºæ–‡ä¿¡æ¯æå–æ¨¡å¼: batch-sections=ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼ (é»˜è®¤: batch-sections)')
@click.option('--batch-size', default=10000, type=int,
              help='ç« èŠ‚æ‰¹æ¬¡å¤„ç†æ—¶æ¯æ‰¹æ¬¡æœ€å¤§å­—ç¬¦æ•° (é»˜è®¤: 10000)')
@click.option('--use-cache/--no-cache', default=True,
              help='æ˜¯å¦ä½¿ç”¨æ–‡æ¡£ç¼“å­˜ (é»˜è®¤: å¯ç”¨)')
@click.option('--clear-cache', is_flag=True, default=False,
              help='æ¸…é™¤æ–‡æ¡£ç¼“å­˜åé‡æ–°å¤„ç†')
def extract(files, output_dir, output_format, extraction_mode, batch_size, use_cache, clear_cache):
    """æå–è®ºæ–‡ç»“æ„åŒ–ä¿¡æ¯å¹¶ç”ŸæˆJSONæ–‡ä»¶
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .docx (Wordæ–‡æ¡£)
    
    æ³¨æ„: æš‚ä¸æ”¯æŒPDFæ ¼å¼æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨Wordæ–‡æ¡£æ ¼å¼ã€‚
    """
    # éªŒè¯æ‰€æœ‰æ–‡ä»¶æ ¼å¼
    for file_path in files:
        validate_file_format(file_path)
        
    click.echo(f"ğŸ“„ å¼€å§‹æå– {len(files)} ä¸ªæ–‡ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯:")
    click.echo(f"ğŸ”§ æå–æ¨¡å¼: {extraction_mode}")
    if extraction_mode == 'batch-sections':
        click.echo(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size:,} å­—ç¬¦")
    click.echo(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")
    
    # å¤„ç†ç¼“å­˜æ¸…é™¤
    if clear_cache:
        click.echo("ğŸ—‘ï¸ æ¸…é™¤æ–‡æ¡£ç¼“å­˜...")
        from .extract_sections_with_ai import get_document_cache
        cache_manager = get_document_cache()
        cache_manager.clear_cache()
        click.echo("âœ… ç¼“å­˜å·²æ¸…é™¤")
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†
    if len(files) == 1:
        return _extract_single_file(files[0], output_dir, output_format, extraction_mode, batch_size, use_cache)
    
    # å¤šä¸ªæ–‡ä»¶æ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†
    click.echo(f"ğŸš€ å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ (æœ€å¤§{min(len(files), 4)}ä¸ªå¹¶å‘)")
    
    try:
        config_mgr = get_config_manager()
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if not output_dir:
            output_dir = config_mgr.get_output_dir()
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†ä»»åŠ¡
        extraction_time = datetime.now().isoformat()
        success_count = 0
        failed_count = 0
        lock = threading.Lock()  # ç”¨äºä¿æŠ¤å…±äº«è®¡æ•°å™¨
        
        def process_file_with_session(file_info):
            """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ç‹¬ç«‹çš„AIä¼šè¯"""
            file_index, file_path = file_info
            session_id = f"extract_session_{file_index}_{int(time.time())}"
            
            try:
                result = _extract_single_file_parallel(
                    file_path, output_path, output_format, 
                    extraction_time, session_id, extraction_mode, batch_size, use_cache
                )
                return result
            except Exception as e:
                return {
                    'success': False,
                    'file_path': file_path,
                    'error': str(e),
                    'session_id': session_id
                }
        
        # æ‰§è¡Œå¹¶è¡Œå¤„ç†
        max_workers = min(len(files), 4)  # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
        file_list = list(enumerate(files))
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(process_file_with_session, file_info): file_info[1] 
                for file_info in file_list
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    
                    with lock:
                        if result['success']:
                            success_count += 1
                        else:
                            failed_count += 1
                            click.echo(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ ({result['session_id']}): {result['error']}", err=True)
                            # æ˜¾ç¤ºå»ºè®®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                            if 'suggestions' in result:
                                click.echo("ğŸ’¡ å»ºè®®:")
                                for suggestion in result['suggestions']:
                                    click.echo(f"   â€¢ {suggestion}")
                            
                except Exception as exc:
                    with lock:
                        failed_count += 1
                        click.echo(f"âŒ å¤„ç†æ–‡ä»¶å¼‚å¸¸: {file_path} - {exc}", err=True)
        
        # æ˜¾ç¤ºæ€»ç»“
        click.echo(f"\nğŸ‰ å¹¶è¡Œæå–å®Œæˆ!")
        click.echo(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        if failed_count > 0:
            click.echo(f"âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        
    except Exception as e:
        import traceback
        click.echo(f"âŒ å¹¶è¡Œæå–è¿‡ç¨‹å¤±è´¥: {e}", err=True)
        click.echo(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}", err=True)

def _extract_single_file(file_path, output_dir, output_format, extraction_mode='batch-sections', batch_size=10000, use_cache=True):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰"""
    try:
        config_mgr = get_config_manager()
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if not output_dir:
            output_dir = config_mgr.get_output_dir()
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        extraction_time = datetime.now().isoformat()
        
        result = _extract_single_file_parallel(
            file_path, output_path, output_format, 
            extraction_time, session_id=None, extraction_mode=extraction_mode, batch_size=batch_size, use_cache=use_cache
        )
        
        if result['success']:
            click.echo(f"\nğŸ‰ æå–å®Œæˆ!")
            click.echo(f"âœ… æˆåŠŸ: 1 ä¸ªæ–‡ä»¶")
            click.echo(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        else:
            click.echo(f"âŒ æå–å¤±è´¥: {result['error']}", err=True)
            
    except Exception as e:
        import traceback
        click.echo(f"âŒ æå–è¿‡ç¨‹å¤±è´¥: {e}", err=True)
        click.echo(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}", err=True)

def _extract_single_file_parallel(file_path, output_path, output_format, extraction_time, session_id, extraction_mode='batch-sections', batch_size=10000, use_cache=True):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒé€»è¾‘ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰"""
    click.echo(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
    if session_id:
        click.echo(f"ğŸ”— ä¼šè¯ID: {session_id}")
    click.echo(f"ğŸ”§ æå–æ¨¡å¼: {extraction_mode}")
    if extraction_mode == 'batch-sections':
        click.echo(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {batch_size:,} å­—ç¬¦")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    input_file = Path(file_path)
    if not input_file.exists():
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
        error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
        suggestions = []
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‘½ä»¤æ ¼å¼é”™è¯¯
        if file_path in ['evaluate', 'extract', 'help', '--help', '-h']:
            error_msg = f"å‘½ä»¤æ ¼å¼é”™è¯¯: '{file_path}' ä¸æ˜¯æ–‡ä»¶è·¯å¾„"
            suggestions.extend([
                "æ­£ç¡®æ ¼å¼: thesis-eval extract <æ–‡ä»¶è·¯å¾„>",
                "æŸ¥çœ‹å¸®åŠ©: thesis-eval extract --help",
                "æŸ¥çœ‹å¯ç”¨æ–‡ä»¶: thesis-eval files"
            ])
        else:
            # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ ¼å¼
            if not file_path.startswith(('data/', './', '/', 'C:', 'D:')):
                suggestions.append("æç¤º: æ–‡ä»¶è·¯å¾„å¯èƒ½éœ€è¦åŒ…å«ç›®å½•ï¼Œå¦‚ 'data/input/æ–‡ä»¶å.docx'")
            
            suggestions.extend([
                "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                "ä½¿ç”¨ 'thesis-eval files' æŸ¥çœ‹å¯ç”¨æ–‡ä»¶",
                "ç¡®ä¿æ–‡ä»¶æ‰©å±•åä¸º .docx æˆ– .md (æš‚ä¸æ”¯æŒPDF)"
            ])
        
        return {
            'success': False,
            'file_path': file_path,
            'error': error_msg,
            'suggestions': suggestions,
            'session_id': session_id
        }
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    file_ext = input_file.suffix.lower()
    if file_ext not in ['.docx']:
        return {
            'success': False,
            'file_path': file_path,
            'error': f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œå½“å‰ä»…æ”¯æŒ .docx æ ¼å¼",
            'session_id': session_id
        }
    
    try:
        # ä½¿ç”¨ä¸“å®¶ç­–ç•¥å¤„ç†æ¨¡å¼æå–è®ºæ–‡ä¿¡æ¯
        from .extract_sections_with_ai import extract_sections_with_pro_strategy
        
        click.echo(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨ä¸“å®¶ç­–ç•¥æå–ç»“æ„åŒ–ä¿¡æ¯...")
        extracted_info = extract_sections_with_pro_strategy(
            file_path=file_path, 
            use_cache=use_cache
        )
        
        if extracted_info:
            # ä¿å­˜æå–ç»“æœ - ä½¿ç”¨ä¸“å®¶ç‰ˆæ–‡ä»¶å
            base_name = input_file.stem
            output_file = output_path / f"{base_name}_pro_extracted_info.json"
            
            output_data = {
                'extracted_info': extracted_info,
                'metadata': {
                    'extraction_time': extraction_time,
                    'file_path': str(file_path),
                    'extraction_mode': extraction_mode,
                    'method': 'pro_strategy',
                    'extractor_version': '2.0',
                    'session_id': session_id
                }
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            click.echo(f"âœ… æå–æˆåŠŸ: {output_file}")
            return {
                'success': True,
                'file_path': file_path,
                'output_file': str(output_file),
                'session_id': session_id
            }
        else:
            return {
                'success': False,
                'file_path': file_path,
                'error': "æå–å¤±è´¥ï¼šæœªè·å¾—æœ‰æ•ˆçš„ç»“æ„åŒ–ä¿¡æ¯",
                'session_id': session_id
            }
            
    except Exception as e:
        return {
            'success': False,
            'file_path': file_path,
            'error': f"æå–è¿‡ç¨‹å‡ºé”™: {str(e)}",
            'session_id': session_id
        }


@cli.command()
def files():
    """åˆ—å‡ºè¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶"""
    click.echo("ğŸ“ æ–‡ä»¶åˆ—è¡¨:")
    
    try:
        config_mgr = get_config_manager()
        
        # åˆ—å‡ºè¾“å…¥æ–‡ä»¶
        input_dir = Path(config_mgr.get_input_dir())
        if input_dir.exists():
            click.echo(f"\nğŸ“¥ è¾“å…¥æ–‡ä»¶ ({input_dir}):")
            input_files = list(input_dir.glob("*"))
            if input_files:
                for file in sorted(input_files):
                    if file.is_file():
                        size = file.stat().st_size
                        click.echo(f"  ğŸ“„ {file.name} ({size:,} bytes)")
            else:
                click.echo("  (æ— æ–‡ä»¶)")
        else:
            click.echo(f"\nâŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        # åˆ—å‡ºè¾“å‡ºæ–‡ä»¶
        output_dir = Path(config_mgr.get_output_dir())
        if output_dir.exists():
            click.echo(f"\nğŸ“¤ è¾“å‡ºæ–‡ä»¶ ({output_dir}):")
            output_files = list(output_dir.glob("*"))
            if output_files:
                for file in sorted(output_files):
                    if file.is_file():
                        size = file.stat().st_size
                        click.echo(f"  ğŸ“„ {file.name} ({size:,} bytes)")
            else:
                click.echo("  (æ— æ–‡ä»¶)")
        else:
            click.echo(f"\nâŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
            
    except Exception as e:
        click.echo(f"âŒ åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}", err=True)


@cli.command(name='md-cnki-search-cn')
@click.option('--input-dir', default=None, help='è¾“å…¥ç›®å½•ï¼Œé»˜è®¤è¯»å–é…ç½®ä¸­çš„ input_dir')
def md_cnki_search_cn(input_dir):
    """è¯»å– data/input ä¸‹æ‰€æœ‰ .mdï¼Œç”Ÿæˆä¸­æ–‡æ£€ç´¢å¼å¹¶è°ƒç”¨CNKIæ£€ç´¢ã€‚

    - ä»…ä¸­æ–‡æ£€ç´¢ï¼ˆlanguages=['Chinese']ï¼‰
    - ä¸è¿›è¡Œç« èŠ‚åˆ†æï¼›ä» .md ä¸­ç²—æé¢˜å/å…³é”®è¯/æ‘˜è¦/æ–¹æ³•
    - ç»“æœå†™å…¥ data/output ä¸‹ç›¸å…³æ–‡çŒ®ä¸TOPæ–‡çŒ®JSON
    """
    try:
        import re
        from datetime import date, datetime
        from pathlib import Path
        from typing import Optional
        from .cnki_client_pool import cnki_auto_search

        def first_heading(md: str) -> str:
            for line in md.splitlines():
                m = re.match(r'^\s{0,3}#\s+(.+)$', line.strip())
                if m:
                    return m.group(1).strip()
            return ''

        def find_keywords(md: str, labels) -> str:
            for line in md.splitlines():
                if any(lbl.lower() in line.lower() for lbl in labels):
                    m = re.search(r'(?:å…³é”®è¯|å…³é”®å­—|Keywords?)[:ï¼š]\s*(.+)', line, re.I)
                    if m:
                        return m.group(1).strip()
            return ''

        def extract_block(md: str, headers) -> str:
            # æå–â€œ## æ‘˜è¦ â€¦ åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜â€
            pat_h = re.compile(r'^\s{0,3}#{1,6}\s*(' + '|'.join(map(re.escape, headers)) + r')\b.*$', re.I | re.M)
            m = pat_h.search(md)
            if m:
                start = m.end()
            else:
                pat_l = re.compile(r'^(?:' + '|'.join(map(re.escape, headers)) + r')\s*[:ï¼š]?\s*$', re.I | re.M)
                m = pat_l.search(md)
                if not m:
                    return ''
                start = m.end()
            nxt = re.search(r'^\s{0,3}#{1,6}\s+\S+', md[start:], re.M)
            end = start + (nxt.start() if nxt else len(md) - start)
            return md[start:end].strip()

        def normalize_label(line: str) -> str:
            cleaned = line.strip()
            cleaned = re.sub(r'^[#\*\s]+', '', cleaned)
            cleaned = re.sub(r'[\*\s]+$', '', cleaned)
            return cleaned.strip()

        def match_date_value(text: str) -> Optional[date]:
            if not text:
                return None
            chunk = text.strip()
            if not chunk:
                return None
            parts = re.split(r'[ï¼š:]', chunk, maxsplit=1)
            if len(parts) == 2:
                chunk = parts[1]
            chunk = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', chunk)
            translation = str.maketrans({'å¹´': '-', 'æœˆ': '-', 'æ—¥': '', '/': '-', '.': '-', 'ï¼': '-'})
            chunk = chunk.translate(translation)
            chunk = re.sub(r'\s+', '', chunk)
            if not chunk:
                return None
            candidates = [chunk, chunk.replace('-', '')]
            formats = ('%Y-%m-%d', '%Y%m%d', '%Y-%m', '%Y%m')
            for candidate in candidates:
                for fmt in formats:
                    try:
                        dt = datetime.strptime(candidate, fmt)
                        if fmt in ('%Y-%m', '%Y%m'):
                            dt = datetime(dt.year, dt.month, 1)
                        return dt.date()
                    except ValueError:
                        continue
            match = re.search(r'(\d{4})(\d{1,2})(\d{1,2})', candidates[0])
            if match:
                year, month, day = map(int, match.groups())
                return date(year, month, day)
            return None

        def extract_date(md: str, labels) -> Optional[date]:
            lines = md.splitlines()
            label_lower = [lbl.lower() for lbl in labels]
            for idx, line in enumerate(lines):
                normalized = normalize_label(line).lower()
                if any(lbl in normalized for lbl in label_lower):
                    dv = match_date_value(line)
                    if not dv and idx + 1 < len(lines):
                        dv = match_date_value(lines[idx + 1])
                    if dv:
                        return dv
            return None

        # å‡†å¤‡è¾“å…¥ç›®å½•
        config_mgr = get_config_manager()
        base_input = input_dir or config_mgr.get_input_dir()
        input_path = Path(base_input)
        if not input_path.exists():
            click.echo(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_path}")
            return

        md_files = sorted(input_path.glob('*.md'))
        if not md_files:
            click.echo("(æœªæ‰¾åˆ° .md æ–‡ä»¶) ç›®å½•: " + str(input_path))
            return

        click.echo(f"ğŸ“„ å‘ç° {len(md_files)} ä¸ª .md æ–‡ä»¶ï¼Œå¼€å§‹ä¸­æ–‡æ£€ç´¢â€¦")

        success = 0
        failed = 0

        for md_file in md_files:
            click.echo(f"\nğŸ“„ å¤„ç†: {md_file.name}")
            try:
                md = md_file.read_text('utf-8', errors='ignore')
                h1 = first_heading(md) or md_file.stem
                abs_cn = extract_block(md, ['ä¸­æ–‡æ‘˜è¦', 'æ‘˜è¦']) or ''
                kw_cn = find_keywords(md, ['å…³é”®è¯', 'å…³é”®å­—']) or ''
                has_cn = bool(re.search(r'[\u4e00-\u9fa5]', h1))
                title_cn = h1 if has_cn else ''
                methods = extract_block(md, ['ç ”ç©¶æ–¹æ³•', 'æ–¹æ³•']) or ''

                defense_dt = extract_date(md, ['è®ºæ–‡ç­”è¾©æ—¥æœŸ', 'ç­”è¾©æ—¥æœŸ', 'ç­”è¾©æ—¶é—´', 'ç­”è¾©'])
                completion_dt = extract_date(md, ['è®ºæ–‡å®Œæˆæ—¥æœŸ', 'å®Œæˆæ—¥æœŸ', 'å®Œæˆæ—¶é—´'])
                submission_dt = extract_date(md, ['è®ºæ–‡æäº¤æ—¥æœŸ', 'æäº¤æ—¥æœŸ', 'æäº¤æ—¶é—´'])
                search_cutoff = defense_dt or completion_dt or submission_dt

                thesis_info = {
                    'title_ch': title_cn,
                    'keywords_ch': kw_cn,
                    'abstract_ch': abs_cn,
                    'title_cn': title_cn,
                    'keywords_cn': kw_cn,
                    'abstract_cn': abs_cn,
                    'ChineseTitle': title_cn,
                    'ChineseKeywords': kw_cn,
                    'ChineseAbstract': abs_cn,
                    'research_methods': methods,
                }

                if defense_dt:
                    thesis_info['defense_date'] = defense_dt.isoformat()
                if completion_dt:
                    thesis_info['completion_date'] = completion_dt.isoformat()
                if submission_dt:
                    thesis_info['submission_date'] = submission_dt.isoformat()
                if search_cutoff:
                    thesis_info['search_cutoff_date'] = search_cutoff.strftime('%Y%m%d')

                # ä»…ä¸­æ–‡æ£€ç´¢
                res = cnki_auto_search(
                    file_path=str(md_file),
                    languages=['Chinese'],
                    existing_thesis_info=thesis_info,
                )

                papers_by_lang = res.get('papers_by_lang', {}) if res else {}
                click.echo(f"  Chinese: Top papers = {len(papers_by_lang.get('Chinese', []))}")
                success += 1
            except Exception as e:
                failed += 1
                click.echo(f"âŒ å¤±è´¥: {e}")

        click.echo(f"\nğŸ‰ å®Œæˆã€‚æˆåŠŸ {success}ï¼Œå¤±è´¥ {failed}")

    except Exception as e:
        click.echo(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", err=True)

@cli.command()
@click.argument('files', nargs=-1, required=True)
@click.option('--output-dir', '-o', default=None,
              help='è¾“å‡ºç›®å½• (é»˜è®¤ä½¿ç”¨é…ç½®çš„è¾“å‡ºç›®å½•)')
def literature_analysis(files, output_dir):
    """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†ææŠ¥å‘Š
    
    ä»…ç”Ÿæˆæ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šï¼Œéœ€è¦å·²æœ‰çš„è®ºæ–‡æå–ä¿¡æ¯å’Œæ–‡çŒ®æ£€ç´¢ç»“æœã€‚
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .docx (Wordæ–‡æ¡£)
    
    æ³¨æ„: æš‚ä¸æ”¯æŒPDFæ ¼å¼æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨Wordæ–‡æ¡£æ ¼å¼ã€‚
    """
    # éªŒè¯æ‰€æœ‰æ–‡ä»¶æ ¼å¼
    for file_path in files:
        validate_file_format(file_path)
        
    try:
        config_mgr = get_config_manager()
        
        if output_dir:
            output_path = Path(output_dir)
        else:
            output_path = Path(config_mgr.get_output_dir())
        
        output_path.mkdir(parents=True, exist_ok=True)
        
        click.echo(f"ğŸ“Š å¼€å§‹ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶:")
        
        for file_path in files:
            click.echo(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(file_path).exists():
                click.echo(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            base_name = Path(file_path).stem
            
            # ä¼˜å…ˆåŠ è½½ä¸“å®¶ç‰ˆè®ºæ–‡æå–ä¿¡æ¯
            pro_extracted_info_file = output_path / f"{base_name}_pro_extracted_info.json"
            standard_extracted_info_file = output_path / f"{base_name}_extracted_info.json"
            
            thesis_extracted_info = None
            if pro_extracted_info_file.exists():
                try:
                    with open(pro_extracted_info_file, 'r', encoding='utf-8') as f:
                        extracted_data = json.load(f)
                        thesis_extracted_info = extracted_data.get('extracted_info', {})
                        click.echo(f"âœ… åŠ è½½ä¸“å®¶ç‰ˆè®ºæ–‡ä¿¡æ¯: {pro_extracted_info_file}")
                except Exception as e:
                    click.echo(f"âš ï¸ è¯»å–ä¸“å®¶ç‰ˆæ–‡ä»¶å¤±è´¥: {e}")
            
            if not thesis_extracted_info and standard_extracted_info_file.exists():
                try:
                    with open(standard_extracted_info_file, 'r', encoding='utf-8') as f:
                        extracted_data = json.load(f)
                        thesis_extracted_info = extracted_data.get('extracted_info', {})
                        click.echo(f"âœ… åŠ è½½æ ‡å‡†ç‰ˆè®ºæ–‡ä¿¡æ¯: {standard_extracted_info_file}")
                except Exception as e:
                    click.echo(f"âš ï¸ è¯»å–æ ‡å‡†ç‰ˆæ–‡ä»¶å¤±è´¥: {e}")
            
            if not thesis_extracted_info:
                click.echo(f"âŒ æ‰¾ä¸åˆ°è®ºæ–‡æå–ä¿¡æ¯æ–‡ä»¶")
                click.echo("   è¯·å…ˆè¿è¡Œ 'thesis-eval evaluate' æˆ– 'thesis-eval extract' ç”Ÿæˆæå–ä¿¡æ¯")
                continue
            
            # åŠ è½½æ–‡çŒ®æ•°æ®
            papers_by_lang = {}
            
            # æ£€æŸ¥ä¸­æ–‡æ–‡çŒ®
            chinese_file = output_path / f"{base_name}_relevant_papers_Chinese.json"
            if chinese_file.exists():
                with open(chinese_file, 'r', encoding='utf-8') as f:
                    chinese_papers = json.load(f)
                    papers_by_lang['Chinese'] = chinese_papers
                    click.echo(f"âœ… åŠ è½½ä¸­æ–‡æ–‡çŒ®: {len(chinese_papers)} ç¯‡")
            
            # æ£€æŸ¥è‹±æ–‡æ–‡çŒ®
            english_file = output_path / f"{base_name}_relevant_papers_English.json"
            if english_file.exists():
                with open(english_file, 'r', encoding='utf-8') as f:
                    english_papers = json.load(f)
                    papers_by_lang['English'] = english_papers
                    click.echo(f"âœ… åŠ è½½è‹±æ–‡æ–‡çŒ®: {len(english_papers)} ç¯‡")
            
            if not papers_by_lang:
                click.echo(f"âŒ æ‰¾ä¸åˆ°æ–‡çŒ®æ•°æ®æ–‡ä»¶")
                click.echo("   è¯·å…ˆè¿è¡Œ 'thesis-eval evaluate' ç”Ÿæˆæ–‡çŒ®æ£€ç´¢ç»“æœ")
                continue
            
            # ç”Ÿæˆæ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Š
            try:
                from .literature_review_analyzer import LiteratureReviewAnalyzer
                
                analyzer = LiteratureReviewAnalyzer()
                report_file = analyzer.analyze_literature_review(
                    str(file_path),
                    thesis_extracted_info,
                    papers_by_lang,
                    str(output_path)
                )
                click.echo(f"âœ… æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
                
            except Exception as e:
                click.echo(f"âŒ ç”Ÿæˆæ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
        
        click.echo(f"\nğŸ‰ æ–‡çŒ®ç»¼è¿°åˆ†æå®Œæˆ!")
        
    except Exception as e:
        click.echo(f"âŒ å¤„ç†å¤±è´¥: {e}", err=True)


@cli.command()
@click.option('--info', is_flag=True, default=False,
              help='æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯')
@click.option('--clear', is_flag=True, default=False,
              help='æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç¼“å­˜')
@click.option('--clear-file', type=str, default=None,
              help='æ¸…é™¤æŒ‡å®šæ–‡ä»¶çš„ç¼“å­˜')
def cache(info, clear, clear_file):
    """ç®¡ç†æ–‡æ¡£ç¼“å­˜"""
    from .extract_sections_with_ai import get_document_cache
    
    cache_manager = get_document_cache()
    
    if clear:
        click.echo("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ–‡æ¡£ç¼“å­˜...")
        success = cache_manager.clear_cache()
        if success:
            click.echo("âœ… æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")
        else:
            click.echo("âŒ æ¸…é™¤ç¼“å­˜å¤±è´¥", err=True)
    
    elif clear_file:
        click.echo(f"ğŸ—‘ï¸ æ¸…é™¤æ–‡ä»¶ç¼“å­˜: {clear_file}")
        if not Path(clear_file).exists():
            click.echo(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {clear_file}", err=True)
            return
            
        success = cache_manager.clear_cache(clear_file)
        if success:
            click.echo("âœ… æ–‡ä»¶ç¼“å­˜å·²æ¸…é™¤")
        else:
            click.echo("âŒ æ¸…é™¤æ–‡ä»¶ç¼“å­˜å¤±è´¥", err=True)
    
    elif info:
        click.echo("ğŸ“Š æ–‡æ¡£ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯:")
        cache_info = cache_manager.get_cache_info()
        
        if cache_info:
            click.echo(f"   ç¼“å­˜ç›®å½•: {cache_info['cache_dir']}")
            click.echo(f"   å·²ç¼“å­˜æ–‡ä»¶: {cache_info['cached_files']} ä¸ª")
            click.echo(f"   å…ƒæ•°æ®æ–‡ä»¶: {cache_info['metadata_files']} ä¸ª")
            click.echo(f"   æ€»å¤§å°: {cache_info['total_size_mb']} MB ({cache_info['total_size_bytes']:,} å­—èŠ‚)")
            
            if cache_info['cached_files'] > 0:
                click.echo("\nğŸ“ ç¼“å­˜æ–‡ä»¶åˆ—è¡¨:")
                cache_dir = Path(cache_info['cache_dir'])
                for md_file in sorted(cache_dir.glob("*.md")):
                    size = md_file.stat().st_size
                    click.echo(f"   ğŸ“„ {md_file.name} ({size:,} å­—èŠ‚)")
        else:
            click.echo("âŒ æ— æ³•è·å–ç¼“å­˜ä¿¡æ¯")
    
    else:
        click.echo("ğŸ’¾ æ–‡æ¡£ç¼“å­˜ç®¡ç†")
        click.echo("ä½¿ç”¨ --info æŸ¥çœ‹ç¼“å­˜ä¿¡æ¯")
        click.echo("ä½¿ç”¨ --clear æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
        click.echo("ä½¿ç”¨ --clear-file <æ–‡ä»¶è·¯å¾„> æ¸…é™¤æŒ‡å®šæ–‡ä»¶ç¼“å­˜")


if __name__ == '__main__':
    cli()
