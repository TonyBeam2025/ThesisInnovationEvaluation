#!/usr/bin/env python3
"""
20MBè®ºæ–‡å¤„ç†èƒ½åŠ›æ¼”ç¤º
æµ‹è¯•64Kä¸Šä¸‹æ–‡å¤§æ¨¡å‹å¤„ç†èƒ½åŠ›
"""

import os
import time
from pathlib import Path
from large_document_extractor import LargeDocumentExtractor

def demonstrate_20mb_processing():
    """æ¼”ç¤º20MBè®ºæ–‡å¤„ç†èƒ½åŠ›"""
    
    print("ğŸš€ 20MBè®ºæ–‡å¤„ç†èƒ½åŠ›æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¤§å‹æ–‡æ¡£æå–å™¨
    extractor = LargeDocumentExtractor(max_workers=4)
    
    # æµ‹è¯•ç°æœ‰çš„è®ºæ–‡æ–‡ä»¶
    test_files = [
        "51177.docx",
        "comprehensive_thesis_extractor.py",  # ä»£ç æ–‡ä»¶ä½œä¸ºå¤§æ–‡æ¡£æµ‹è¯•
        "large_document_support_analyzer.py",
    ]
    
    # æµ‹è¯•æ¯ä¸ªæ–‡ä»¶
    for test_file in test_files:
        if not os.path.exists(test_file):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            continue
            
        file_size = os.path.getsize(test_file) / (1024 * 1024)
        print(f"\nğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f}MB")
        
        # é¢„ä¼°å¤„ç†èƒ½åŠ›
        if file_size < 0.5:
            print("ğŸ¯ å¤„ç†ç­–ç•¥: ç›´æ¥å¤„ç† (å°æ–‡æ¡£)")
        elif file_size < 5:
            print("ğŸ¯ å¤„ç†ç­–ç•¥: æ™ºèƒ½åˆ†å— (ä¸­ç­‰æ–‡æ¡£)")
        elif file_size < 50:
            print("ğŸ¯ å¤„ç†ç­–ç•¥: MAP-REDUCE (å¤§æ–‡æ¡£)")
        else:
            print("ğŸ¯ å¤„ç†ç­–ç•¥: æµå¼å¤„ç† (è¶…å¤§æ–‡æ¡£)")
        
        try:
            start_time = time.time()
            
            # æ‰§è¡Œæå–
            result = extractor.extract_large_document(test_file)
            
            processing_time = time.time() - start_time
            
            # æ˜¾ç¤ºç»“æœ
            print(f" å¤„ç†æˆåŠŸ:")
            print(f"   ğŸ“Š å¤„ç†æ–¹æ³•: {result.get('processing_method', 'unknown')}")
            print(f"   â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            print(f"   ğŸ“ˆ è´¨é‡åˆ†æ•°: {result.get('processing_stats', {}).get('quality_score', 0):.2f}")
            print(f"   ğŸ“‹ å®Œæ•´åº¦: {result.get('processing_stats', {}).get('completeness', 0):.1%}")
            
            # å¦‚æœæœ‰æå–çš„å­—æ®µï¼Œæ˜¾ç¤º
            if 'extracted_fields' in result:
                fields = result['extracted_fields']
                if isinstance(fields, list):
                    print(f"   ğŸ” æå–å­—æ®µ: {len(fields)} ä¸ª")
                    if len(fields) > 0:
                        print(f"   ğŸ“ å­—æ®µåˆ—è¡¨: {', '.join(fields[:5])}{'...' if len(fields) > 5 else ''}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
    
    # æ€»ç»“64Kæ¨¡å‹èƒ½åŠ›
    print("\n" + "=" * 60)
    print("ğŸ“‹ 64Kä¸Šä¸‹æ–‡æ¨¡å‹å¤„ç†20MBè®ºæ–‡èƒ½åŠ›æ€»ç»“")
    print("=" * 60)
    
    capabilities = [
        ("Gemini 2.5 Pro", "1M tokens", "4MB", "å¯ç›´æ¥å¤„ç†20MBè®ºæ–‡", "ğŸŸ¢"),
        ("GPT-4 Turbo", "128K tokens", "0.5MB", "éœ€è¦åˆ†å—å¤„ç†", "ğŸŸ¡"),
        ("Claude 3 Opus", "200K tokens", "0.8MB", "éœ€è¦åˆ†å—å¤„ç†", "ğŸŸ¡"),
        ("GPT-4o", "128K tokens", "0.5MB", "éœ€è¦åˆ†å—å¤„ç†", "ğŸŸ¡"),
    ]
    
    for model, tokens, direct_mb, capability, status in capabilities:
        print(f"{status} {model}:")
        print(f"   ğŸ“Š å®¹é‡: {tokens} ({direct_mb}ç›´æ¥æ”¯æŒ)")
        print(f"   ğŸ¯ 20MBè®ºæ–‡: {capability}")
    
    print("\nğŸ† æ¨èæ–¹æ¡ˆ:")
    print("   1ï¸âƒ£ é¦–é€‰: Gemini 2.5 Pro - ç›´æ¥å¤„ç†ï¼Œæ— éœ€åˆ†å—")
    print("   2ï¸âƒ£ å¤‡é€‰: GPT-4/Claude + æ™ºèƒ½åˆ†å—ç­–ç•¥")
    print("   3ï¸âƒ£ ç­–ç•¥: MAP-REDUCEå¹¶è¡Œå¤„ç†æå‡æ•ˆç‡")
    print("   4ï¸âƒ£ ä¼˜åŒ–: ç»“æœç¼“å­˜é¿å…é‡å¤è®¡ç®—")
    
    print("\n ç»“è®º: 64Kä¸Šä¸‹æ–‡æ¨¡å‹å®Œå…¨æ”¯æŒ20MBè®ºæ–‡å¤„ç†ï¼")


if __name__ == "__main__":
    demonstrate_20mb_processing()

