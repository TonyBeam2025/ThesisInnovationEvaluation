#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ™ºèƒ½ç›®å½•æŠ½å–å™¨ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨AITocExtractoræŠ½å–ä¸åŒæ ¼å¼è®ºæ–‡çš„ç›®å½•
"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from thesis_toc_extractor import AITocExtractor

def demo_usage():
    """æ¼”ç¤ºAIç›®å½•æŠ½å–å™¨çš„ä½¿ç”¨æ–¹æ³•"""
    
    print("ğŸš€ AIæ™ºèƒ½å­¦ä½è®ºæ–‡ç›®å½•æŠ½å–å™¨æ¼”ç¤º\n")
    
    # åˆ›å»ºæŠ½å–å™¨å®ä¾‹
    extractor = AITocExtractor()
    
    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    test_files = [
        "cache/documents/51177_b6ac1c475108811bd4a31a6ebcd397df.md",
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•æ–‡ä»¶
        # "path/to/your/thesis.docx",
        # "path/to/your/thesis.md"
    ]
    
    for file_path in test_files:
        file_path = Path(file_path)
        if not file_path.exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
            continue
        
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†: {file_path.name}")
        print("=" * 60)
        
        try:
            # æŠ½å–ç›®å½•
            toc = extractor.extract_toc(str(file_path))
            
            # æ˜¾ç¤ºæŠ½å–ç»“æœ
            extractor.print_toc(toc)
            
            # ä¿å­˜åˆ°å¤šç§æ ¼å¼
            output_base = file_path.stem + "_extracted_toc"
            
            # JSONæ ¼å¼ - é€‚åˆç¨‹åºå¤„ç†
            json_file = f"{output_base}.json"
            extractor.save_toc(toc, json_file, 'json')
            print(f" JSONæ ¼å¼å·²ä¿å­˜: {json_file}")
            
            # Markdownæ ¼å¼ - é€‚åˆé˜…è¯»å’Œå±•ç¤º
            md_file = f"{output_base}.md"
            extractor.save_toc(toc, md_file, 'markdown')
            print(f" Markdownæ ¼å¼å·²ä¿å­˜: {md_file}")
            
            # æ–‡æœ¬æ ¼å¼ - é€‚åˆç®€å•æŸ¥çœ‹
            txt_file = f"{output_base}.txt"
            extractor.save_toc(toc, txt_file, 'txt')
            print(f" æ–‡æœ¬æ ¼å¼å·²ä¿å­˜: {txt_file}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š æŠ½å–ç»Ÿè®¡:")
            print(f"   - æ€»æ¡ç›®æ•°: {toc.total_entries}")
            print(f"   - æœ€å¤§å±‚çº§: {toc.max_level}")
            print(f"   - ç½®ä¿¡åº¦: {toc.confidence_score:.2f}")
            
            # å„å±‚çº§ç»Ÿè®¡
            level_stats = {}
            for entry in toc.entries:
                level_stats[entry.level] = level_stats.get(entry.level, 0) + 1
            
            print(f"   - å±‚çº§åˆ†å¸ƒ:")
            for level in sorted(level_stats.keys()):
                print(f"     ç¬¬{level}çº§: {level_stats[level]}ä¸ª")
            
            print("\n" + "="*60 + "\n")
            
        except Exception as e:
            print(f"âŒ æŠ½å–å¤±è´¥: {e}")
            print("\n" + "="*60 + "\n")

def analyze_extraction_quality():
    """åˆ†ææŠ½å–è´¨é‡"""
    
    print("ğŸ” AIæŠ½å–è´¨é‡åˆ†æ\n")
    
    extractor = AITocExtractor()
    test_file = "cache/documents/51177_b6ac1c475108811bd4a31a6ebcd397df.md"
    
    if not Path(test_file).exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return
    
    toc = extractor.extract_toc(test_file)
    
    print("ğŸ“‹ è´¨é‡åˆ†ææŠ¥å‘Š:")
    print("-" * 40)
    
    # 1. ç½®ä¿¡åº¦åˆ†æ
    high_conf = [e for e in toc.entries if e.confidence >= 0.9]
    medium_conf = [e for e in toc.entries if 0.7 <= e.confidence < 0.9]
    low_conf = [e for e in toc.entries if e.confidence < 0.7]
    
    print(f"ğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    print(f"   é«˜ç½®ä¿¡åº¦ (â‰¥0.9): {len(high_conf)} ä¸ª ({len(high_conf)/len(toc.entries)*100:.1f}%)")
    print(f"   ä¸­ç½®ä¿¡åº¦ (0.7-0.9): {len(medium_conf)} ä¸ª ({len(medium_conf)/len(toc.entries)*100:.1f}%)")
    print(f"   ä½ç½®ä¿¡åº¦ (<0.7): {len(low_conf)} ä¸ª ({len(low_conf)/len(toc.entries)*100:.1f}%)")
    
    # 2. ç« èŠ‚ç»“æ„åˆ†æ
    print(f"\nğŸ“š ç« èŠ‚ç»“æ„åˆ†æ:")
    main_chapters = [e for e in toc.entries if e.level == 1]
    print(f"   ä¸»ç« èŠ‚æ•°: {len(main_chapters)}")
    
    for chapter in main_chapters[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä¸»ç« èŠ‚
        sub_count = len([e for e in toc.entries if e.number.startswith(chapter.number.replace('ç¬¬', '').replace('ç« ', '')) and e.level > 1])
        print(f"   - {chapter.number} {chapter.title}: {sub_count} ä¸ªå­ç« èŠ‚")
    
    # 3. é—®é¢˜è¯†åˆ«
    print(f"\nâš ï¸ æ½œåœ¨é—®é¢˜:")
    issues = []
    
    # æ£€æŸ¥ç¼ºå¤±æ ‡é¢˜
    missing_titles = [e for e in toc.entries if not e.title.strip()]
    if missing_titles:
        issues.append(f"ç¼ºå¤±æ ‡é¢˜: {len(missing_titles)} ä¸ª")
    
    # æ£€æŸ¥å¼‚å¸¸ç¼–å·
    unusual_numbers = [e for e in toc.entries if e.number and not any(c.isdigit() for c in e.number)]
    if unusual_numbers:
        issues.append(f"å¼‚å¸¸ç¼–å·: {len(unusual_numbers)} ä¸ª")
    
    if issues:
        for issue in issues:
            print(f"   - {issue}")
    else:
        print(f"    æœªå‘ç°æ˜æ˜¾é—®é¢˜")
    
    print(f"\nğŸ“ˆ æ€»ä½“è¯„ä»·: {'ä¼˜ç§€' if toc.confidence_score >= 0.85 else 'è‰¯å¥½' if toc.confidence_score >= 0.7 else 'éœ€è¦æ”¹è¿›'}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ“ AIæ™ºèƒ½å­¦ä½è®ºæ–‡ç›®å½•æŠ½å–å™¨")
    print("=" * 50)
    print("åŠŸèƒ½ç‰¹ç‚¹:")
    print("â€¢ æ”¯æŒWordæ–‡æ¡£(.docx)å’ŒMarkdownæ–‡æ¡£(.md)")
    print("â€¢ AIæ™ºèƒ½è¯†åˆ«å¤æ‚çš„ç« èŠ‚ç»“æ„")
    print("â€¢ å¤šç§è¾“å‡ºæ ¼å¼(JSON/Markdown/Text)")
    print("â€¢ ç½®ä¿¡åº¦è¯„ä¼°å’Œè´¨é‡åˆ†æ")
    print("â€¢ æ”¯æŒä¸­è‹±æ–‡æ··åˆç›®å½•")
    print("=" * 50)
    print()
    
    # æ¼”ç¤ºä½¿ç”¨
    demo_usage()
    
    # è´¨é‡åˆ†æ
    analyze_extraction_quality()
    
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("1. å°†ä½ çš„è®ºæ–‡æ–‡ä»¶æ”¾å…¥é€‚å½“ç›®å½•")
    print("2. ä¿®æ”¹test_filesåˆ—è¡¨ä¸­çš„æ–‡ä»¶è·¯å¾„")
    print("3. è¿è¡Œç¨‹åºè‡ªåŠ¨æŠ½å–ç›®å½•")
    print("4. æŸ¥çœ‹ç”Ÿæˆçš„JSON/Markdown/Textæ–‡ä»¶")

if __name__ == "__main__":
    main()

