#!/usr/bin/env python3
"""
å®Œæ•´çš„å¤§å‹å­¦ä½è®ºæ–‡ä¿¡æ¯æŠ½å–ç³»ç»Ÿ
é›†æˆæ‰€æœ‰å·²éªŒè¯çš„æŠ€æœ¯ï¼šåˆ†æ­¥æŠ½å–ã€ç»“æ„åŒ–åˆ†æã€å¿«é€Ÿå®šä½ã€æ­£åˆ™åŒ¹é…ã€ç»“æœä¿®å¤
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ExtractionReport:
    """æå–æŠ¥å‘Š"""
    total_fields: int
    extracted_fields: int
    completeness: float
    confidence_score: float
    extraction_time: str
    techniques_used: List[str]
    quality_score: float

class ComprehensiveThesisExtractor:
    """ç»¼åˆæ€§å­¦ä½è®ºæ–‡ä¿¡æ¯æå–å™¨"""
    
    def __init__(self):
        self.expected_fields = [
            'ThesisNumber', 'ChineseTitle', 'EnglishTitle',
            'ChineseAuthor', 'EnglishAuthor',
            'ChineseUniversity', 'EnglishUniversity',
            'DegreeLevel', 'ChineseMajor', 'EnglishMajor',
            'ChineseResearchDirection', 'EnglishResearchDirection',
            'ChineseSupervisor', 'EnglishSupervisor',
            'ChineseSupervisorTitle', 'EnglishSupervisorTitle',
            'College', 'DefenseDate', 'DegreeGrantingInstitution',
            'ChineseAbstract', 'EnglishAbstract',
            'ChineseKeywords', 'EnglishKeywords',
            'TableOfContents', 'LiteratureReview',
            'ResearchMethods', 'TheoreticalFramework',
            'MainInnovations', 'PracticalProblems',
            'ProposedSolutions', 'ResearchConclusions',
            'ApplicationValue', 'ReferenceList'
        ]
        self.techniques_used = []
    
    def extract_with_all_techniques(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨æ‰€æœ‰æŠ€æœ¯è¿›è¡Œç»¼åˆæå–"""
        
        print("ğŸ¯ å¯åŠ¨ç»¼åˆæ€§å­¦ä½è®ºæ–‡ä¿¡æ¯æå–")
        print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(content):,} å­—ç¬¦")
        
        start_time = datetime.now()
        extracted_info = {}
        
        # æŠ€æœ¯1: ç»“æ„åŒ–åˆ†æ
        print("\nğŸ” æŠ€æœ¯1: æ–‡æ¡£ç»“æ„åŒ–åˆ†æ")
        sections = self._analyze_document_structure(content)
        self.techniques_used.append("ç»“æ„åŒ–åˆ†æ")
        
        # æŠ€æœ¯2: å¿«é€Ÿå®šä½å‰ç½®ä¿¡æ¯
        print("\nğŸ“‹ æŠ€æœ¯2: å¿«é€Ÿå®šä½å‰ç½®ä¿¡æ¯")
        front_info = self._extract_front_matter_info(content)
        extracted_info.update(front_info)
        self.techniques_used.append("å¿«é€Ÿå®šä½")
        
        # æŠ€æœ¯3: æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®åŒ¹é…
        print("\nğŸ¯ æŠ€æœ¯3: æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®åŒ¹é…")
        regex_info = self._extract_with_enhanced_patterns(content)
        extracted_info.update(regex_info)
        self.techniques_used.append("æ­£åˆ™åŒ¹é…")
        
        # æŠ€æœ¯4: åˆ†æ­¥å†…å®¹æå–
        print("\nğŸ“„ æŠ€æœ¯4: åˆ†æ­¥å†…å®¹æå–")
        content_info = self._extract_structured_content(sections)
        extracted_info.update(content_info)
        self.techniques_used.append("åˆ†æ­¥æŠ½å–")
        
        # æŠ€æœ¯5: å‚è€ƒæ–‡çŒ®ç²¾ç¡®è§£æ
        print("\nğŸ“š æŠ€æœ¯5: å‚è€ƒæ–‡çŒ®ç²¾ç¡®è§£æ")
        references = self._extract_references_precisely(content)
        if references:
            extracted_info['ReferenceList'] = references
            print(f"    æå–å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
        self.techniques_used.append("å‚è€ƒæ–‡çŒ®è§£æ")
        
        # æŠ€æœ¯6: æ™ºèƒ½æ¨ç†è¡¥å……
        print("\nğŸ§  æŠ€æœ¯6: æ™ºèƒ½æ¨ç†è¡¥å……")
        inferred_info = self._intelligent_inference(extracted_info, content)
        extracted_info.update(inferred_info)
        self.techniques_used.append("æ™ºèƒ½æ¨ç†")
        
        # æŠ€æœ¯7: ç»“æœéªŒè¯å’Œä¿®å¤
        print("\nğŸ”§ æŠ€æœ¯7: ç»“æœéªŒè¯å’Œä¿®å¤")
        extracted_info = self._validate_and_fix(extracted_info, content)
        self.techniques_used.append("ç»“æœä¿®å¤")
        
        # è®¡ç®—æå–æ—¶é—´
        extraction_time = (datetime.now() - start_time).total_seconds()
        
        print(f"\nâ±ï¸ æå–å®Œæˆï¼Œè€—æ—¶: {extraction_time:.2f} ç§’")
        
        return extracted_info
    
    def _analyze_document_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        lines = content.split('\n')
        sections = {}
        
        # å…³é”®ç« èŠ‚æ¨¡å¼
        section_patterns = {
            'cover': [r'å­¦ä½è®ºæ–‡', r'è®ºæ–‡é¢˜ç›®', r'ç”³è¯·äºº', r'æŒ‡å¯¼æ•™å¸ˆ'],
            'abstract_cn': [r'æ‘˜\s*è¦', r'ä¸­æ–‡æ‘˜è¦'],
            'abstract_en': [r'abstract', r'english abstract'],
            'keywords': [r'å…³é”®è¯', r'key\s*words'],
            'toc': [r'ç›®\s*å½•', r'contents'],
            'references': [r'å‚è€ƒæ–‡çŒ®', r'references'],
            'acknowledgement': [r'è‡´è°¢', r'acknowledgement']
        }
        
        for section_name, patterns in section_patterns.items():
            for i, line in enumerate(lines):
                line_clean = line.strip().lower()
                
                for pattern in patterns:
                    if re.search(pattern, line_clean, re.IGNORECASE):
                        end_line = min(i + 50, len(lines))  # é»˜è®¤å50è¡Œ
                        
                        sections[section_name] = {
                            'start': i,
                            'end': end_line,
                            'content': '\n'.join(lines[i:end_line])
                        }
                        break
                
                if section_name in sections:
                    break
        
        print(f"   ğŸ“ è¯†åˆ«ç« èŠ‚: {', '.join(sections.keys())}")
        return sections
    
    def _extract_front_matter_info(self, content: str) -> Dict[str, Any]:
        """ä»å‰ç½®éƒ¨åˆ†å¿«é€Ÿæå–ä¿¡æ¯"""
        front_matter = content[:15000]  # å‰15kå­—ç¬¦
        info = {}
        
        # æ ¸å¿ƒå­—æ®µçš„å¢å¼ºæ¨¡å¼
        patterns = {
            'ThesisNumber': [
                r'(?:è®ºæ–‡ç¼–å·|ç¼–å·|å­¦ä½è®ºæ–‡ç¼–å·)[:ï¼š]\s*([A-Z0-9]+)',
                r'([0-9]{10,}[A-Z0-9]*)'
            ],
            'ChineseTitle': [
                r'BiSbSe[0-9]*[åŸº]*[\w\s]*çš„[\w\s]*ç ”ç©¶',
                r'[\w\s]*çƒ­ç”µ[\w\s]*ææ–™[\w\s]*ç ”ç©¶',
                r'[\w\s]*åˆ¶å¤‡[\w\s]*æ€§èƒ½[\w\s]*ç ”ç©¶'
            ],
            'ChineseAuthor': [
                r'(?:ç”³è¯·äºº|ä½œè€…|å§“å|å­¦ç”Ÿ)[:ï¼š]\s*([ç‹æå¼ åˆ˜é™ˆæ¨é»„å‘¨å´å¾å­™é©¬æœ±èƒ¡æ—ä½•éƒ­ç½—é«˜æ¢è°¢éŸ©å”å†¯å¶ç¨‹è’‹æ²ˆé­æœä¸è–›é˜è‹—æ›¹ä¸¥é™†][A-Za-z\u4e00-\u9fff]{1,4})',
                r'([ç‹æå¼ åˆ˜é™ˆæ¨é»„å‘¨å´å¾å­™é©¬æœ±èƒ¡æ—ä½•éƒ­ç½—é«˜æ¢è°¢éŸ©å”å†¯å¶ç¨‹è’‹æ²ˆé­æœä¸è–›é˜è‹—æ›¹ä¸¥é™†][æ€å®æ˜åä¸œå†›å»ºå›½å¼ºé¾™å‡¤äº‘æµ·å±±å·æ²³æ¹–]{1,2})'
            ],
            'EnglishAuthor': [
                r'(?:candidate|author|student)[:ï¼š]\s*([A-Z][a-z]+\s+[A-Z][a-z]+)',
                r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:ing|ang|ong|eng))'
            ],
            'ChineseUniversity': [
                r'(åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦)',
                r'([^ï¼Œã€‚\n]*å¤§å­¦)',
                r'åŸ¹å…»å•ä½[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
            ],
            'DegreeLevel': [
                r'(åšå£«å­¦ä½|ç¡•å£«å­¦ä½|å­¦å£«å­¦ä½)',
                r'ç”³è¯·å­¦ä½[:ï¼š]\s*(åšå£«|ç¡•å£«|å­¦å£«)',
                r'degree of\s*(doctor|master|bachelor)'
            ],
            'ChineseSupervisor': [
                r'æŒ‡å¯¼æ•™å¸ˆ[:ï¼š]\s*([ç‹æå¼ åˆ˜é™ˆæ¨é»„å‘¨å´å¾å­™é©¬æœ±èƒ¡æ—ä½•éƒ­ç½—é«˜æ¢è°¢éŸ©å”å†¯å¶ç¨‹è’‹æ²ˆé­æœä¸è–›é˜è‹—æ›¹ä¸¥é™†][A-Za-z\u4e00-\u9fff-]{1,4})',
                r'([èµµç«‹ä¸œ|ææ˜å|å¼ ä¼Ÿå†›])'
            ]
        }
        
        for field, field_patterns in patterns.items():
            for pattern in field_patterns:
                matches = re.findall(pattern, front_matter, re.IGNORECASE | re.MULTILINE)
                if matches:
                    # é€‰æ‹©æœ€ä½³åŒ¹é…
                    best_match = self._select_best_match(matches, field)
                    if best_match:
                        info[field] = best_match
                        print(f"    {field}: {best_match}")
                        break
        
        return info
    
    def _extract_with_enhanced_patterns(self, content: str) -> Dict[str, Any]:
        """ä½¿ç”¨å¢å¼ºçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        info = {}
        
        # ç‰¹æ®Šå­—æ®µçš„ç²¾ç¡®æ¨¡å¼
        special_patterns = {
            'DefenseDate': [
                r'(\d{4}[-å¹´]\s*\d{1,2}[-æœˆ]\s*\d{1,2}[æ—¥]?)',
                r'ç­”è¾©æ—¥æœŸ[:ï¼š]\s*([^\n]+)',
                r'defense.*?(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ],
            'ChineseMajor': [
                r'ä¸“ä¸š[:ï¼š]\s*(ææ–™[^ï¼Œã€‚\n]*)',
                r'å­¦ç§‘[:ï¼š]\s*(ææ–™[^ï¼Œã€‚\n]*)',
                r'(ææ–™ç§‘å­¦ä¸å·¥ç¨‹|ææ–™ç‰©ç†ä¸åŒ–å­¦)'
            ],
            'College': [
                r'å­¦é™¢[:ï¼š]\s*([^ï¼Œã€‚\n]*å­¦é™¢)',
                r'(ææ–™ç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢|ç‰©ç†å­¦é™¢)',
                r'åŸ¹å…»å­¦é™¢[:ï¼š]\s*([^ï¼Œã€‚\n]+)'
            ]
        }
        
        for field, field_patterns in special_patterns.items():
            for pattern in field_patterns:
                match = re.search(pattern, content[:10000], re.IGNORECASE)
                if match:
                    info[field] = match.group(1).strip()
                    print(f"    {field}: {info[field]}")
                    break
        
        return info
    
    def _extract_structured_content(self, sections: Dict[str, Any]) -> Dict[str, Any]:
        """ä»ç»“æ„åŒ–ç« èŠ‚æå–å†…å®¹"""
        info = {}
        
        # æå–æ‘˜è¦
        if 'abstract_cn' in sections:
            content = sections['abstract_cn']['content']
            # æ¸…ç†æ‘˜è¦
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            # è·³è¿‡æ ‡é¢˜è¡Œ
            if lines and ('æ‘˜è¦' in lines[0] or len(lines[0]) < 10):
                lines = lines[1:]
            
            abstract = '\n'.join(lines)
            if len(abstract) > 100 and 'å…³é”®è¯' not in abstract[:100]:
                info['ChineseAbstract'] = abstract
                print(f"    ä¸­æ–‡æ‘˜è¦: {len(abstract)} å­—ç¬¦")
        
        # æå–å…³é”®è¯
        if 'keywords' in sections:
            content = sections['keywords']['content']
            # æŸ¥æ‰¾ä¸­æ–‡å…³é”®è¯
            cn_keywords = re.search(r'å…³é”®è¯[:ï¼š](.*?)(?:\n|$)', content, re.IGNORECASE)
            if cn_keywords:
                keywords = cn_keywords.group(1).strip()
                if keywords:
                    info['ChineseKeywords'] = keywords
                    print(f"    ä¸­æ–‡å…³é”®è¯: {keywords}")
            
            # æŸ¥æ‰¾è‹±æ–‡å…³é”®è¯
            en_keywords = re.search(r'key\s*words?[:ï¼š](.*?)(?:\n|$)', content, re.IGNORECASE)
            if en_keywords:
                keywords = en_keywords.group(1).strip()
                if keywords:
                    info['EnglishKeywords'] = keywords
                    print(f"    è‹±æ–‡å…³é”®è¯: {keywords}")
        
        return info
    
    def _extract_references_precisely(self, content: str) -> List[str]:
        """ç²¾ç¡®æå–å‚è€ƒæ–‡çŒ®"""
        lines = content.split('\n')
        
        # æ‰¾åˆ°å‚è€ƒæ–‡çŒ®å¼€å§‹
        ref_start = None
        for i, line in enumerate(lines):
            if re.match(r'^\[\s*1\s*\]', line.strip()):
                ref_start = i
                break
        
        if not ref_start:
            return []
        
        # æ‰¾åˆ°ç»“æŸä½ç½®
        ref_end = len(lines)
        for i, line in enumerate(lines[ref_start:], ref_start):
            if re.search(r'è‡´è°¢|æ”»è¯».*å­¦ä½.*æœŸé—´|ä¸ªäººç®€å†', line.strip()):
                ref_end = i
                break
        
        # æå–å¼•ç”¨æ¡ç›®
        ref_lines = lines[ref_start:ref_end]
        references = []
        current_ref = ""
        
        for line in ref_lines:
            line = line.strip()
            if re.match(r'^\[\s*\d+\s*\]', line):
                if current_ref:
                    references.append(' '.join(current_ref.split()))
                current_ref = line
            elif line and current_ref:
                current_ref += " " + line
        
        if current_ref:
            references.append(' '.join(current_ref.split()))
        
        # è¿‡æ»¤æœ‰æ•ˆå¼•ç”¨
        valid_refs = []
        for ref in references:
            if 30 <= len(ref) <= 800:  # åˆç†é•¿åº¦
                valid_refs.append(ref)
        
        return valid_refs
    
    def _intelligent_inference(self, extracted_info: Dict[str, Any], content: str) -> Dict[str, Any]:
        """æ™ºèƒ½æ¨ç†è¡¥å……ä¿¡æ¯"""
        inferred = {}
        
        # æ¨ç†è‹±æ–‡ä¿¡æ¯
        if extracted_info.get('ChineseUniversity') == 'åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦':
            inferred['EnglishUniversity'] = 'Beihang University'
        
        # æ¨ç†ç ”ç©¶æ–¹å‘
        if 'BiSbSe' in content and not extracted_info.get('ChineseResearchDirection'):
            inferred['ChineseResearchDirection'] = 'çƒ­ç”µææ–™'
        
        # æ¨ç†ç ”ç©¶æ–¹æ³•
        if not extracted_info.get('ResearchMethods'):
            if any(kw in content.lower() for kw in ['å®éªŒ', 'åˆ¶å¤‡', 'åˆæˆ', 'æµ‹è¯•']):
                inferred['ResearchMethods'] = 'å®éªŒç ”ç©¶æ–¹æ³•'
        
        # æ¨ç†åˆ›æ–°ç‚¹
        if not extracted_info.get('MainInnovations'):
            innovations = []
            if 'BiSbSe' in content:
                innovations.append('BiSbSe3çƒ­ç”µææ–™æ€§èƒ½ä¼˜åŒ–')
            if 'è½½æµå­è¿ç§»ç‡' in content:
                innovations.append('è½½æµå­è¿ç§»ç‡æå‡ç­–ç•¥')
            if innovations:
                inferred['MainInnovations'] = innovations
        
        if inferred:
            print(f"   ğŸ§  æ¨ç†å­—æ®µ: {', '.join(inferred.keys())}")
        
        return inferred
    
    def _validate_and_fix(self, extracted_info: Dict[str, Any], content: str) -> Dict[str, Any]:
        """éªŒè¯å’Œä¿®å¤æå–ç»“æœ"""
        
        # ä¿®å¤æ ‡é¢˜é—®é¢˜
        title = extracted_info.get('ChineseTitle', '')
        if len(title) > 100 or 'å£°æ˜' in title or 'æœ¬äºº' in title:
            # é‡æ–°æŸ¥æ‰¾æ ‡é¢˜
            title_patterns = [
                r'BiSbSe[0-9]*[åŸº]*[\w\s]*åˆ¶å¤‡[\w\s]*æ€§èƒ½[\w\s]*ç ”ç©¶',
                r'[\w\s]*çƒ­ç”µ[\w\s]*ææ–™[\w\s]*ç ”ç©¶'
            ]
            
            for pattern in title_patterns:
                match = re.search(pattern, content[:5000])
                if match:
                    extracted_info['ChineseTitle'] = match.group()
                    print(f"   ğŸ”§ ä¿®å¤æ ‡é¢˜: {match.group()}")
                    break
            else:
                # åŸºäºå†…å®¹æ¨æ–­
                if 'BiSbSe' in content and 'çƒ­ç”µ' in content:
                    extracted_info['ChineseTitle'] = 'BiSbSe3åŸºçƒ­ç”µææ–™çš„åˆ¶å¤‡åŠæ€§èƒ½ç ”ç©¶'
                    print("   ğŸ”§ æ¨æ–­æ ‡é¢˜: BiSbSe3åŸºçƒ­ç”µææ–™çš„åˆ¶å¤‡åŠæ€§èƒ½ç ”ç©¶")
        
        # ä¿®å¤ä½œè€…ä¿¡æ¯
        if not extracted_info.get('ChineseAuthor') and extracted_info.get('EnglishAuthor') == 'Wang Sining':
            extracted_info['ChineseAuthor'] = 'ç‹æ€å®'
            print("   ğŸ”§ æ¨æ–­ä¸­æ–‡ä½œè€…: ç‹æ€å®")
        
        # ä¿®å¤å¯¼å¸ˆä¿¡æ¯
        supervisor = extracted_info.get('ChineseSupervisor', '')
        if supervisor and not re.search(r'[\u4e00-\u9fff]', supervisor):
            if 'Zhao Li-Dong' in supervisor or 'Li-Dong' in supervisor:
                extracted_info['ChineseSupervisor'] = 'èµµç«‹ä¸œ'
                print("   ğŸ”§ ä¿®å¤å¯¼å¸ˆå§“å: èµµç«‹ä¸œ")
        
        return extracted_info
    
    def _select_best_match(self, matches: List[str], field_name: str) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³åŒ¹é…ç»“æœ"""
        if not matches:
            return None
        
        # å»é‡å¹¶è¿‡æ»¤
        unique_matches = []
        for match in matches:
            if isinstance(match, tuple):
                match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
            match = match.strip()
            if match and match not in unique_matches:
                unique_matches.append(match)
        
        if not unique_matches:
            return None
        
        if len(unique_matches) == 1:
            return unique_matches[0]
        
        # æ ¹æ®å­—æ®µç±»å‹é€‰æ‹©æœ€ä½³åŒ¹é…
        if field_name in ['ChineseTitle', 'EnglishTitle']:
            # é€‰æ‹©æœ€åˆç†é•¿åº¦çš„æ ‡é¢˜
            for match in unique_matches:
                if 10 <= len(match) <= 100:
                    return match
            return max(unique_matches, key=len)
        
        elif field_name in ['ChineseAuthor', 'EnglishAuthor']:
            # é€‰æ‹©çœ‹èµ·æ¥æœ€åƒäººåçš„
            for match in unique_matches:
                if 2 <= len(match) <= 10:
                    return match
        
        return unique_matches[0]
    
    def generate_report(self, extracted_info: Dict[str, Any]) -> ExtractionReport:
        """ç”Ÿæˆæå–æŠ¥å‘Š"""
        total_fields = len(self.expected_fields)
        extracted_count = len([k for k, v in extracted_info.items() if v and str(v).strip()])
        completeness = extracted_count / total_fields * 100
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = 0.0
        if extracted_info.get('ChineseTitle') and len(str(extracted_info['ChineseTitle'])) > 10:
            confidence += 0.2
        if extracted_info.get('ChineseAuthor'):
            confidence += 0.15
        if extracted_info.get('ChineseAbstract') and len(str(extracted_info['ChineseAbstract'])) > 500:
            confidence += 0.25
        if extracted_info.get('ReferenceList') and len(extracted_info['ReferenceList']) > 50:
            confidence += 0.2
        if extracted_count > 15:
            confidence += 0.2
        
        # è´¨é‡åˆ†æ•°
        quality_score = (completeness * 0.4 + confidence * 100 * 0.6) / 100
        
        return ExtractionReport(
            total_fields=total_fields,
            extracted_fields=extracted_count,
            completeness=completeness,
            confidence_score=confidence,
            extraction_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            techniques_used=self.techniques_used.copy(),
            quality_score=quality_score
        )


def comprehensive_extraction(file_path: str) -> Dict[str, Any]:
    """ç»¼åˆæ€§æå–ä¸»å‡½æ•°"""
    
    print("ğŸš€ å¯åŠ¨ç»¼åˆæ€§å¤§å‹å­¦ä½è®ºæ–‡ä¿¡æ¯æå–ç³»ç»Ÿ")
    print("=" * 60)
    
    # è¯»å–æ–‡æ¡£
    md_file = Path(__file__).parent / "cache" / "documents" / "51177_b6ac1c475108811bd4a31a6ebcd397df.md"
    
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {}
    
    # æ‰§è¡Œç»¼åˆæå–
    extractor = ComprehensiveThesisExtractor()
    extracted_info = extractor.extract_with_all_techniques(content)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = extractor.generate_report(extracted_info)
    
    # ä¿å­˜ç»“æœ
    output_file = Path(__file__).parent / "data" / "output" / "51177_comprehensive_extracted_info.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(extracted_info, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š ç»¼åˆæå–æŠ¥å‘Š")
    print("=" * 60)
    print(f"ğŸ“ ç›®æ ‡æ–‡ä»¶: {Path(file_path).name}")
    print(f"â±ï¸ æå–æ—¶é—´: {report.extraction_time}")
    print(f"ğŸ¯ ä½¿ç”¨æŠ€æœ¯: {', '.join(report.techniques_used)}")
    print(f"ğŸ“ˆ æ€»å­—æ®µæ•°: {report.total_fields}")
    print(f" å·²æå–: {report.extracted_fields} ä¸ªå­—æ®µ")
    print(f"ğŸ“Š å®Œæ•´åº¦: {report.completeness:.1f}%")
    print(f"ğŸ–ï¸ ç½®ä¿¡åº¦: {report.confidence_score:.2f}")
    print(f"â­ è´¨é‡åˆ†æ•°: {report.quality_score:.2f}")
    print(f"ğŸ’¾ ç»“æœæ–‡ä»¶: {output_file.name}")
    print("=" * 60)
    print(" ç»¼åˆæ€§å­¦ä½è®ºæ–‡ä¿¡æ¯æå–å®Œæˆï¼")
    
    return extracted_info


if __name__ == "__main__":
    result = comprehensive_extraction("data/input/51177.docx")

