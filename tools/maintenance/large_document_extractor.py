#!/usr/bin/env python3
"""
å¤§å‹æ–‡æ¡£å¢å¼ºæå–å™¨
æ”¯æŒ64Kä¸Šä¸‹æ–‡å¤§æ¨¡å‹å¤„ç†20MBè®ºæ–‡
"""

import os
import asyncio
from typing import Dict, Any, List
from pathlib import Path
import json
import time
from concurrent.futures import ThreadPoolExecutor
import threading

class LargeDocumentExtractor:
    """å¤§å‹æ–‡æ¡£æå–å™¨"""
    
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.processing_queue = []
        self.results_cache = {}
        self.chunk_size = 64000
        
    def extract_large_document(self, file_path: str) -> Dict[str, Any]:
        """å¤„ç†å¤§å‹æ–‡æ¡£çš„ä¸»å‡½æ•°"""
        
        print(f"ğŸ¯ å¼€å§‹å¤„ç†å¤§å‹æ–‡æ¡£: {Path(file_path).name}")
        start_time = time.time()
        
        # 1. æ–‡æ¡£é¢„å¤„ç†
        doc_info = self._preprocess_document(file_path)
        print(f"ğŸ“Š æ–‡æ¡£è§„æ¨¡: {doc_info['size_mb']:.1f}MB, {doc_info['estimated_tokens']:,} tokens")
        
        # 2. é€‰æ‹©å¤„ç†ç­–ç•¥
        strategy = self._select_strategy(doc_info)
        print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥: {strategy}")
        
        # 3. æ‰§è¡Œæå–
        if strategy == "direct_processing":
            result = self._direct_extraction(file_path)
        elif strategy == "intelligent_chunking":
            result = self._chunked_extraction(file_path, doc_info)
        elif strategy == "map_reduce_processing":
            result = self._map_reduce_extraction(file_path, doc_info)
        else:
            result = self._streaming_extraction(file_path, doc_info)
        
        # 4. ç»“æœéªŒè¯å’Œä¼˜åŒ–
        result = self._validate_and_enhance(result)
        
        processing_time = time.time() - start_time
        print(f" å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.1f}ç§’")
        
        return result
    
    def _preprocess_document(self, file_path: str) -> Dict[str, Any]:
        """æ–‡æ¡£é¢„å¤„ç†"""
        file_size = os.path.getsize(file_path) / (1024 * 1024)
        
        # ä¼°ç®—å†…å®¹è§„æ¨¡
        if file_path.lower().endswith('.pdf'):
            estimated_tokens = int(file_size * 1024 * 8 // 2)  # PDFä¼°ç®—
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            estimated_tokens = len(content) // 2
        
        return {
            "size_mb": file_size,
            "estimated_tokens": estimated_tokens,
            "file_type": Path(file_path).suffix.lower()
        }
    
    def _select_strategy(self, doc_info: Dict[str, Any]) -> str:
        """é€‰æ‹©å¤„ç†ç­–ç•¥"""
        tokens = doc_info["estimated_tokens"]
        
        if tokens <= 64000:
            return "direct_processing"
        elif tokens <= 200000:
            return "intelligent_chunking"
        elif tokens <= 1000000:
            return "map_reduce_processing"
        else:
            return "streaming_processing"
    
    def _direct_extraction(self, file_path: str) -> Dict[str, Any]:
        """ç›´æ¥æå–ï¼ˆå°æ–‡æ¡£ï¼‰"""
        # ä½¿ç”¨ç°æœ‰çš„comprehensive_thesis_extractor
        try:
            from comprehensive_thesis_extractor import comprehensive_extraction
            return comprehensive_extraction(file_path)
        except ImportError:
            print("âš ï¸ comprehensive_thesis_extractoræœªæ‰¾åˆ°ï¼Œä½¿ç”¨åŸºç¡€æå–")
            return {"status": "basic_extraction"}
    
    def _chunked_extraction(self, file_path: str, doc_info: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†å—æå–ï¼ˆä¸­ç­‰æ–‡æ¡£ï¼‰"""
        print("ğŸ“„ æ‰§è¡Œæ™ºèƒ½åˆ†å—æå–...")
        
        # è¯»å–æ–‡æ¡£
        content = self._load_document(file_path)
        
        # æ™ºèƒ½åˆ†å—
        chunks = self._intelligent_chunk(content)
        print(f"   åˆ†æˆ {len(chunks)} ä¸ªæ™ºèƒ½å—")
        
        # å¹¶è¡Œå¤„ç†å„å—
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._process_chunk, chunk, i) 
                      for i, chunk in enumerate(chunks)]
            
            for future in futures:
                results.append(future.result())
        
        # åˆå¹¶ç»“æœ
        return self._merge_results(results)
    
    def _map_reduce_extraction(self, file_path: str, doc_info: Dict[str, Any]) -> Dict[str, Any]:
        """MAP-REDUCEæå–ï¼ˆå¤§æ–‡æ¡£ï¼‰"""
        print("ğŸ”„ æ‰§è¡ŒMAP-REDUCEæå–...")
        
        content = self._load_document(file_path)
        
        # MAPé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†
        chunks = self._create_chunks(content, self.chunk_size)
        print(f"   MAPé˜¶æ®µ: {len(chunks)} ä¸ªå¤„ç†å—")
        
        map_results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._map_process, chunk, i) 
                      for i, chunk in enumerate(chunks)]
            
            for future in futures:
                map_results.append(future.result())
        
        # REDUCEé˜¶æ®µï¼šç»“æœåˆå¹¶
        print("   REDUCEé˜¶æ®µ: åˆå¹¶ç»“æœ")
        return self._reduce_results(map_results)
    
    def _streaming_extraction(self, file_path: str, doc_info: Dict[str, Any]) -> Dict[str, Any]:
        """æµå¼æå–ï¼ˆè¶…å¤§æ–‡æ¡£ï¼‰"""
        print("ğŸ“¡ æ‰§è¡Œæµå¼æå–...")
        
        # æµå¼å¤„ç†å®ç°
        result = {}
        processed_chunks = 0
        
        for chunk in self._stream_chunks(file_path):
            chunk_result = self._process_streaming_chunk(chunk, processed_chunks)
            result = self._accumulate_result(result, chunk_result)
            processed_chunks += 1
            
            if processed_chunks % 10 == 0:
                print(f"   å·²å¤„ç† {processed_chunks} ä¸ªæµå¼å—")
        
        return result
    
    def _load_document(self, file_path: str) -> str:
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        if file_path.lower().endswith('.pdf'):
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨PDFå¤„ç†åº“
            print("   PDFæ–‡æ¡£å¤„ç†éœ€è¦é¢å¤–çš„PDFåº“æ”¯æŒ")
            return "PDFå†…å®¹å ä½ç¬¦"
        else:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
    
    def _intelligent_chunk(self, content: str) -> List[str]:
        """æ™ºèƒ½åˆ†å—ï¼šä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
        # åŸºäºç« èŠ‚ç»“æ„åˆ†å—
        sections = self._detect_sections(content)
        
        chunks = []
        current_chunk = ""
        current_size = 0
        
        for section in sections:
            section_size = len(section) // 2  # ä¼°ç®—tokens
            
            if current_size + section_size > self.chunk_size and current_chunk:
                chunks.append(current_chunk)
                current_chunk = section
                current_size = section_size
            else:
                current_chunk += "\n" + section
                current_size += section_size
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _detect_sections(self, content: str) -> List[str]:
        """æ£€æµ‹æ–‡æ¡£ç« èŠ‚"""
        import re
        
        # ç« èŠ‚åˆ†å‰²æ¨¡å¼
        section_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'ç¬¬[\d]+èŠ‚',
            r'\d+\.\d+',
            r'#+\s',  # Markdownæ ‡é¢˜
            r'æ‘˜è¦|abstract|å‚è€ƒæ–‡çŒ®|è‡´è°¢'
        ]
        
        sections = []
        current_section = ""
        
        lines = content.split('\n')
        
        for line in lines:
            is_section_start = any(re.search(pattern, line, re.IGNORECASE) 
                                 for pattern in section_patterns)
            
            if is_section_start and current_section:
                sections.append(current_section.strip())
                current_section = line
            else:
                current_section += "\n" + line
        
        if current_section:
            sections.append(current_section.strip())
        
        return sections if sections else [content]
    
    def _create_chunks(self, content: str, chunk_size: int) -> List[str]:
        """åˆ›å»ºå›ºå®šå¤§å°çš„å—"""
        chunks = []
        words = content.split()
        current_chunk = ""
        current_size = 0
        
        for word in words:
            word_size = len(word) // 2  # ä¼°ç®—tokens
            
            if current_size + word_size > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = word
                current_size = word_size
            else:
                current_chunk += " " + word
                current_size += word_size
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _process_chunk(self, chunk: str, chunk_id: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªå—"""
        print(f"   å¤„ç†å— {chunk_id}: {len(chunk):,} å­—ç¬¦")
        
        # æ¨¡æ‹ŸAIå¤„ç†
        import time
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "chunk_id": chunk_id,
            "content_length": len(chunk),
            "extracted_fields": ["title", "author", "abstract"],
            "confidence": 0.9
        }
    
    def _map_process(self, chunk: str, chunk_id: int) -> Dict[str, Any]:
        """MAPé˜¶æ®µå¤„ç†"""
        return self._process_chunk(chunk, chunk_id)
    
    def _process_streaming_chunk(self, chunk: str, chunk_id: int) -> Dict[str, Any]:
        """å¤„ç†æµå¼å—"""
        return self._process_chunk(chunk, chunk_id)
    
    def _stream_chunks(self, file_path: str):
        """æµå¼ç”Ÿæˆå—"""
        content = self._load_document(file_path)
        chunks = self._create_chunks(content, self.chunk_size)
        
        for chunk in chunks:
            yield chunk
    
    def _merge_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªæå–ç»“æœ"""
        merged = {
            "total_chunks": len(results),
            "processing_method": "chunked_extraction",
            "confidence": sum(r.get("confidence", 0) for r in results) / len(results),
            "extracted_fields": []
        }
        
        # åˆå¹¶å­—æ®µ
        all_fields = set()
        for result in results:
            all_fields.update(result.get("extracted_fields", []))
        
        merged["extracted_fields"] = list(all_fields)
        
        return merged
    
    def _reduce_results(self, map_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """REDUCEé˜¶æ®µï¼šåˆå¹¶MAPç»“æœ"""
        return self._merge_results(map_results)
    
    def _accumulate_result(self, current_result: Dict[str, Any], new_chunk: Dict[str, Any]) -> Dict[str, Any]:
        """ç´¯ç§¯æµå¼ç»“æœ"""
        if not current_result:
            current_result = {"chunks_processed": 0, "accumulated_fields": []}
        
        current_result["chunks_processed"] += 1
        current_result["accumulated_fields"].extend(new_chunk.get("extracted_fields", []))
        
        return current_result
    
    def _validate_and_enhance(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œå¢å¼ºç»“æœ"""
        # æ·»åŠ å¤„ç†ç»Ÿè®¡
        result["processing_stats"] = {
            "total_processing_time": "å·²å®Œæˆ",
            "quality_score": result.get("confidence", 0.8),
            "completeness": len(result.get("extracted_fields", [])) / 33  # 33ä¸ªæœŸæœ›å­—æ®µ
        }
        
        return result


def main():
    """æµ‹è¯•å¤§å‹æ–‡æ¡£å¤„ç†èƒ½åŠ›"""
    
    print("ğŸš€ å¤§å‹æ–‡æ¡£æå–å™¨åŠŸèƒ½æµ‹è¯•")
    
    # åˆ›å»ºå¢å¼ºå‹æå–å™¨
    extractor = LargeDocumentExtractor(max_workers=4)
    
    # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„æ–‡æ¡£æµ‹è¯•
    test_scenarios = [
        {"name": "å°å‹æ–‡æ¡£", "content": "A" * 50000},   # çº¦25K tokens
        {"name": "ä¸­å‹æ–‡æ¡£", "content": "B" * 200000}, # çº¦100K tokens
        {"name": "å¤§å‹æ–‡æ¡£", "content": "C" * 500000}, # çº¦250K tokens
    ]
    
    for scenario in test_scenarios:
        print(f"\nğŸ“„ æµ‹è¯•: {scenario['name']}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = f"temp_{scenario['name'].lower()}.txt"
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(scenario['content'])
        
        try:
            # æµ‹è¯•æå–
            result = extractor.extract_large_document(temp_file)
            print(f"    æå–å®Œæˆ: {result.get('processing_method', 'unknown')}")
            print(f"   ğŸ“Š è´¨é‡åˆ†æ•°: {result.get('processing_stats', {}).get('quality_score', 0):.2f}")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)
    
    print("\n å¤§å‹æ–‡æ¡£æå–å™¨æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()

