#!/usr/bin/env python3
"""
ä¿®å¤åçš„å‚è€ƒæ–‡çŒ®æå–åŠŸèƒ½
"""

import re
from typing import List, Optional
from pathlib import Path

def extract_references_with_ai_fixed(text: str, ai_client=None) -> List[str]:
    """ä½¿ç”¨AIæ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®çš„ä¿®å¤ç‰ˆæœ¬"""
    print("   ğŸ” å¯åŠ¨AIæ™ºèƒ½å‚è€ƒæ–‡çŒ®è§£æ...")
    
    # å®šä½å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    ref_text = locate_references_section(text)
    
    if not ref_text:
        print("   âš ï¸ æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†")
        return []
    
    print(f"   ğŸ“ æ‰¾åˆ°å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œé•¿åº¦: {len(ref_text)} å­—ç¬¦")
    
    # ä½¿ç”¨AIæ™ºèƒ½æå–å‚è€ƒæ–‡çŒ®æ¡ç›®
    references = extract_references_with_ai(ref_text, ai_client)
    
    print(f"    AIæå–å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
    return references

def locate_references_section(text: str) -> str:
    """å®šä½å‚è€ƒæ–‡çŒ®éƒ¨åˆ†"""
    # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®æ ‡é¢˜çš„å¤šç§æ¨¡å¼
    ref_patterns = [
        r'(?:^|\n)(?:##\s*)?å‚è€ƒæ–‡çŒ®\s*\n([\s\S]*?)(?=\n\s*(?:ç¼©ç•¥è¯è¡¨|æ–‡çŒ®ç»¼è¿°|è‡´è°¢|é™„å½•|ä½œè€…ç®€ä»‹|ä¸ªäººç®€å†)|$)',
        r'(?:^|\n)(?:##\s*)?REFERENCES?\s*\n([\s\S]*?)(?=\n\s*(?:ACKNOWLEDGMENT|APPENDIX|æ–‡çŒ®ç»¼è¿°|è‡´è°¢)|$)',
        r'(?:^|\n)(?:##\s*)?Bibliography\s*\n([\s\S]*?)(?=\n\s*(?:ACKNOWLEDGMENT|APPENDIX|æ–‡çŒ®ç»¼è¿°|è‡´è°¢)|$)',
    ]
    
    for pattern in ref_patterns:
        matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
        if matches:
            best_match = max(matches, key=lambda m: len(m.group(1)))
            ref_text = best_match.group(1).strip()
            if len(ref_text) >= 1000:
                return ref_text
    
    # å¤‡ç”¨æ–¹æ³•ï¼šå…³é”®è¯å®šä½
    ref_keywords = ['å‚è€ƒæ–‡çŒ®', 'References', 'REFERENCES', 'Bibliography']
    for keyword in ref_keywords:
        pos = text.find(keyword)
        if pos != -1:
            remaining_text = text[pos+len(keyword):]
            end_markers = ['ç¼©ç•¥è¯è¡¨', 'æ–‡çŒ®ç»¼è¿°', 'è‡´è°¢', 'ACKNOWLEDGMENT', 'APPENDIX', 'é™„å½•', 'ä½œè€…ç®€ä»‹', 'ä¸ªäººç®€å†']
            end_pos = len(remaining_text)
            
            for marker in end_markers:
                marker_pos = remaining_text.find(marker)
                if marker_pos != -1 and marker_pos < end_pos:
                    end_pos = marker_pos
            
            ref_text = remaining_text[:end_pos]
            if len(ref_text) >= 1000:
                return ref_text
    
    return ""

def extract_references_with_ai(ref_text: str, ai_client=None) -> List[str]:
    """ä½¿ç”¨AIå¤§æ¨¡å‹æå–å‚è€ƒæ–‡çŒ®æ¡ç›®"""
    try:
        # é™åˆ¶è¾“å…¥é•¿åº¦ä»¥é¿å…tokenè¶…é™
        max_length = 50000  # çº¦50kå­—ç¬¦ï¼Œå¯¹åº”å¤§çº¦12-15k tokens
        if len(ref_text) > max_length:
            print(f"   ğŸ“ å‚è€ƒæ–‡çŒ®å†…å®¹è¿‡é•¿({len(ref_text)}å­—ç¬¦)ï¼Œæˆªå–å‰{max_length}å­—ç¬¦")
            ref_text = ref_text[:max_length]
        
        prompt = f"""è¯·ä»ä»¥ä¸‹å‚è€ƒæ–‡çŒ®æ–‡æœ¬ä¸­æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªå‚è€ƒæ–‡çŒ®æ¡ç›®åº”è¯¥æ˜¯å®Œæ•´çš„ä¸€æ¡è®°å½•
2. ä¿æŒåŸæœ‰çš„ç¼–å·æ ¼å¼ï¼ˆå¦‚ï¼»1ï¼½ã€[1]ã€1.ç­‰ï¼‰
3. æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
4. æ¯æ¡å‚è€ƒæ–‡çŒ®åº”è¯¥åŒ…å«ï¼šä½œè€…ã€æ ‡é¢˜ã€æœŸåˆŠ/ä¼šè®®/å‡ºç‰ˆç¤¾ã€å¹´ä»½ç­‰ä¿¡æ¯
5. å¦‚æœæ ¼å¼æ··ä¹±ï¼Œè¯·æ™ºèƒ½é‡ç»„æˆæ ‡å‡†æ ¼å¼
6. æŒ‰ç¼–å·é¡ºåºæ’åˆ—
7. è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸€æ¡å‚è€ƒæ–‡çŒ®ï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜

å‚è€ƒæ–‡çŒ®æ–‡æœ¬ï¼š
{ref_text}

è¯·æå–æ‰€æœ‰å‚è€ƒæ–‡çŒ®æ¡ç›®ï¼š"""

        if ai_client:
            print("   ğŸ¤– è°ƒç”¨AIå¤§æ¨¡å‹æå–å‚è€ƒæ–‡çŒ®...")
            response = ai_client.send_message(prompt)
            
            if response and hasattr(response, 'content'):
                content = response.content.strip()
                
                # è§£æAIè¿”å›çš„ç»“æœ
                references = []
                lines = content.split('\n')
                
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åƒå‚è€ƒæ–‡çŒ®æ¡ç›®
                    if is_valid_reference(line):
                        # æ¸…ç†æ ¼å¼
                        cleaned_ref = re.sub(r'\s+', ' ', line).strip()
                        references.append(cleaned_ref)
                
                print(f"    AIæˆåŠŸæå– {len(references)} æ¡å‚è€ƒæ–‡çŒ®")
                return references
            else:
                print("   âš ï¸ AIå“åº”ä¸ºç©º")
        else:
            print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"   âŒ AIæå–å¤±è´¥: {e}")
    
    # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼
    print("   ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•...")
    return extract_references_fallback(ref_text)

def is_valid_reference(line: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å‚è€ƒæ–‡çŒ®æ¡ç›®"""
    # åŸºæœ¬é•¿åº¦æ£€æŸ¥
    if len(line) < 20:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–å·æ ¼å¼
    has_number = bool(re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.|ï¼ˆ\d+ï¼‰|\(\d+\))', line))
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœŸåˆŠã€ä¼šè®®ã€å‡ºç‰ˆç¤¾ç­‰å…³é”®è¯
    has_publication = any(keyword in line for keyword in [
        'Journal', 'Proceedings', 'Conference', 'IEEE', 'ACM', 'Optics',
        'æœŸåˆŠ', 'ä¼šè®®', 'å­¦æŠ¥', 'å¤§å­¦å­¦æŠ¥', 'è®ºæ–‡é›†', 'å‡ºç‰ˆç¤¾', 'DOI'
    ])
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
    has_year = bool(re.search(r'(?:19|20)\d{2}', line))
    
    # è‡³å°‘æ»¡è¶³ç¼–å·+å¹´ä»½ï¼Œæˆ–è€…ç¼–å·+å‡ºç‰ˆç‰©
    return has_number and (has_year or has_publication)

def extract_references_fallback(ref_text: str) -> List[str]:
    """å¤‡ç”¨çš„å‚è€ƒæ–‡çŒ®æå–æ–¹æ³•"""
    references = []
    
    # æ™ºèƒ½æ®µè½åˆ†å‰²å’Œé‡ç»„
    print("   ğŸ”§ ä½¿ç”¨æ™ºèƒ½æ®µè½é‡ç»„æ–¹æ³•...")
    
    # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
    paragraphs = re.split(r'\n\s*\n', ref_text)
    current_ref = ""
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„å‚è€ƒæ–‡çŒ®å¼€å§‹
        if re.match(r'^\s*(?:ï¼»\d+ï¼½|\[\d+\]|\d+\.)', para):
            # ä¿å­˜ä¹‹å‰çš„å‚è€ƒæ–‡çŒ®
            if current_ref and len(current_ref) > 20:
                cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
                references.append(cleaned_ref)
            
            # å¼€å§‹æ–°çš„å‚è€ƒæ–‡çŒ®
            current_ref = para
        else:
            # ç»§ç»­å½“å‰å‚è€ƒæ–‡çŒ®
            if current_ref:
                current_ref += " " + para
    
    # æ·»åŠ æœ€åä¸€æ¡å‚è€ƒæ–‡çŒ®
    if current_ref and len(current_ref) > 20:
        cleaned_ref = re.sub(r'\s+', ' ', current_ref).strip()
        references.append(cleaned_ref)
    
    return references[:100]  # é™åˆ¶æ•°é‡

# æµ‹è¯•å‡½æ•°
def test_fixed_extraction():
    """æµ‹è¯•ä¿®å¤åçš„æå–åŠŸèƒ½"""
    cache_file = Path("cache/documents/åŸºäºç¥ç»ç½‘ç»œçš„ç›¸å¹²å…‰é€šä¿¡ç³»ç»Ÿéçº¿æ€§æŸä¼¤å‡è¡¡æŠ€æœ¯ç ”ç©¶_å†¯æ™“å€©_f28c5133e8a3bd43f6c85222b885a8ce.md")
    
    if not cache_file.exists():
        print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        return
    
    with open(cache_file, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # è·å–AIå®¢æˆ·ç«¯
    try:
        import sys
        sys.path.append('src')
        from thesis_inno_eval.ai_client import get_ai_client
        ai_client = get_ai_client()
        print(" AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        ai_client = None
    
    # æµ‹è¯•æå–
    references = extract_references_with_ai_fixed(text, ai_client)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æå–åˆ°å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
    
    if references:
        print(f"\nğŸ“‹ å‰3æ¡å‚è€ƒæ–‡çŒ®:")
        for i, ref in enumerate(references[:3]):
            print(f"   {i+1}. {ref[:100]}...")

if __name__ == '__main__':
    test_fixed_extraction()

