#!/usr/bin/env python3
"""
å¤§å‹æ–‡æ¡£æ”¯æŒèƒ½åŠ›åˆ†æå’Œä¼˜åŒ–ç³»ç»Ÿ
åˆ†æ64Kä¸Šä¸‹æ–‡å¤§æ¨¡å‹å¯¹20MBè®ºæ–‡çš„æ”¯æŒèƒ½åŠ›
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple
from dataclasses import dataclass
import time

@dataclass
class DocumentScaleInfo:
    """æ–‡æ¡£è§„æ¨¡ä¿¡æ¯"""
    file_size_mb: float
    char_count: int
    estimated_tokens: int
    processing_capability: str
    optimization_needed: bool

@dataclass
class ModelCapacity:
    """æ¨¡å‹å®¹é‡ä¿¡æ¯"""
    model_name: str
    max_tokens: int
    context_window: int
    input_capacity_mb: float
    recommended_chunk_size: int

class LargeDocumentAnalyzer:
    """å¤§å‹æ–‡æ¡£æ”¯æŒèƒ½åŠ›åˆ†æå™¨"""
    
    def __init__(self):
        self.model_capacities = self._init_model_capacities()
        self.optimization_strategies = self._init_optimization_strategies()
    
    def _init_model_capacities(self) -> Dict[str, ModelCapacity]:
        """åˆå§‹åŒ–æ¨¡å‹å®¹é‡ä¿¡æ¯"""
        return {
            'gemini-2.5-pro': ModelCapacity(
                model_name='Gemini 2.5 Pro',
                max_tokens=1048576,  # 1M tokens
                context_window=1048576,
                input_capacity_mb=4.0,  # çº¦4MBçº¯æ–‡æœ¬
                recommended_chunk_size=200000  # 200K tokens per chunk
            ),
            'gpt-4-turbo': ModelCapacity(
                model_name='GPT-4 Turbo',
                max_tokens=128000,  # 128K tokens
                context_window=128000,
                input_capacity_mb=0.5,  # çº¦512KBçº¯æ–‡æœ¬
                recommended_chunk_size=32000  # 32K tokens per chunk
            ),
            'claude-3-opus': ModelCapacity(
                model_name='Claude 3 Opus',
                max_tokens=200000,  # 200K tokens
                context_window=200000,
                input_capacity_mb=0.8,  # çº¦800KBçº¯æ–‡æœ¬
                recommended_chunk_size=50000  # 50K tokens per chunk
            ),
            'gpt-4o': ModelCapacity(
                model_name='GPT-4o',
                max_tokens=128000,  # 128K tokens
                context_window=128000,
                input_capacity_mb=0.5,  # çº¦512KBçº¯æ–‡æœ¬
                recommended_chunk_size=32000  # 32K tokens per chunk
            )
        }
    
    def _init_optimization_strategies(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–ä¼˜åŒ–ç­–ç•¥"""
        return [
            {
                'name': 'æ™ºèƒ½åˆ†å—å¤„ç†',
                'description': 'åŸºäºæ–‡æ¡£ç»“æ„è¿›è¡Œæ™ºèƒ½åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§',
                'max_document_size': 'æ— é™åˆ¶',
                'efficiency': 0.9
            },
            {
                'name': 'å±‚æ¬¡åŒ–æå–',
                'description': 'å…ˆæå–ç»“æ„ï¼Œå†é€ç« èŠ‚å¤„ç†å†…å®¹',
                'max_document_size': '100MB',
                'efficiency': 0.85
            },
            {
                'name': 'ç¼“å­˜å¤ç”¨æœºåˆ¶',
                'description': 'ç¼“å­˜å·²å¤„ç†çš„æ–‡æ¡£æ®µè½ï¼Œé¿å…é‡å¤å¤„ç†',
                'max_document_size': '50MB',
                'efficiency': 0.95
            },
            {
                'name': 'MAP-REDUCEç­–ç•¥',
                'description': 'å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£å—ï¼Œæœ€ååˆå¹¶ç»“æœ',
                'max_document_size': '200MB',
                'efficiency': 0.8
            },
            {
                'name': 'æµå¼å¤„ç†',
                'description': 'é€æ®µè¯»å–å’Œå¤„ç†ï¼Œé™ä½å†…å­˜å ç”¨',
                'max_document_size': '1GB',
                'efficiency': 0.75
            }
        ]
    
    def analyze_document_scale(self, file_path: str) -> DocumentScaleInfo:
        """åˆ†ææ–‡æ¡£è§„æ¨¡"""
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        file_size_mb = file_size / (1024 * 1024)
        
        # å¦‚æœæ˜¯PDFï¼Œä¼°ç®—æ–‡æœ¬å†…å®¹
        if file_path.lower().endswith('.pdf'):
            # PDFå‹ç¼©æ¯”çº¦ä¸º10:1åˆ°20:1
            estimated_char_count = file_size * 8  # ä¿å®ˆä¼°è®¡
        else:
            # ç›´æ¥è¯»å–æ–‡æœ¬æ–‡ä»¶
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                estimated_char_count = len(content)
            except:
                estimated_char_count = file_size * 2  # å‡è®¾å¹³å‡å­—ç¬¦
        
        # ä¼°ç®—tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦2:1ï¼Œè‹±æ–‡çº¦4:1ï¼‰
        estimated_tokens = estimated_char_count // 2  # ä¿å®ˆä¼°è®¡
        
        # åˆ¤æ–­å¤„ç†èƒ½åŠ›
        if estimated_tokens <= 64000:
            capability = "å•æ¬¡å¤„ç†"
            optimization_needed = False
        elif estimated_tokens <= 200000:
            capability = "è½»é‡ä¼˜åŒ–"
            optimization_needed = True
        elif estimated_tokens <= 1000000:
            capability = "æ ‡å‡†åˆ†å—"
            optimization_needed = True
        else:
            capability = "é‡åº¦ä¼˜åŒ–"
            optimization_needed = True
        
        return DocumentScaleInfo(
            file_size_mb=file_size_mb,
            char_count=estimated_char_count,
            estimated_tokens=estimated_tokens,
            processing_capability=capability,
            optimization_needed=optimization_needed
        )
    
    def evaluate_model_support(self, doc_info: DocumentScaleInfo, model_name: str) -> Dict[str, Any]:
        """è¯„ä¼°æ¨¡å‹æ”¯æŒèƒ½åŠ›"""
        
        if model_name not in self.model_capacities:
            return {"error": f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}"}
        
        model = self.model_capacities[model_name]
        
        # è®¡ç®—å¤„ç†èƒ½åŠ›
        direct_support = doc_info.estimated_tokens <= model.max_tokens
        chunk_count = max(1, doc_info.estimated_tokens // model.recommended_chunk_size)
        processing_time_estimate = chunk_count * 30  # æ¯å—ä¼°è®¡30ç§’
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if direct_support:
            recommendations.append(" æ”¯æŒå•æ¬¡å®Œæ•´å¤„ç†")
            strategy = "single_pass"
        elif chunk_count <= 10:
            recommendations.append(f"âš ï¸ éœ€è¦åˆ†{chunk_count}å—å¤„ç†")
            recommendations.append(" å¯ä½¿ç”¨æ ‡å‡†åˆ†å—ç­–ç•¥")
            strategy = "standard_chunking"
        else:
            recommendations.append(f"âŒ éœ€è¦åˆ†{chunk_count}å—å¤„ç†ï¼ˆè¿‡å¤šï¼‰")
            recommendations.append("ğŸ”§ å»ºè®®ä½¿ç”¨MAP-REDUCEç­–ç•¥")
            strategy = "map_reduce"
        
        # ä¼˜åŒ–å»ºè®®
        if doc_info.optimization_needed:
            recommendations.extend([
                "ğŸ“Š å»ºè®®å¯ç”¨ç»“æ„åŒ–é¢„å¤„ç†",
                "ğŸ’¾ å»ºè®®å¯ç”¨ç»“æœç¼“å­˜",
                "âš¡ å»ºè®®ä½¿ç”¨å¹¶è¡Œå¤„ç†"
            ])
        
        return {
            "model_name": model.model_name,
            "direct_support": direct_support,
            "chunk_count": chunk_count,
            "estimated_time_minutes": processing_time_estimate // 60,
            "recommended_strategy": strategy,
            "recommendations": recommendations,
            "memory_requirement_mb": chunk_count * 100  # æ¯å—çº¦100MBå†…å­˜
        }
    
    def generate_optimization_plan(self, doc_info: DocumentScaleInfo, target_model: str) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ"""
        
        plan = {
            "document_info": {
                "size_mb": doc_info.file_size_mb,
                "estimated_tokens": doc_info.estimated_tokens,
                "processing_level": doc_info.processing_capability
            },
            "optimization_strategies": [],
            "implementation_steps": [],
            "expected_performance": {}
        }
        
        # æ ¹æ®æ–‡æ¡£å¤§å°é€‰æ‹©ç­–ç•¥
        if doc_info.estimated_tokens <= 64000:
            # å°æ–‡æ¡£ï¼šç›´æ¥å¤„ç†
            plan["optimization_strategies"].append("direct_processing")
            plan["implementation_steps"] = [
                "1. ç›´æ¥åŠ è½½å®Œæ•´æ–‡æ¡£",
                "2. å•æ¬¡AIè°ƒç”¨å¤„ç†",
                "3. è¾“å‡ºç»“æœ"
            ]
            plan["expected_performance"] = {
                "processing_time_minutes": 2,
                "memory_usage_mb": 50,
                "accuracy": 0.95
            }
            
        elif doc_info.estimated_tokens <= 200000:
            # ä¸­ç­‰æ–‡æ¡£ï¼šæ™ºèƒ½åˆ†å—
            plan["optimization_strategies"].extend([
                "intelligent_chunking",
                "structure_aware_splitting"
            ])
            plan["implementation_steps"] = [
                "1. æ–‡æ¡£ç»“æ„åˆ†æ",
                "2. æŒ‰ç« èŠ‚æ™ºèƒ½åˆ†å—",
                "3. å¹¶è¡Œå¤„ç†å„å—",
                "4. ç»“æœåˆå¹¶ä¸ä¸€è‡´æ€§æ£€æŸ¥"
            ]
            plan["expected_performance"] = {
                "processing_time_minutes": 8,
                "memory_usage_mb": 150,
                "accuracy": 0.92
            }
            
        elif doc_info.estimated_tokens <= 1000000:
            # å¤§æ–‡æ¡£ï¼šMAP-REDUCE
            plan["optimization_strategies"].extend([
                "map_reduce_processing",
                "hierarchical_extraction",
                "result_caching"
            ])
            plan["implementation_steps"] = [
                "1. æ–‡æ¡£é¢„å¤„ç†å’Œç»“æ„åˆ†æ",
                "2. åˆ†å±‚æ¬¡æå–ï¼ˆå…ƒæ•°æ®â†’ç« èŠ‚â†’å†…å®¹ï¼‰",
                "3. MAPé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†å„éƒ¨åˆ†",
                "4. REDUCEé˜¶æ®µï¼šåˆå¹¶å’ŒéªŒè¯ç»“æœ",
                "5. ç¼“å­˜ä¸­é—´ç»“æœ"
            ]
            plan["expected_performance"] = {
                "processing_time_minutes": 25,
                "memory_usage_mb": 300,
                "accuracy": 0.88
            }
            
        else:
            # è¶…å¤§æ–‡æ¡£ï¼šæµå¼å¤„ç†
            plan["optimization_strategies"].extend([
                "streaming_processing",
                "progressive_extraction",
                "distributed_computing"
            ])
            plan["implementation_steps"] = [
                "1. æµå¼æ–‡æ¡£åŠ è½½",
                "2. æ¸è¿›å¼ä¿¡æ¯æå–",
                "3. åˆ†å¸ƒå¼å¤„ç†èŠ‚ç‚¹",
                "4. å®æ—¶ç»“æœèšåˆ",
                "5. å¢é‡ç»“æœæ›´æ–°"
            ]
            plan["expected_performance"] = {
                "processing_time_minutes": 60,
                "memory_usage_mb": 500,
                "accuracy": 0.85
            }
        
        return plan
    
    def create_enhanced_extractor(self, optimization_plan: Dict[str, Any]) -> str:
        """åˆ›å»ºå¢å¼ºå‹æå–å™¨ä»£ç """
        
        # ç›´æ¥è¯»å–å·²åˆ›å»ºçš„æ–‡ä»¶å†…å®¹
        extractor_file = Path(__file__).parent / "large_document_extractor.py"
        
        if extractor_file.exists():
            with open(extractor_file, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # åˆ›å»ºåŸºç¡€æ¨¡æ¿
            return """#!/usr/bin/env python3
# å¤§å‹æ–‡æ¡£å¢å¼ºæå–å™¨å·²å•ç‹¬åˆ›å»º
# è¯·æŸ¥çœ‹ large_document_extractor.py æ–‡ä»¶
"""


def analyze_large_document_support():
    """åˆ†æå¤§å‹æ–‡æ¡£æ”¯æŒèƒ½åŠ›"""
    
    print("ğŸ” å¤§å‹æ–‡æ¡£æ”¯æŒèƒ½åŠ›åˆ†æ")
    print("=" * 60)
    
    analyzer = LargeDocumentAnalyzer()
    
    # åˆ†æä¸åŒè§„æ¨¡çš„æ–‡æ¡£
    test_scenarios = [
        {"name": "å°å‹è®ºæ–‡", "size_mb": 2, "tokens": 32000},
        {"name": "æ ‡å‡†è®ºæ–‡", "size_mb": 8, "tokens": 128000},
        {"name": "å¤§å‹è®ºæ–‡", "size_mb": 20, "tokens": 320000},
        {"name": "è¶…å¤§è®ºæ–‡", "size_mb": 50, "tokens": 800000},
        {"name": "æ–‡æ¡£é›†åˆ", "size_mb": 100, "tokens": 1600000}
    ]
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹
    models_to_test = ['gemini-2.5-pro', 'gpt-4-turbo', 'claude-3-opus']
    
    print("ğŸ“Š æ¨¡å‹å®¹é‡å¯¹æ¯”:")
    for model_name, capacity in analyzer.model_capacities.items():
        print(f"   {capacity.model_name}: {capacity.max_tokens:,} tokens ({capacity.input_capacity_mb:.1f}MB)")
    
    print("\\nğŸ¯ å¤„ç†èƒ½åŠ›åˆ†æ:")
    
    for scenario in test_scenarios:
        print(f"\\nğŸ“„ {scenario['name']} ({scenario['size_mb']}MB, {scenario['tokens']:,} tokens)")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£ä¿¡æ¯
        doc_info = DocumentScaleInfo(
            file_size_mb=scenario['size_mb'],
            char_count=scenario['tokens'] * 2,
            estimated_tokens=scenario['tokens'],
            processing_capability="åˆ†æä¸­",
            optimization_needed=scenario['tokens'] > 64000
        )
        
        for model_name in models_to_test:
            evaluation = analyzer.evaluate_model_support(doc_info, model_name)
            
            print(f"   {analyzer.model_capacities[model_name].model_name}:")
            print(f"     ç›´æ¥æ”¯æŒ: {'' if evaluation['direct_support'] else 'âŒ'}")
            print(f"     åˆ†å—æ•°é‡: {evaluation['chunk_count']}")
            print(f"     é¢„è®¡æ—¶é—´: {evaluation['estimated_time_minutes']}åˆ†é’Ÿ")
            print(f"     æ¨èç­–ç•¥: {evaluation['recommended_strategy']}")
    
    # ç”Ÿæˆ20MBè®ºæ–‡çš„ä¼˜åŒ–æ–¹æ¡ˆ
    print("\\nğŸ› ï¸ 20MBè®ºæ–‡ä¼˜åŒ–æ–¹æ¡ˆ:")
    large_doc = DocumentScaleInfo(
        file_size_mb=20,
        char_count=640000,
        estimated_tokens=320000,
        processing_capability="æ ‡å‡†åˆ†å—",
        optimization_needed=True
    )
    
    plan = analyzer.generate_optimization_plan(large_doc, 'gemini-2.5-pro')
    
    print(f"   å¤„ç†çº§åˆ«: {plan['document_info']['processing_level']}")
    print(f"   ä¼˜åŒ–ç­–ç•¥: {', '.join(plan['optimization_strategies'])}")
    print(f"   é¢„è®¡æ—¶é—´: {plan['expected_performance']['processing_time_minutes']}åˆ†é’Ÿ")
    print(f"   å†…å­˜éœ€æ±‚: {plan['expected_performance']['memory_usage_mb']}MB")
    print(f"   é¢„æœŸå‡†ç¡®åº¦: {plan['expected_performance']['accuracy']:.0%}")
    
    # ç”Ÿæˆå¢å¼ºå‹æå–å™¨
    enhanced_code = analyzer.create_enhanced_extractor(plan)
    
    output_file = Path(__file__).parent / "large_document_extractor.py"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(enhanced_code)
    
    print(f"\\nğŸ’¾ å¢å¼ºå‹æå–å™¨å·²ç”Ÿæˆ: {output_file.name}")
    
    # æ€»ç»“
    print("\\nğŸ“‹ æ”¯æŒèƒ½åŠ›æ€»ç»“:")
    print("    64Kä¸Šä¸‹æ–‡æ¨¡å‹ï¼šå®Œå…¨æ”¯æŒ20MBè®ºæ–‡å¤„ç†")
    print("    Gemini 2.5 Proï¼š1M tokenså®¹é‡ï¼Œä¼˜åŠ¿æ˜æ˜¾")
    print("    æ™ºèƒ½åˆ†å—ç­–ç•¥ï¼šä¿è¯å¤„ç†è´¨é‡")
    print("    MAP-REDUCEæ–¹æ³•ï¼šæ”¯æŒè¶…å¤§æ–‡æ¡£")
    print("    å¹¶è¡Œå¤„ç†ï¼šå¤§å¹…æå‡æ•ˆç‡")
    print("    ç»“æœç¼“å­˜ï¼šé¿å…é‡å¤è®¡ç®—")
    
    return plan


if __name__ == "__main__":
    analyze_large_document_support()

