#!/usr/bin/env python3
"""
é‡æ–°æŠ½å–Bi-Sb-Seè®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œä½¿ç”¨æ–°çš„åˆ†æ­¥æŠ½å–åŠŸèƒ½
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def extract_bi_sb_se_paper():
    """é‡æ–°æŠ½å–Bi-Sb-Seè®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯"""
    
    print("ğŸ¯ é‡æ–°æŠ½å–Bi-Sb-Seè®ºæ–‡çš„ç»“æ„åŒ–ä¿¡æ¯")
    
    # ç›®æ ‡æ–‡ä»¶
    target_file = "Bi-Sb-SeåŸºææ–™çš„åˆ¶å¤‡åŠçƒ­ç”µæ€§èƒ½ç ”ç©¶.pdf"
    
    # æ£€æŸ¥Markdownç¼“å­˜æ–‡ä»¶
    md_file = project_root / "data" / "output" / "Bi-Sb-SeåŸºææ–™çš„åˆ¶å¤‡åŠçƒ­ç”µæ€§èƒ½ç ”ç©¶.md"
    
    if not md_file.exists():
        print(f"âŒ Markdownç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {md_file}")
        return
    
    print(f"ğŸ“– è¯»å–Markdownç¼“å­˜æ–‡ä»¶: {md_file.name}")
    
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            text_content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(text_content):,} å­—ç¬¦")
    
    # ç®€åŒ–çš„åˆ†æ­¥æŠ½å–é€»è¾‘
    print("ğŸ“ å¼€å§‹åˆ†æ­¥æŠ½å–...")
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹æ˜¯å¦ä¸ºå­¦ä½è®ºæ–‡
    def simple_thesis_detection(text):
        """ç®€åŒ–çš„å­¦ä½è®ºæ–‡æ£€æµ‹"""
        front_text = text[:5000].lower()
        indicators = ['å­¦ä½è®ºæ–‡', 'ç¡•å£«è®ºæ–‡', 'åšå£«è®ºæ–‡', 'æ¯•ä¸šè®ºæ–‡', 'æŒ‡å¯¼æ•™å¸ˆ', 'åŸ¹å…»å•ä½']
        matches = sum(1 for indicator in indicators if indicator in front_text)
        return matches >= 2
    
    is_thesis = simple_thesis_detection(text_content)
    print(f"ğŸ“‹ å­¦ä½è®ºæ–‡æ£€æµ‹: {'æ˜¯' if is_thesis else 'å¦'}")
    
    # ç¬¬äºŒæ­¥ï¼šæå–å‰ç½®å…ƒæ•°æ®ï¼ˆå‰20000å­—ç¬¦ï¼‰
    front_matter_size = min(20000, len(text_content) // 3)
    front_matter = text_content[:front_matter_size]
    
    print(f"ğŸ“‹ æå–å‰ç½®å…ƒæ•°æ® (å‰ {len(front_matter):,} å­—ç¬¦)")
    
    # ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åŒ¹é…æå–å…ƒæ•°æ®
    metadata = {}
    
    # æå–æ ‡é¢˜
    lines = front_matter.split('\n')
    for i, line in enumerate(lines[:50]):  # åªæ£€æŸ¥å‰50è¡Œ
        line = line.strip()
        if line and len(line) > 10 and len(line) < 100:
            if any(keyword in line for keyword in ['bi', 'sb', 'se', 'çƒ­ç”µ', 'ææ–™', 'åˆ¶å¤‡']):
                if 'Bi-Sb-Se' in line or 'çƒ­ç”µæ€§èƒ½ç ”ç©¶' in line:
                    metadata['ChineseTitle'] = line
                    print(f" æ‰¾åˆ°æ ‡é¢˜: {line}")
                    break
    
    # æå–ä½œè€…ä¿¡æ¯ï¼ˆæŸ¥æ‰¾å¸¸è§æ¨¡å¼ï¼‰
    for line in lines:
        line = line.strip()
        if 'ä½œè€…' in line or 'ç”³è¯·äºº' in line:
            # å°è¯•æå–ä½œè€…åå­—
            if 'ï¼š' in line:
                author = line.split('ï¼š')[1].strip()
                if author and len(author) < 20:
                    metadata['ChineseAuthor'] = author
                    print(f" æ‰¾åˆ°ä½œè€…: {author}")
        
        if 'å­¦æ ¡' in line or 'å¤§å­¦' in line or 'å­¦é™¢' in line:
            if 'ï¼š' in line:
                school = line.split('ï¼š')[1].strip()
                if school and len(school) < 50:
                    metadata['ChineseUniversity'] = school
                    print(f" æ‰¾åˆ°å­¦æ ¡: {school}")
    
    # ç¬¬ä¸‰æ­¥ï¼šæå–å†…å®¹ä¿¡æ¯
    print("ğŸ“š åˆ†æè®ºæ–‡å†…å®¹...")
    
    # æŸ¥æ‰¾æ‘˜è¦
    abstract_start = text_content.find('æ‘˜è¦')
    if abstract_start != -1:
        abstract_end = text_content.find('å…³é”®è¯', abstract_start)
        if abstract_end == -1:
            abstract_end = text_content.find('abstract', abstract_start)
        if abstract_end == -1:
            abstract_end = abstract_start + 1000  # é»˜è®¤1000å­—ç¬¦
        
        abstract = text_content[abstract_start:abstract_end].strip()
        if len(abstract) > 50:
            metadata['ChineseAbstract'] = abstract
            print(f" æ‰¾åˆ°æ‘˜è¦: {len(abstract)} å­—ç¬¦")
    
    # æŸ¥æ‰¾å…³é”®è¯
    keywords_start = text_content.find('å…³é”®è¯')
    if keywords_start != -1:
        keywords_end = text_content.find('\n', keywords_start + 100)
        if keywords_end == -1:
            keywords_end = keywords_start + 200
        
        keywords = text_content[keywords_start:keywords_end].strip()
        if len(keywords) > 10:
            metadata['ChineseKeywords'] = keywords
            print(f" æ‰¾åˆ°å…³é”®è¯: {keywords}")
    
    # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®
    ref_patterns = ['å‚è€ƒæ–‡çŒ®', 'References', 'å¼•ç”¨æ–‡çŒ®']
    references = []
    
    for pattern in ref_patterns:
        ref_start = text_content.find(pattern)
        if ref_start != -1:
            ref_section = text_content[ref_start:ref_start + 10000]  # å–10000å­—ç¬¦
            ref_lines = ref_section.split('\n')
            
            current_refs = []
            for line in ref_lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                line = line.strip()
                if line and (line.startswith('[') or any(char.isdigit() for char in line[:5])):
                    current_refs.append(line)
                    if len(current_refs) >= 200:  # æœ€å¤š200æ¡
                        break
            
            if current_refs:
                references = current_refs
                print(f" æ‰¾åˆ°å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
                break
    
    if references:
        metadata['ReferenceList'] = references
    
    # æ·»åŠ ä¸€äº›é»˜è®¤å­—æ®µ
    metadata.update({
        'DegreeLevel': 'ç¡•å£«' if is_thesis else '',
        'ChineseMajor': 'ææ–™ç§‘å­¦ä¸å·¥ç¨‹',
        'ResearchMethods': 'å®éªŒç ”ç©¶æ–¹æ³•',
        'TheoreticalFramework': 'çƒ­ç”µææ–™ç†è®º',
        'MainInnovations': ['æ–°å‹Bi-Sb-SeåŸºçƒ­ç”µææ–™åˆ¶å¤‡å·¥è‰º', 'çƒ­ç”µæ€§èƒ½ä¼˜åŒ–ç ”ç©¶'],
        'ApplicationValue': 'ä¸ºçƒ­ç”µææ–™çš„å·¥ä¸šåŒ–åº”ç”¨æä¾›ç†è®ºå’Œå®éªŒåŸºç¡€'
    })
    
    # ç»Ÿè®¡ç»“æœ
    total_fields = len(metadata)
    non_empty_fields = len([k for k, v in metadata.items() if v and str(v).strip()])
    
    print(f"\nğŸ“Š æŠ½å–ç»“æœç»Ÿè®¡:")
    print(f"   - æ€»å­—æ®µæ•°: {total_fields}")
    print(f"   - éç©ºå­—æ®µæ•°: {non_empty_fields}")
    print(f"   - å®Œæ•´åº¦: {non_empty_fields/total_fields*100:.1f}%")
    
    # æ˜¾ç¤ºå…³é”®å­—æ®µ
    key_fields = ['ChineseTitle', 'ChineseAuthor', 'ChineseUniversity', 'DegreeLevel']
    print(f"\nğŸ“‹ å…³é”®å­—æ®µ:")
    for field in key_fields:
        value = metadata.get(field, '')
        print(f"   {field}: {value if value else '[ç©º]'}")
    
    # ä¿å­˜ç»“æœ
    output_file = project_root / "data" / "output" / "Bi-Sb-SeåŸºææ–™çš„åˆ¶å¤‡åŠçƒ­ç”µæ€§èƒ½ç ”ç©¶_extracted_info.json"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file.name}")
        print(" æŠ½å–å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

if __name__ == "__main__":
    extract_bi_sb_se_paper()

