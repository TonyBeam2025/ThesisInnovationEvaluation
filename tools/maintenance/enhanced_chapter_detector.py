#!/usr/bin/env python3
"""
å¢å¼ºç« èŠ‚è¯†åˆ«å™¨ - ä¸“é—¨å¤„ç†æ•°å­—æ ¼å¼ç« èŠ‚æ ‡é¢˜
"""

import sys
import os
import re
import json
from typing import Dict, Any, List, Tuple

# æ·»åŠ æºä»£ç è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

def enhanced_chapter_detection(text: str) -> Dict[str, Any]:
    """å¢å¼ºçš„ç« èŠ‚æ£€æµ‹ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    
    sections = {}
    
    # æ‰©å±•çš„ç« èŠ‚è¯†åˆ«æ¨¡å¼
    enhanced_patterns = {
        # ä¼ ç»Ÿæ ¼å¼ä¿æŒä¸å˜
        'abstract_cn': r'((?:ä¸­æ–‡)?æ‘˜\s*è¦[\s\S]{100,5000}?)(?=å…³é”®è¯|è‹±æ–‡æ‘˜è¦|ABSTRACT|ç›®\s*å½•)',
        'abstract_en': r'((?:ABSTRACT|Abstract)[\s\S]{100,5000}?)(?=Keywords?|Key\s+Words?|ç›®\s*å½•|1\s)',
        'keywords_cn': r'(å…³é”®è¯[ï¼š:\s]*[^\n\r]{5,200})',
        'keywords_en': r'((?:Keywords?|KEY\s+WORDS?|Key\s+Words?)[ï¼š:\s]*[^\n\r]{5,200})',
        
        # å¢å¼ºçš„ç›®å½•è¯†åˆ«
        'toc': r'(ç›®\s*å½•[\s\S]{100,2000}?)(?=1\s+ç»ªè®º|1\s|æ‘˜\s*è¦)',
        
        # æ•°å­—ç« èŠ‚æ ¼å¼ - è¿™æ˜¯å…³é”®æ”¹è¿›
        'chapter_1': r'((?:^|\n)\s*1\s+ç»ª\s*è®º[\s\S]{500,10000}?)(?=2\s+|$)',
        'chapter_2': r'((?:^|\n)\s*2\s+[\u4e00-\u9fff].*?åŸºç¡€ç†è®º[\s\S]{1000,20000}?)(?=3\s+|$)',
        'chapter_3': r'((?:^|\n)\s*3\s+[\u4e00-\u9fff].*?CTA.*?åˆ†å‰²[\s\S]{1000,15000}?)(?=4\s+|$)',
        'chapter_4': r'((?:^|\n)\s*4\s+å››ç»´åŠ¨æ€[\s\S]{1000,15000}?)(?=5\s+|ç»“\s*è®º|$)',
        'chapter_5': r'((?:^|\n)\s*5\s+ç»“\s*è®º[\s\S]{200,8000}?)(?=å‚\s*è€ƒ\s*æ–‡\s*çŒ®|è‡´è°¢|$)',
        
        # ä¼ ç»Ÿç« èŠ‚æ ¼å¼ä½œä¸ºå¤‡é€‰
        'introduction_alt': r'((?:ç¬¬ä¸€ç« |ç¬¬1ç« |å¼•\s*è¨€|ç»ª\s*è®º)[\s\S]{500,10000}?)(?=ç¬¬äºŒç« |ç¬¬2ç« |2\s)',
        'literature_alt': r'((?:ç¬¬äºŒç« |ç¬¬2ç« |æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|åŸºç¡€ç†è®º)[\s\S]{1000,20000}?)(?=ç¬¬ä¸‰ç« |ç¬¬3ç« |3\s)',
        'methodology_alt': r'((?:ç¬¬ä¸‰ç« |ç¬¬3ç« |ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å›¾åƒåˆ†å‰²)[\s\S]{1000,15000}?)(?=ç¬¬å››ç« |ç¬¬4ç« |4\s)',
        'results_alt': r'((?:ç¬¬å››ç« |ç¬¬4ç« |å®éªŒç»“æœ|ç»“æœåˆ†æ|æ¨¡å‹æ„å»º)[\s\S]{1000,15000}?)(?=ç¬¬äº”ç« |ç¬¬5ç« |5\s|ç»“è®º)',
        
        # å…¶ä»–ç« èŠ‚ä¿æŒä¸å˜
        'conclusion': r'((?:ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›)[\s\S]{200,8000}?)(?=å‚\s*è€ƒ\s*æ–‡\s*çŒ®|è‡´è°¢|é™„å½•|$)',
        'references': r'((?:å‚\s*è€ƒ\s*æ–‡\s*çŒ®|REFERENCES?|References?)(?:\s*\n+\s*(?:\[?\d+\]?|\d+\.|\ã€\d+ã€‘|\(\d+\))\s*[\s\S]*?)?)(?:\n+\s*(?:è‡´\s*è°¢|é™„\s*å½•|ACKNOWLEDGMENT|$)|$)',
        'acknowledgement': r'(è‡´\s*è°¢[\s\S]{100,2000}?)(?=é™„å½•|å¤§è¿ç†å·¥å¤§å­¦|$)',
    }
    
    print("ğŸ” å¢å¼ºç« èŠ‚æ£€æµ‹å¼€å§‹...")
    
    # è¯†åˆ«ç« èŠ‚
    detected_sections = {}
    for section_name, pattern in enhanced_patterns.items():
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            section_content = match.group(1).strip()
            detected_sections[section_name] = {
                'content': section_content,
                'start_pos': match.start(),
                'end_pos': match.end(),
                'length': len(section_content)
            }
            
            # æå–ç« èŠ‚æ ‡é¢˜
            title = extract_chapter_title(section_content, section_name)
            detected_sections[section_name]['title'] = title
            
            print(f"    å‘ç°ç« èŠ‚: {section_name} | æ ‡é¢˜: {title} | é•¿åº¦: {len(section_content)}")
    
    return detected_sections

def extract_chapter_title(content: str, section_name: str) -> str:
    """ä»ç« èŠ‚å†…å®¹ä¸­æå–æ ‡é¢˜"""
    
    # é¦–å…ˆå°è¯•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
    first_line = content.split('\n')[0].strip()
    
    # é’ˆå¯¹ä¸åŒç±»å‹çš„ç« èŠ‚ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜æå–ç­–ç•¥
    if section_name.startswith('chapter_'):
        # æ•°å­—ç« èŠ‚æ ¼å¼
        title_match = re.match(r'(\d+\s+[^\d\n\r]{2,50})', first_line)
        if title_match:
            return title_match.group(1).strip()
    
    elif section_name == 'abstract_cn':
        return 'æ‘˜è¦'
    elif section_name == 'abstract_en':
        return 'Abstract'
    elif section_name == 'keywords_cn':
        return 'å…³é”®è¯'
    elif section_name == 'keywords_en':
        return 'Keywords'
    elif section_name == 'toc':
        return 'ç›®å½•'
    elif section_name == 'conclusion':
        return 'ç»“è®º'
    elif section_name == 'references':
        return 'å‚è€ƒæ–‡çŒ®'
    elif section_name == 'acknowledgement':
        return 'è‡´è°¢'
    
    # é€šç”¨æ ‡é¢˜æå–
    if first_line and len(first_line) < 100:
        return first_line
    
    return section_name

def analyze_document_structure(file_path: str):
    """åˆ†ææ–‡æ¡£ç»“æ„"""
    
    print(f"ğŸ“– åˆ†ææ–‡æ¡£: {os.path.basename(file_path)}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"   ğŸ“„ æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æ‰§è¡Œå¢å¼ºç« èŠ‚æ£€æµ‹
        sections = enhanced_chapter_detection(content)
        
        print(f"\nğŸ“Š æ£€æµ‹ç»“æœæ±‡æ€»:")
        print(f"   å‘ç°ç« èŠ‚æ•°é‡: {len(sections)}")
        
        # æŒ‰ç« èŠ‚é¡ºåºæ’åº
        ordered_sections = []
        
        # é¦–å…ˆæ·»åŠ å‰ç½®ç« èŠ‚
        for section_name in ['abstract_cn', 'abstract_en', 'keywords_cn', 'keywords_en', 'toc']:
            if section_name in sections:
                ordered_sections.append((section_name, sections[section_name]))
        
        # æ·»åŠ ä¸»è¦ç« èŠ‚
        for i in range(1, 6):
            chapter_key = f'chapter_{i}'
            if chapter_key in sections:
                ordered_sections.append((chapter_key, sections[chapter_key]))
        
        # æ·»åŠ å¤‡é€‰ä¸»è¦ç« èŠ‚
        for section_name in ['introduction_alt', 'literature_alt', 'methodology_alt', 'results_alt']:
            if section_name in sections and f"chapter_{section_name.split('_')[0]}" not in sections:
                ordered_sections.append((section_name, sections[section_name]))
        
        # æ·»åŠ åç½®ç« èŠ‚
        for section_name in ['conclusion', 'references', 'acknowledgement']:
            if section_name in sections:
                ordered_sections.append((section_name, sections[section_name]))
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        print(f"\nğŸ“‹ ç« èŠ‚è¯¦æƒ…:")
        for section_name, section_info in ordered_sections:
            title = section_info['title']
            length = section_info['length']
            start_pos = section_info['start_pos']
            
            print(f"   ğŸ“– {section_name:15} | {title:30} | {length:>5} å­—ç¬¦ | ä½ç½®: {start_pos}")
        
        # ç”Ÿæˆè¾¹ç•Œä¿¡æ¯
        boundaries = {}
        for section_name, section_info in sections.items():
            # è®¡ç®—è¡Œå·
            start_line = content[:section_info['start_pos']].count('\n') + 1
            end_line = content[:section_info['end_pos']].count('\n') + 1
            
            boundaries[section_name] = {
                'section_name': section_name,
                'title': section_info['title'],
                'start_position': section_info['start_pos'],
                'end_position': section_info['end_pos'],
                'content_length': section_info['length'],
                'boundaries': {
                    'start_line': start_line,
                    'end_line': end_line
                },
                'boundary_confidence': 0.9  # é«˜ç½®ä¿¡åº¦ï¼Œå› ä¸ºæ˜¯ç²¾ç¡®åŒ¹é…
            }
        
        return {
            'sections': {name: info['content'] for name, info in sections.items()},
            'boundaries': boundaries,
            'ordered_sections': [name for name, _ in ordered_sections]
        }
        
    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_enhanced_detection():
    """æµ‹è¯•å¢å¼ºæ£€æµ‹åŠŸèƒ½"""
    
    print("ğŸ” å¢å¼ºç« èŠ‚æ£€æµ‹æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•ç¼“å­˜æ–‡æ¡£
    cache_dir = os.path.join(current_dir, 'cache', 'documents')
    if not os.path.exists(cache_dir):
        print("   âš ï¸ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ‰¾åˆ°ç›®æ ‡æ–‡æ¡£
    target_file = None
    for file in os.listdir(cache_dir):
        if file.endswith('.md') and 'HUSSEIN' in file:
            target_file = os.path.join(cache_dir, file)
            break
    
    if not target_file:
        print("   âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡æ–‡æ¡£")
        return
    
    # åˆ†ææ–‡æ¡£ç»“æ„
    result = analyze_document_structure(target_file)
    
    if result:
        print(f"\n åˆ†æå®Œæˆ!")
        
        # ä¿å­˜åˆ†æç»“æœ
        output_file = os.path.join(current_dir, 'enhanced_section_detection_result.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            # åªä¿å­˜è¾¹ç•Œä¿¡æ¯ï¼Œå› ä¸ºå†…å®¹å¤ªå¤§
            json.dump({
                'boundaries': result['boundaries'],
                'ordered_sections': result['ordered_sections'],
                'section_count': len(result['sections']),
                'detection_method': 'enhanced_numeric_chapters'
            }, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {os.path.basename(output_file)}")
        
        # å¯¹æ¯”åŸå§‹ç»“æœ
        original_json = os.path.join(current_dir, 'data', 'output', '1_ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹_21709201_HUSSEIN Y. Y. ALGHALBAN_LW_pro_extracted_info.json')
        if os.path.exists(original_json):
            with open(original_json, 'r', encoding='utf-8') as f:
                original_data = json.load(f)
            
            original_boundaries = original_data.get('extracted_info', {}).get('section_boundaries', {})
            
            print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
            print(f"   åŸå§‹è¯†åˆ«: {len(original_boundaries)} ä¸ªç« èŠ‚")
            print(f"   å¢å¼ºè¯†åˆ«: {len(result['boundaries'])} ä¸ªç« èŠ‚")
            print(f"   æ”¹è¿›æå‡: +{len(result['boundaries']) - len(original_boundaries)} ä¸ªç« èŠ‚")
            
            # æ˜¾ç¤ºæ–°è¯†åˆ«çš„ç« èŠ‚
            original_keys = set(original_boundaries.keys())
            enhanced_keys = set(result['boundaries'].keys())
            new_sections = enhanced_keys - original_keys
            
            if new_sections:
                print(f"\nğŸ†• æ–°è¯†åˆ«çš„ç« èŠ‚:")
                for section in sorted(new_sections):
                    info = result['boundaries'][section]
                    print(f"   ğŸ“– {section}: {info['title']}")

if __name__ == "__main__":
    test_enhanced_detection()

