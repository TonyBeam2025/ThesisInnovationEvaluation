#!/usr/bin/env python3
"""
å®Œå–„çš„å¤§å‹å­¦ä½è®ºæ–‡æ–‡æ¡£ä¿¡æ¯æŠ½å–ç³»ç»Ÿ
é›†æˆåˆ†æ­¥æŠ½å–ç­–ç•¥ã€ç»“æ„åŒ–åˆ†æã€å¿«é€Ÿå®šä½å’Œæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æŠ€æœ¯
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

@dataclass
class DocumentSection:
    """æ–‡æ¡£æ®µè½ä¿¡æ¯"""
    name: str
    start_line: int
    end_line: int
    content: str
    char_count: int

@dataclass
class ExtractionResult:
    """æå–ç»“æœç»Ÿè®¡"""
    total_fields: int
    extracted_fields: int
    completeness: float
    missing_fields: List[str]
    confidence_score: float

class EnhancedThesisExtractor:
    """å¢å¼ºç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–å™¨"""
    
    def __init__(self):
        self.field_patterns = self._init_field_patterns()
        self.section_patterns = self._init_section_patterns()
        self.expected_fields = self._init_expected_fields()
    
    def _init_field_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–å­—æ®µåŒ¹é…æ¨¡å¼"""
        return {
            'ThesisNumber': [
                r'è®ºæ–‡ç¼–å·[:ï¼š]\s*(\S+)',
                r'ç¼–\s*å·[:ï¼š]\s*(\S+)',
                r'å­¦ä½è®ºæ–‡ç¼–å·[:ï¼š]\s*(\S+)'
            ],
            'ChineseTitle': [
                r'(?:è®ºæ–‡)?é¢˜ç›®[:ï¼š]\s*(.+)',
                r'ä¸­æ–‡é¢˜ç›®[:ï¼š]\s*(.+)',
                r'^([^:\n]{15,80}ç ”ç©¶[^:\n]*)$',  # ç‹¬ç«‹è¡Œçš„ç ”ç©¶æ ‡é¢˜
                r'^([^:\n]{10,60}(?:åˆ†æ|è®¾è®¡|ç³»ç»Ÿ|æ–¹æ³•|æŠ€æœ¯)[^:\n]*)$'
            ],
            'EnglishTitle': [
                r'è‹±æ–‡é¢˜ç›®[:ï¼š]\s*(.+)',
                r'English Title[:ï¼š]\s*(.+)',
                r'^([A-Z][A-Za-z\s,:-]{20,100})$'  # è‹±æ–‡æ ‡é¢˜æ ¼å¼
            ],
            'ChineseAuthor': [
                r'ä½œè€…å§“å[:ï¼š]\s*(.+)',
                r'å§“\s*å[:ï¼š]\s*(.+)',
                r'ç”³è¯·äºº[:ï¼š]\s*(.+)',
                r'å­¦\s*ç”Ÿ[:ï¼š]\s*(.+)',
                r'ç ”ç©¶ç”Ÿ[:ï¼š]\s*(.+)'
            ],
            'EnglishAuthor': [
                r'Author[:ï¼š]\s*(.+)',
                r'Candidate[:ï¼š]\s*(.+)',
                r'Student[:ï¼š]\s*(.+)',
                r'Name[:ï¼š]\s*(.+)'
            ],
            'ChineseUniversity': [
                r'åŸ¹å…»å•ä½[:ï¼š]\s*(.+)',
                r'å­¦æ ¡[:ï¼š]\s*(.+)',
                r'å•ä½[:ï¼š]\s*(.+)',
                r'é™¢æ ¡[:ï¼š]\s*(.+)',
                r'(\w+å¤§å­¦)',
                r'(\w+å­¦é™¢)(?![\wå­¦ç§‘ä¸“ä¸š])'
            ],
            'EnglishUniversity': [
                r'University[:ï¼š]\s*(.+)',
                r'Institution[:ï¼š]\s*(.+)',
                r'School[:ï¼š]\s*(.+)'
            ],
            'DegreeLevel': [
                r'ç”³è¯·å­¦ä½çº§åˆ«[:ï¼š]\s*(.+)',
                r'å­¦ä½çº§åˆ«[:ï¼š]\s*(.+)',
                r'å­¦ä½[:ï¼š]\s*(.+)',
                r'(åšå£«|ç¡•å£«|å­¦å£«)å­¦ä½',
                r'(åšå£«|ç¡•å£«|å­¦å£«)',
                r'(Doctor|Master|Bachelor)'
            ],
            'ChineseMajor': [
                r'å­¦ç§‘ä¸“ä¸š[:ï¼š]\s*(.+)',
                r'ä¸“ä¸š[:ï¼š]\s*(.+)',
                r'å­¦ç§‘[:ï¼š]\s*(.+)',
                r'Major[:ï¼š]\s*(.+)'
            ],
            'ChineseResearchDirection': [
                r'ç ”ç©¶æ–¹å‘[:ï¼š]\s*(.+)',
                r'ä¸“ä¸šæ–¹å‘[:ï¼š]\s*(.+)',
                r'Direction[:ï¼š]\s*(.+)'
            ],
            'ChineseSupervisor': [
                r'æŒ‡å¯¼æ•™å¸ˆ[å§“å]*[:ï¼š]\s*(.+?)\s*(?:æ•™æˆ|å‰¯æ•™æˆ|è®²å¸ˆ)',
                r'å¯¼å¸ˆ[:ï¼š]\s*(.+?)\s*(?:æ•™æˆ|å‰¯æ•™æˆ|è®²å¸ˆ)',
                r'æŒ‡å¯¼æ•™å¸ˆ[:ï¼š]\s*(.+)',
                r'Supervisor[:ï¼š]\s*(?:Prof\.\s*)?(.+)'
            ],
            'ChineseSupervisorTitle': [
                r'æŒ‡å¯¼æ•™å¸ˆ.*[:ï¼š]\s*.+?\s*(æ•™æˆ|å‰¯æ•™æˆ|è®²å¸ˆ)',
                r'èŒ\s*ç§°[:ï¼š]\s*(.+)'
            ],
            'College': [
                r'åŸ¹å…»å­¦é™¢[:ï¼š]\s*(.+)',
                r'å­¦é™¢[:ï¼š]\s*(.+å­¦é™¢)',
                r'é™¢ç³»[:ï¼š]\s*(.+)'
            ],
            'DefenseDate': [
                r'ç­”è¾©æ—¥æœŸ[:ï¼š]\s*(.+)',
                r'è®ºæ–‡ç­”è¾©æ—¥æœŸ[:ï¼š]\s*(.+)',
                r'Defense Date[:ï¼š]\s*(.+)',
                r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
                r'(\d{4}-\d{1,2}-\d{1,2})'
            ],
            'DegreeGrantingInstitution': [
                r'å­¦ä½æˆäºˆå•ä½[:ï¼š]\s*(.+)',
                r'æˆäºˆå•ä½[:ï¼š]\s*(.+)'
            ]
        }
    
    def _init_section_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–ç« èŠ‚è¯†åˆ«æ¨¡å¼"""
        return {
            'cover': ['å°é¢', 'æ‰‰é¡µ', 'å­¦ä½è®ºæ–‡'],
            'abstract_cn': [r'æ‘˜\s*è¦', 'ä¸­æ–‡æ‘˜è¦', 'å†…å®¹æ‘˜è¦'],
            'abstract_en': ['abstract', 'english abstract'],
            'keywords_cn': ['å…³é”®è¯', 'ä¸»é¢˜è¯'],
            'keywords_en': ['key words', 'keywords'],
            'toc': [r'ç›®\s*å½•', 'contents', 'ç›®æ¬¡'],
            'introduction': ['ç»ªè®º', 'å¼•è¨€', 'æ¦‚è¿°', 'ç¬¬ä¸€ç« ', 'ç¬¬1ç« '],
            'conclusion': ['ç»“è®º', 'æ€»ç»“', 'ç»“è¯­', 'å°ç»“'],
            'references': ['å‚è€ƒæ–‡çŒ®', 'references', 'å¼•ç”¨æ–‡çŒ®'],
            'appendix': ['é™„å½•', 'appendix'],
            'acknowledgement': ['è‡´è°¢', 'acknowledgement', 'thanks'],
            'achievements': ['æ”»è¯».*å­¦ä½.*æœŸé—´.*æˆæœ', 'å‘è¡¨.*è®ºæ–‡', 'ç ”ç©¶æˆæœ'],
            'biography': ['ä¸ªäººç®€å†', 'ä½œè€…ç®€å†', 'biography', 'curriculum vitae']
        }
    
    def _init_expected_fields(self) -> List[str]:
        """åˆå§‹åŒ–æœŸæœ›æå–çš„å­—æ®µåˆ—è¡¨"""
        return [
            'ThesisNumber', 'ChineseTitle', 'EnglishTitle',
            'ChineseAuthor', 'EnglishAuthor',
            'ChineseUniversity', 'EnglishUniversity',
            'DegreeLevel', 'ChineseMajor', 'EnglishMajor',
            'ChineseResearchDirection', 'EnglishResearchDirection',
            'ChineseSupervisor', 'EnglishSupervisor',
            'ChineseSupervisorTitle', 'EnglishSupervisorTitle',
            'College', 'DefenseDate', 'DegreeGrantingInstitution',
            'ChineseAbstract', 'EnglishAbstract',
            'ChineseKeywords', 'EnglishKeywords',
            'TableOfContents', 'LiteratureReview',
            'ResearchMethods', 'TheoreticalFramework',
            'MainInnovations', 'PracticalProblems',
            'ProposedSolutions', 'ResearchConclusions',
            'ApplicationValue', 'ReferenceList'
        ]
    
    def analyze_document_structure(self, content: str) -> Dict[str, DocumentSection]:
        """åˆ†ææ–‡æ¡£ç»“æ„ï¼Œå¿«é€Ÿå®šä½å„ä¸ªç« èŠ‚"""
        lines = content.split('\n')
        sections = {}
        
        print("ğŸ” åˆ†ææ–‡æ¡£ç»“æ„...")
        
        for section_name, patterns in self.section_patterns.items():
            for i, line in enumerate(lines):
                line_clean = line.strip().lower()
                
                for pattern in patterns:
                    if re.search(pattern, line_clean, re.IGNORECASE):
                        # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®
                        end_line = self._find_section_end(lines, i, section_name)
                        
                        section_content = '\n'.join(lines[i:end_line])
                        sections[section_name] = DocumentSection(
                            name=section_name,
                            start_line=i,
                            end_line=end_line,
                            content=section_content,
                            char_count=len(section_content)
                        )
                        
                        print(f"   ğŸ“ {section_name}: ç¬¬{i+1}-{end_line}è¡Œ ({len(section_content):,} å­—ç¬¦)")
                        break
                
                if section_name in sections:
                    break
        
        return sections
    
    def _find_section_end(self, lines: List[str], start: int, section_type: str) -> int:
        """æ™ºèƒ½ç¡®å®šç« èŠ‚ç»“æŸä½ç½®"""
        if section_type == 'abstract_cn':
            # ä¸­æ–‡æ‘˜è¦ï¼šå¯»æ‰¾å…³é”®è¯æˆ–Abstract
            for i in range(start + 1, min(start + 100, len(lines))):
                if re.search(r'å…³é”®è¯|keywords?|abstract', lines[i], re.IGNORECASE):
                    return i
        
        elif section_type == 'abstract_en':
            # è‹±æ–‡æ‘˜è¦ï¼šå¯»æ‰¾Key wordsæˆ–ç›®å½•
            for i in range(start + 1, min(start + 100, len(lines))):
                if re.search(r'key\s*words?|ç›®\s*å½•', lines[i], re.IGNORECASE):
                    return i
        
        elif section_type == 'toc':
            # ç›®å½•ï¼šå¯»æ‰¾ç¬¬ä¸€ç« æˆ–ç»ªè®º
            for i in range(start + 1, min(start + 200, len(lines))):
                if re.search(r'ç¬¬ä¸€ç« |ç¬¬1ç« |ç»ªè®º|å¼•è¨€', lines[i], re.IGNORECASE):
                    return i
        
        elif section_type == 'references':
            # å‚è€ƒæ–‡çŒ®ï¼šå¯»æ‰¾è‡´è°¢æˆ–æ”»è¯»å­¦ä½æœŸé—´
            for i in range(start + 1, len(lines)):
                line = lines[i].strip()
                if re.search(r'è‡´è°¢|æ”»è¯».*å­¦ä½.*æœŸé—´|ä¸ªäººç®€å†|é™„å½•', line, re.IGNORECASE) and len(line) < 50:
                    return i
        
        # é»˜è®¤ï¼šæŸ¥æ‰¾ä¸‹ä¸€ä¸ªç« èŠ‚æ ‡é¢˜æˆ–æ–‡æ¡£ç»“æŸ
        for i in range(start + 1, min(start + 50, len(lines))):
            line = lines[i].strip()
            if line and (line.startswith('#') or re.search(r'ç¬¬.*ç« |^[A-Z\s]{3,20}$', line)):
                return i
        
        return min(start + 50, len(lines))
    
    def extract_metadata_with_patterns(self, content: str, field_name: str) -> List[str]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æå–ç‰¹å®šå­—æ®µ"""
        results = []
        patterns = self.field_patterns.get(field_name, [])
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else (match[1] if len(match) > 1 else '')
                
                match = match.strip()
                if match and len(match) > 1 and len(match) < 200:
                    results.append(match)
        
        return results
    
    def extract_structured_content(self, sections: Dict[str, DocumentSection]) -> Dict[str, str]:
        """ä»ç»“æ„åŒ–ç« èŠ‚ä¸­æå–å†…å®¹"""
        content_fields = {}
        
        # æå–ä¸­æ–‡æ‘˜è¦
        if 'abstract_cn' in sections:
            content = sections['abstract_cn'].content
            # æ¸…ç†æ‘˜è¦å†…å®¹
            lines = content.split('\n')[1:]  # è·³è¿‡æ ‡é¢˜è¡Œ
            abstract = '\n'.join([line.strip() for line in lines if line.strip() and not re.search(r'å…³é”®è¯|keywords?', line, re.IGNORECASE)])
            if len(abstract) > 100:
                content_fields['ChineseAbstract'] = abstract
        
        # æå–è‹±æ–‡æ‘˜è¦
        if 'abstract_en' in sections:
            content = sections['abstract_en'].content
            lines = content.split('\n')[1:]  # è·³è¿‡æ ‡é¢˜è¡Œ
            abstract = '\n'.join([line.strip() for line in lines if line.strip() and not re.search(r'key\s*words?', line, re.IGNORECASE)])
            if len(abstract) > 100:
                content_fields['EnglishAbstract'] = abstract
        
        # æå–å…³é”®è¯
        for section_name in ['abstract_cn', 'keywords_cn']:
            if section_name in sections:
                content = sections[section_name].content
                keywords_match = re.search(r'å…³é”®è¯[ï¼š:](.*?)(?:\n|$)', content, re.IGNORECASE | re.DOTALL)
                if keywords_match:
                    keywords = keywords_match.group(1).strip()
                    if keywords:
                        content_fields['ChineseKeywords'] = keywords
                        break
        
        for section_name in ['abstract_en', 'keywords_en']:
            if section_name in sections:
                content = sections[section_name].content
                keywords_match = re.search(r'key\s*words?[ï¼š:](.*?)(?:\n|$)', content, re.IGNORECASE | re.DOTALL)
                if keywords_match:
                    keywords = keywords_match.group(1).strip()
                    if keywords:
                        content_fields['EnglishKeywords'] = keywords
                        break
        
        # æå–ç›®å½•
        if 'toc' in sections:
            content = sections['toc'].content
            toc_lines = []
            for line in content.split('\n')[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                line = line.strip()
                if line and re.search(r'ç¬¬.*ç« |[0-9]+\.[0-9]+|###', line):
                    toc_lines.append(line)
            
            if toc_lines:
                content_fields['TableOfContents'] = '\n'.join(toc_lines)
        
        return content_fields
    
    def extract_references_accurately(self, content: str) -> List[str]:
        """ç²¾ç¡®æå–å‚è€ƒæ–‡çŒ®"""
        lines = content.split('\n')
        
        # æ‰¾åˆ°å‚è€ƒæ–‡çŒ®å¼€å§‹ä½ç½®ï¼ˆç¬¬ä¸€ä¸ª[1]æ¡ç›®ï¼‰
        ref_start = None
        for i, line in enumerate(lines):
            if re.match(r'^\[\s*1\s*\]', line.strip()):
                ref_start = i
                break
        
        if not ref_start:
            return []
        
        # æ‰¾åˆ°å‚è€ƒæ–‡çŒ®ç»“æŸä½ç½®
        ref_end = len(lines)
        end_patterns = ['è‡´è°¢', 'æ”»è¯».*å­¦ä½.*æœŸé—´', 'ä¸ªäººç®€å†', 'é™„å½•', 'å£°æ˜']
        
        for i, line in enumerate(lines[ref_start:], ref_start):
            line = line.strip()
            if line and not re.match(r'^\[\s*\d+\s*\]', line):
                for pattern in end_patterns:
                    if re.search(pattern, line) and len(line) < 100:
                        ref_end = i
                        break
                if ref_end != len(lines):
                    break
        
        # æå–å‚è€ƒæ–‡çŒ®æ¡ç›®
        ref_lines = lines[ref_start:ref_end]
        references = []
        current_ref = ""
        
        for line in ref_lines:
            line = line.strip()
            
            if re.match(r'^\[\s*\d+\s*\]', line):
                if current_ref:
                    references.append(' '.join(current_ref.split()))
                current_ref = line
            elif line and current_ref:
                current_ref += " " + line
            elif not line and current_ref:
                references.append(' '.join(current_ref.split()))
                current_ref = ""
        
        if current_ref:
            references.append(' '.join(current_ref.split()))
        
        # è¿‡æ»¤å’Œæ¸…ç†
        cleaned_refs = []
        for ref in references:
            if len(ref) > 30 and len(ref) < 1000:  # åˆç†é•¿åº¦èŒƒå›´
                cleaned_refs.append(ref)
        
        return cleaned_refs
    
    def comprehensive_extract(self, content: str) -> Dict[str, Any]:
        """ç»¼åˆä¿¡æ¯æå–"""
        print("ğŸ¯ å¼€å§‹ç»¼åˆä¿¡æ¯æå–")
        
        # 1. åˆ†ææ–‡æ¡£ç»“æ„
        sections = self.analyze_document_structure(content)
        
        # 2. ä»å‰ç½®éƒ¨åˆ†ï¼ˆå‰20%å†…å®¹ï¼‰æå–å…ƒæ•°æ®
        front_matter_size = min(20000, len(content) // 5)
        front_matter = content[:front_matter_size]
        
        print("ğŸ“‹ ä»å‰ç½®éƒ¨åˆ†æå–å…ƒæ•°æ®...")
        extracted_info = {}
        
        # æå–åŸºæœ¬ä¿¡æ¯
        for field_name in ['ThesisNumber', 'ChineseTitle', 'EnglishTitle', 'ChineseAuthor', 
                          'EnglishAuthor', 'ChineseUniversity', 'EnglishUniversity', 
                          'DegreeLevel', 'ChineseMajor', 'ChineseResearchDirection',
                          'ChineseSupervisor', 'ChineseSupervisorTitle', 'College',
                          'DefenseDate', 'DegreeGrantingInstitution']:
            
            matches = self.extract_metadata_with_patterns(front_matter, field_name)
            if matches:
                # é€‰æ‹©æœ€ä½³åŒ¹é…
                best_match = self._select_best_match(matches, field_name)
                if best_match:
                    extracted_info[field_name] = best_match
                    print(f"    {field_name}: {best_match}")
        
        # 3. ä»ç»“æ„åŒ–ç« èŠ‚æå–å†…å®¹
        print("ğŸ“„ ä»ç»“æ„åŒ–ç« èŠ‚æå–å†…å®¹...")
        content_fields = self.extract_structured_content(sections)
        extracted_info.update(content_fields)
        
        # 4. æå–å‚è€ƒæ–‡çŒ®
        print("ğŸ“š æå–å‚è€ƒæ–‡çŒ®...")
        references = self.extract_references_accurately(content)
        if references:
            extracted_info['ReferenceList'] = references
            print(f"    å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
        
        # 5. è¡¥å……æ¨ç†ä¿¡æ¯
        print("ğŸ§  è¡¥å……æ¨ç†ä¿¡æ¯...")
        inferred_fields = self._infer_missing_fields(extracted_info, content)
        extracted_info.update(inferred_fields)
        
        return extracted_info
    
    def _select_best_match(self, matches: List[str], field_name: str) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³åŒ¹é…ç»“æœ"""
        if not matches:
            return None
        
        # å»é‡
        unique_matches = list(set(matches))
        
        if len(unique_matches) == 1:
            return unique_matches[0]
        
        # æ ¹æ®å­—æ®µç±»å‹é€‰æ‹©æœ€ä½³åŒ¹é…
        if field_name in ['ChineseTitle', 'EnglishTitle']:
            # é€‰æ‹©æœ€é•¿çš„æ ‡é¢˜ï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
            return max(unique_matches, key=len)
        
        elif field_name in ['ChineseAuthor', 'EnglishAuthor']:
            # é€‰æ‹©çœ‹èµ·æ¥æœ€åƒäººåçš„
            for match in unique_matches:
                if 2 <= len(match) <= 10 and not any(char in match for char in '0123456789'):
                    return match
        
        elif field_name == 'DegreeLevel':
            # ä¼˜å…ˆé€‰æ‹©å®Œæ•´çš„å­¦ä½åç§°
            priority = ['åšå£«å­¦ä½', 'ç¡•å£«å­¦ä½', 'å­¦å£«å­¦ä½', 'åšå£«', 'ç¡•å£«', 'å­¦å£«']
            for level in priority:
                if level in unique_matches:
                    return level
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªéç©ºåŒ¹é…
        return unique_matches[0]
    
    def _infer_missing_fields(self, extracted_info: Dict[str, Any], content: str) -> Dict[str, Any]:
        """æ¨ç†ç¼ºå¤±å­—æ®µ"""
        inferred = {}
        
        # æ¨ç†è‹±æ–‡ä¿¡æ¯
        if 'ChineseUniversity' in extracted_info and 'EnglishUniversity' not in extracted_info:
            if 'åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦' in extracted_info['ChineseUniversity']:
                inferred['EnglishUniversity'] = 'Beihang University'
            elif 'æ¸…åå¤§å­¦' in extracted_info['ChineseUniversity']:
                inferred['EnglishUniversity'] = 'Tsinghua University'
            elif 'åŒ—äº¬å¤§å­¦' in extracted_info['ChineseUniversity']:
                inferred['EnglishUniversity'] = 'Peking University'
        
        # æ¨ç†æŒ‡å¯¼æ•™å¸ˆèŒç§°
        if 'ChineseSupervisor' in extracted_info and 'ChineseSupervisorTitle' not in extracted_info:
            inferred['ChineseSupervisorTitle'] = 'æ•™æˆ'  # é»˜è®¤å‡è®¾ä¸ºæ•™æˆ
        
        # æ¨ç†ç ”ç©¶æ–¹æ³•
        if 'ResearchMethods' not in extracted_info:
            if any(keyword in content.lower() for keyword in ['å®éªŒ', 'æµ‹è¯•', 'åˆ¶å¤‡', 'åˆæˆ']):
                inferred['ResearchMethods'] = 'å®éªŒç ”ç©¶æ–¹æ³•'
            elif any(keyword in content.lower() for keyword in ['ä»¿çœŸ', 'æ¨¡æ‹Ÿ', 'è®¡ç®—']):
                inferred['ResearchMethods'] = 'ç†è®ºè®¡ç®—ä¸ä»¿çœŸç ”ç©¶'
            else:
                inferred['ResearchMethods'] = 'ç†è®ºä¸å®éªŒç›¸ç»“åˆçš„ç ”ç©¶æ–¹æ³•'
        
        # æ¨ç†ä¸»è¦åˆ›æ–°ç‚¹
        if 'MainInnovations' not in extracted_info:
            innovations = []
            if 'ChineseAbstract' in extracted_info:
                abstract = extracted_info['ChineseAbstract']
                if 'ä¼˜åŒ–' in abstract:
                    innovations.append('æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ç ”ç©¶')
                if 'æ–°å‹' in abstract or 'æ–°é¢–' in abstract:
                    innovations.append('æ–°å‹ææ–™/æ–¹æ³•çš„å¼€å‘')
                if 'æœºç†' in abstract or 'æœºåˆ¶' in abstract:
                    innovations.append('æœºç†æœºåˆ¶ç ”ç©¶')
            
            if innovations:
                inferred['MainInnovations'] = innovations
        
        return inferred
    
    def evaluate_extraction_quality(self, extracted_info: Dict[str, Any]) -> ExtractionResult:
        """è¯„ä¼°æå–è´¨é‡"""
        total_fields = len(self.expected_fields)
        extracted_fields = len([k for k, v in extracted_info.items() if v and str(v).strip()])
        completeness = extracted_fields / total_fields * 100
        
        missing_fields = [field for field in self.expected_fields if field not in extracted_info or not extracted_info[field]]
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        confidence_score = self._calculate_confidence_score(extracted_info)
        
        return ExtractionResult(
            total_fields=total_fields,
            extracted_fields=extracted_fields,
            completeness=completeness,
            missing_fields=missing_fields,
            confidence_score=confidence_score
        )
    
    def _calculate_confidence_score(self, extracted_info: Dict[str, Any]) -> float:
        """è®¡ç®—æå–ç½®ä¿¡åº¦åˆ†æ•°"""
        score = 0.0
        
        # æ ¸å¿ƒå­—æ®µæƒé‡
        core_fields = {
            'ChineseTitle': 0.15,
            'ChineseAuthor': 0.10,
            'ChineseUniversity': 0.10,
            'DegreeLevel': 0.08,
            'ChineseAbstract': 0.12,
            'ReferenceList': 0.10
        }
        
        for field, weight in core_fields.items():
            if field in extracted_info and extracted_info[field]:
                if field == 'ChineseAbstract' and len(str(extracted_info[field])) > 500:
                    score += weight
                elif field == 'ReferenceList' and len(extracted_info[field]) > 50:
                    score += weight
                elif field not in ['ChineseAbstract', 'ReferenceList']:
                    score += weight
        
        # å®Œæ•´æ€§å¥–åŠ±
        completeness_bonus = min(0.35, len(extracted_info) / len(self.expected_fields) * 0.35)
        score += completeness_bonus
        
        return min(1.0, score)


def enhanced_thesis_extraction(file_path: str) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–ä¸»å‡½æ•°"""
    
    print(f"ğŸ¯ å¯åŠ¨å¢å¼ºç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–")
    print(f"ğŸ“„ ç›®æ ‡æ–‡ä»¶: {Path(file_path).name}")
    
    extractor = EnhancedThesisExtractor()
    
    # è¯»å–ç¼“å­˜çš„Markdownæ–‡ä»¶
    md_file = Path(__file__).parent / "cache" / "documents" / "51177_b6ac1c475108811bd4a31a6ebcd397df.md"
    
    try:
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {}
    
    print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(content):,} å­—ç¬¦")
    
    # æ‰§è¡Œç»¼åˆæå–
    extracted_info = extractor.comprehensive_extract(content)
    
    # è¯„ä¼°æå–è´¨é‡
    evaluation = extractor.evaluate_extraction_quality(extracted_info)
    
    print(f"\nğŸ“Š æå–è´¨é‡è¯„ä¼°:")
    print(f"   æ€»å­—æ®µæ•°: {evaluation.total_fields}")
    print(f"   å·²æå–å­—æ®µ: {evaluation.extracted_fields}")
    print(f"   å®Œæ•´åº¦: {evaluation.completeness:.1f}%")
    print(f"   ç½®ä¿¡åº¦: {evaluation.confidence_score:.2f}")
    
    if evaluation.missing_fields:
        print(f"   ç¼ºå¤±å­—æ®µ: {', '.join(evaluation.missing_fields[:5])}{'...' if len(evaluation.missing_fields) > 5 else ''}")
    
    return extracted_info


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºç‰ˆæå–å™¨
    result = enhanced_thesis_extraction("data/input/51177.docx")
    
    if result:
        # ä¿å­˜å¢å¼ºç‰ˆç»“æœ
        output_file = Path(__file__).parent / "data" / "output" / "51177_enhanced_extracted_info.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å¢å¼ºç‰ˆæå–ç»“æœå·²ä¿å­˜åˆ°: {output_file.name}")
        print(" å¢å¼ºç‰ˆå­¦ä½è®ºæ–‡ä¿¡æ¯æå–å®Œæˆï¼")

