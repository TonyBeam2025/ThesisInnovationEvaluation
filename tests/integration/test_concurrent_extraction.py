#!/usr/bin/env python3
"""
æµ‹è¯•è®ºæ–‡ä¿¡æ¯ç»“æ„åŒ–æŠ½å–çš„å¹¶å‘å¤„ç†åŠŸèƒ½
éªŒè¯å¤šçº¿ç¨‹å¹¶å‘åˆ†ææ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import os
import sys
import time
import json
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œæ”¯æŒuvç¯å¢ƒ
project_root = PROJECT_ROOT
sys.path.insert(0, str(project_root / "src"))

# ç¡®ä¿åœ¨uvç¯å¢ƒä¸­èƒ½æ­£ç¡®å¯¼å…¥æ¨¡å—
if str(project_root / "src") not in sys.path:
    sys.path.insert(0, str(project_root / "src"))

try:
    from thesis_inno_eval.extract_sections_with_ai import ThesisExtractorPro
    from thesis_inno_eval.ai_client import get_ai_client
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

class ConcurrentExtractionTester:
    """å¹¶å‘æŠ½å–æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.timing_stats = {}
        self.thread_usage = {}
        
    def find_test_documents(self) -> list:
        """æŸ¥æ‰¾å¯ç”¨äºæµ‹è¯•çš„æ–‡æ¡£"""
        # æŒ‡å®šæµ‹è¯•æ–‡æ¡£
        target_doc = "1_è®¡ç®—æœºåº”ç”¨æŠ€æœ¯_17211204005-è‹æ…§å©§-åŸºäºMLPå’ŒSepCNNæ¨¡å‹çš„è—æ–‡æ–‡æœ¬åˆ†ç±»ç ”ç©¶ä¸å®ç°-è®¡ç®—æœºåº”ç”¨æŠ€æœ¯-ç¾¤è¯º.docx"
        target_path = project_root / "data" / "input" / target_doc
        
        test_docs = []
        
        # æ£€æŸ¥æŒ‡å®šæ–‡æ¡£æ˜¯å¦å­˜åœ¨
        if target_path.exists():
            test_docs.append(str(target_path))
            print(f"    æ‰¾åˆ°æŒ‡å®šæµ‹è¯•æ–‡æ¡£: {target_doc}")
        else:
            print(f"   âŒ æœªæ‰¾åˆ°æŒ‡å®šæ–‡æ¡£: {target_path}")
            print("   ğŸ“ æ­£åœ¨æœç´¢å…¶ä»–å¯ç”¨æ–‡æ¡£...")
            
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾å…¶ä»–æ–‡æ¡£
            for file_path in (project_root / "data" / "input").glob("*.docx"):
                if file_path.stat().st_size > 100000:  # æ–‡ä»¶å¤§å°å¤§äº100KB
                    test_docs.append(str(file_path))
                    if len(test_docs) >= 1:  # åªéœ€è¦ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£
                        break
        
        return test_docs
    
    def extract_single_document(self, doc_path: str, doc_id: str) -> dict:
        """æå–å•ä¸ªæ–‡æ¡£çš„ä¿¡æ¯ï¼ˆç”¨äºæµ‹è¯•å¹¶å‘ï¼‰"""
        thread_id = threading.current_thread().ident
        start_time = time.time()
        
        print(f"ğŸ” çº¿ç¨‹ {thread_id} å¼€å§‹å¤„ç†æ–‡æ¡£: {Path(doc_path).name}")
        
        try:
            # åˆ›å»ºæå–å™¨å®ä¾‹
            extractor = ThesisExtractorPro()
            
            # å…ˆè¯»å–æ–‡æ¡£å†…å®¹
            if doc_path.lower().endswith('.pdf'):
                from thesis_inno_eval.extract_sections_with_ai import extract_text_from_pdf
                text = extract_text_from_pdf(doc_path)
            elif doc_path.lower().endswith(('.docx', '.doc')):
                from thesis_inno_eval.extract_sections_with_ai import extract_text_from_word
                text = extract_text_from_word(doc_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {doc_path}")
            
            # æ‰§è¡Œæå–
            result = extractor.extract_with_integrated_strategy(text, doc_path)
            
            elapsed_time = time.time() - start_time
            
            # è®°å½•çº¿ç¨‹ä½¿ç”¨æƒ…å†µ
            if thread_id not in self.thread_usage:
                self.thread_usage[thread_id] = []
            self.thread_usage[thread_id].append({
                'doc_id': doc_id,
                'doc_name': Path(doc_path).name,
                'processing_time': elapsed_time,
                'success': bool(result and result.get('extracted_fields', 0) > 0)
            })
            
            print(f" çº¿ç¨‹ {thread_id} å®Œæˆæ–‡æ¡£å¤„ç†: {Path(doc_path).name} ({elapsed_time:.1f}s)")
            
            return {
                'doc_id': doc_id,
                'doc_path': doc_path,
                'result': result,
                'processing_time': elapsed_time,
                'thread_id': thread_id,
                'success': bool(result and result.get('extracted_fields', 0) > 0)
            }
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"âŒ çº¿ç¨‹ {thread_id} å¤„ç†æ–‡æ¡£å¤±è´¥: {Path(doc_path).name} - {e}")
            
            return {
                'doc_id': doc_id,
                'doc_path': doc_path,
                'result': None,
                'processing_time': elapsed_time,
                'thread_id': thread_id,
                'success': False,
                'error': str(e)
            }
    
    def test_concurrent_processing(self, test_docs: list, max_workers: int = 3) -> dict:
        """æµ‹è¯•å¹¶å‘å¤„ç† - é’ˆå¯¹å•ç¯‡è®ºæ–‡å¤šæ¬¡å¹¶å‘åˆ†æ"""
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•...")
        print(f"   ğŸ“„ æµ‹è¯•æ–‡æ¡£: {Path(test_docs[0]).name}")
        print(f"   ğŸ”§ æœ€å¤§å¹¶å‘æ•°: {max_workers}")
        print(f"   ğŸ”„ å¹¶å‘ä»»åŠ¡æ•°: {max_workers} (åŒä¸€æ–‡æ¡£å¤šæ¬¡å¤„ç†)")
        print(f"   â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
        
        concurrent_start = time.time()
        results = []
        
        # å¯¹åŒä¸€æ–‡æ¡£å¯åŠ¨å¤šä¸ªå¹¶å‘ä»»åŠ¡æ¥æµ‹è¯•å¹¶å‘åŠŸèƒ½
        doc_path = test_docs[0]
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤å¤šä¸ªç›¸åŒæ–‡æ¡£çš„å¤„ç†ä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿå¹¶å‘å¤„ç†åœºæ™¯ï¼‰
            future_to_doc = {
                executor.submit(self.extract_single_document, doc_path, f"concurrent_task_{i+1}"): f"task_{i+1}"
                for i in range(max_workers)
            }
            
            print(f"   ğŸ“¤ å·²æäº¤ {len(future_to_doc)} ä¸ªå¹¶å‘ä»»åŠ¡")
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_doc):
                task_name = future_to_doc[future]
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    results.append(result)
                    
                    status = " æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
                    print(f"   {status} {task_name} ({result['processing_time']:.1f}s, çº¿ç¨‹{result['thread_id']})")
                    
                except Exception as e:
                    print(f"   âŒ å¹¶å‘ä»»åŠ¡å¼‚å¸¸: {task_name} - {e}")
                    results.append({
                        'doc_id': f"{task_name}_error",
                        'doc_path': doc_path,
                        'result': None,
                        'processing_time': 0,
                        'thread_id': None,
                        'success': False,
                        'error': str(e)
                    })
        
        concurrent_total_time = time.time() - concurrent_start
        
        return {
            'concurrent_results': results,
            'concurrent_total_time': concurrent_total_time,
            'max_workers': max_workers,
            'success_count': sum(1 for r in results if r['success']),
            'total_docs': len(test_docs),
            'concurrent_tasks': len(results)
        }
    
    def test_sequential_processing(self, test_docs: list, task_count: int = 3) -> dict:
        """æµ‹è¯•é¡ºåºå¤„ç†ï¼ˆç”¨äºå¯¹æ¯”ï¼‰ - å¯¹åŒä¸€æ–‡æ¡£è¿›è¡Œå¤šæ¬¡é¡ºåºå¤„ç†"""
        print(f"\nğŸ”„ å¼€å§‹é¡ºåºæµ‹è¯•ï¼ˆå¯¹æ¯”åŸºå‡†ï¼‰...")
        print(f"   ğŸ“„ æµ‹è¯•æ–‡æ¡£: {Path(test_docs[0]).name}")
        print(f"   ğŸ”„ é¡ºåºä»»åŠ¡æ•°: {task_count}")
        
        sequential_start = time.time()
        results = []
        doc_path = test_docs[0]
        
        for i in range(task_count):
            print(f"   ğŸ“„ é¡ºåºå¤„ç†ä»»åŠ¡ {i+1}/{task_count}")
            result = self.extract_single_document(doc_path, f"seq_task_{i+1}")
            results.append(result)
            
            status = " æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
            print(f"   {status} ä»»åŠ¡{i+1} ({result['processing_time']:.1f}s)")
        
        sequential_total_time = time.time() - sequential_start
        
        return {
            'sequential_results': results,
            'sequential_total_time': sequential_total_time,
            'success_count': sum(1 for r in results if r['success']),
            'total_docs': len(test_docs),
            'sequential_tasks': len(results)
        }
    
    def analyze_concurrency_performance(self, concurrent_data: dict, sequential_data: dict):
        """åˆ†æå¹¶å‘æ€§èƒ½"""
        print(f"\nğŸ“Š å¹¶å‘æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“„ æµ‹è¯•æ–‡æ¡£: 1ç¯‡ (å¤šæ¬¡å¤„ç†)")
        print(f"ğŸ”§ å¹¶å‘å·¥ä½œçº¿ç¨‹: {concurrent_data['max_workers']}")
        print(f"ï¿½ å¹¶å‘ä»»åŠ¡æ•°: {concurrent_data.get('concurrent_tasks', 0)}")
        print(f"ğŸ”„ é¡ºåºä»»åŠ¡æ•°: {sequential_data.get('sequential_tasks', 0)}")
        print(f" å¹¶å‘æˆåŠŸç‡: {concurrent_data['success_count']}/{concurrent_data.get('concurrent_tasks', 0)} ({concurrent_data['success_count']/concurrent_data.get('concurrent_tasks', 1)*100:.1f}%)")
        print(f" é¡ºåºæˆåŠŸç‡: {sequential_data['success_count']}/{sequential_data.get('sequential_tasks', 0)} ({sequential_data['success_count']/sequential_data.get('sequential_tasks', 1)*100:.1f}%)")
        
        # æ—¶é—´å¯¹æ¯”
        print(f"\nâ±ï¸ æ—¶é—´æ€§èƒ½å¯¹æ¯”:")
        print(f"   ğŸš€ å¹¶å‘æ€»æ—¶é—´: {concurrent_data['concurrent_total_time']:.1f}s")
        print(f"   ğŸ”„ é¡ºåºæ€»æ—¶é—´: {sequential_data['sequential_total_time']:.1f}s")
        
        if sequential_data['sequential_total_time'] > 0:
            speedup = sequential_data['sequential_total_time'] / concurrent_data['concurrent_total_time']
            efficiency = speedup / concurrent_data['max_workers'] * 100
            print(f"   âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
            print(f"   ğŸ“ˆ å¹¶å‘æ•ˆç‡: {efficiency:.1f}%")
            
            if speedup > 1.5:
                print(f"   ğŸ‰ å¹¶å‘å¤„ç†æ˜¾è‘—æå‡æ€§èƒ½!")
            elif speedup > 1.1:
                print(f"   ğŸ‘ å¹¶å‘å¤„ç†æœ‰ä¸€å®šæ•ˆæœ")
            else:
                print(f"   âš ï¸ å¹¶å‘æ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½å—é™äºI/Oæˆ–APIé™åˆ¶")
        
        # çº¿ç¨‹ä½¿ç”¨åˆ†æ
        print(f"\nğŸ§µ çº¿ç¨‹ä½¿ç”¨æƒ…å†µ:")
        for thread_id, tasks in self.thread_usage.items():
            total_time = sum(task['processing_time'] for task in tasks)
            success_count = sum(1 for task in tasks if task['success'])
            print(f"   çº¿ç¨‹ {thread_id}: å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡, æ€»æ—¶é—´ {total_time:.1f}s, æˆåŠŸ {success_count} ä¸ª")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†å¤„ç†ç»“æœ:")
        for result in concurrent_data['concurrent_results']:
            status = "" if result['success'] else "âŒ"
            print(f"   {status} {result['doc_id']} - {result['processing_time']:.1f}s (çº¿ç¨‹{result['thread_id']})")
            
            if result['result']:
                extracted_count = result['result'].get('extracted_fields', 0)
                confidence = result['result'].get('confidence', 0)
                print(f"      ğŸ“Š æå–å­—æ®µ: {extracted_count}, ç½®ä¿¡åº¦: {confidence:.1f}%")
    
    def save_test_results(self, concurrent_data: dict, sequential_data: dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        report_file = project_root / f"concurrent_test_report_{timestamp}.json"
        
        report_data = {
            'test_timestamp': timestamp,
            'test_config': {
                'max_workers': concurrent_data['max_workers'],
                'total_docs': concurrent_data['total_docs']
            },
            'concurrent_performance': {
                'total_time': concurrent_data['concurrent_total_time'],
                'success_count': concurrent_data['success_count'],
                'results': concurrent_data['concurrent_results']
            },
            'sequential_performance': {
                'total_time': sequential_data['sequential_total_time'],
                'success_count': sequential_data['success_count'],
                'results': sequential_data['sequential_results']
            },
            'thread_usage': self.thread_usage,
            'performance_metrics': {
                'speedup': sequential_data['sequential_total_time'] / concurrent_data['concurrent_total_time'] if concurrent_data['concurrent_total_time'] > 0 else 0,
                'efficiency': (sequential_data['sequential_total_time'] / concurrent_data['concurrent_total_time'] / concurrent_data['max_workers'] * 100) if concurrent_data['concurrent_total_time'] > 0 else 0
            }
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ”¬ è®ºæ–‡ä¿¡æ¯ç»“æ„åŒ–æŠ½å–å¹¶å‘å¤„ç†æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥AIå®¢æˆ·ç«¯
    print("ğŸ”§ æ£€æŸ¥AIå®¢æˆ·ç«¯...")
    try:
        ai_client = get_ai_client()
        if ai_client:
            print("    AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
    except Exception as e:
        print(f"   âŒ AIå®¢æˆ·ç«¯æ£€æŸ¥å¤±è´¥: {e}")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ConcurrentExtractionTester()
    
    # æŸ¥æ‰¾æµ‹è¯•æ–‡æ¡£
    print("\nğŸ“ æŸ¥æ‰¾æµ‹è¯•æ–‡æ¡£...")
    test_docs = tester.find_test_documents()
    
    if not test_docs:
        print("   âŒ æœªæ‰¾åˆ°é€‚åˆçš„æµ‹è¯•æ–‡æ¡£")
        print("   ğŸ’¡ è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•æ”¾ç½®ä¸€äº› .docx æˆ– .pdf è®ºæ–‡æ–‡ä»¶")
        return
    
    print(f"    æ‰¾åˆ° {len(test_docs)} ä¸ªæµ‹è¯•æ–‡æ¡£:")
    for doc in test_docs:
        print(f"      ğŸ“„ {Path(doc).name}")
    
    # æ‰§è¡Œæµ‹è¯•
    try:
        # å¹¶å‘æµ‹è¯• - å¯¹åŒä¸€æ–‡æ¡£å¯åŠ¨3ä¸ªå¹¶å‘ä»»åŠ¡
        concurrent_data = tester.test_concurrent_processing(test_docs, max_workers=3)
        
        # é¡ºåºæµ‹è¯• - å¯¹åŒä¸€æ–‡æ¡£è¿›è¡Œ3æ¬¡é¡ºåºå¤„ç†
        sequential_data = tester.test_sequential_processing(test_docs, task_count=3)
        
        # åˆ†æç»“æœ
        tester.analyze_concurrency_performance(concurrent_data, sequential_data)
        
        # ä¿å­˜ç»“æœ
        tester.save_test_results(concurrent_data, sequential_data)
        
        print(f"\nğŸ‰ å¹¶å‘æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print(f"   ğŸ“„ æµ‹è¯•æ–‡æ¡£: {Path(test_docs[0]).name}")
        print(f"   ğŸš€ å¹¶å‘å¤„ç†: {concurrent_data['success_count']}/{concurrent_data.get('concurrent_tasks', 0)} æˆåŠŸ")
        print(f"   ğŸ”„ é¡ºåºå¤„ç†: {sequential_data['success_count']}/{sequential_data.get('sequential_tasks', 0)} æˆåŠŸ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
