#!/usr/bin/env python3
"""
æœ€æ–°è®ºæ–‡æ–‡æ¡£ç« èŠ‚è¾¹ç•Œæµ‹è¯•åˆ†æ
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
import json
import glob
import re
from typing import Dict, Any, List

# æ·»åŠ æºä»£ç è·¯å¾„
current_dir = str(PROJECT_ROOT)
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

def get_latest_documents(count=2):
    """è·å–æœ€æ–°çš„æ–‡æ¡£"""
    cache_dir = os.path.join(current_dir, 'cache', 'documents')
    if not os.path.exists(cache_dir):
        return []
    
    md_files = glob.glob(os.path.join(cache_dir, "*.md"))
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    md_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    documents = []
    for md_file in md_files[:count]:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è¯»å–å…ƒæ•°æ®
            metadata_file = md_file.replace('.md', '_metadata.json')
            metadata = {}
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                except:
                    pass
            
            documents.append({
                'filename': os.path.basename(md_file),
                'filepath': md_file,
                'content': content,
                'metadata': metadata,
                'size': len(content),
                'mtime': os.path.getmtime(md_file)
            })
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {md_file}: {e}")
    
    return documents

def analyze_document_structure(content: str):
    """åˆ†ææ–‡æ¡£ç»“æ„"""
    # åŸºæœ¬ç»Ÿè®¡
    stats = {
        'total_chars': len(content),
        'total_lines': content.count('\n'),
        'total_words': len(content.split()),
        'paragraphs': len([p for p in content.split('\n\n') if p.strip()]),
        'chinese_chars': len(re.findall(r'[\u4e00-\u9fff]', content)),
        'english_words': len(re.findall(r'\b[a-zA-Z]+\b', content))
    }
    
    # ç« èŠ‚æ ‡é¢˜æ£€æµ‹
    chapter_patterns = [
        (r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s+(.+)', 'traditional_chapter'),
        (r'^\d+\.\s*(.{5,50})', 'numbered_section'),
        (r'^(æ‘˜\s*è¦|Abstract|å…³é”®è¯|Keywords?)', 'standard_section'),
        (r'^(å¼•\s*è¨€|ç»ª\s*è®º|å‰\s*è¨€|èƒŒæ™¯)', 'introduction_section'),
        (r'^(æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|å›½å†…å¤–ç ”ç©¶ç°çŠ¶)', 'literature_section'),
        (r'^(ç ”ç©¶æ–¹æ³•|æ–¹æ³•è®º|å®éªŒæ–¹æ³•)', 'methodology_section'),
        (r'^(å®éªŒç»“æœ|ç»“æœåˆ†æ|å®éªŒä¸åˆ†æ)', 'results_section'),
        (r'^(ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›)', 'conclusion_section'),
        (r'^(å‚è€ƒæ–‡çŒ®|REFERENCES?)', 'references_section'),
        (r'^(è‡´\s*è°¢|ACKNOWLEDGMENT)', 'acknowledgment_section'),
    ]
    
    detected_sections = []
    lines = content.split('\n')
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        if not line_stripped:
            continue
            
        for pattern, section_type in chapter_patterns:
            match = re.match(pattern, line_stripped, re.IGNORECASE)
            if match:
                detected_sections.append({
                    'line_number': i + 1,
                    'line_content': line_stripped,
                    'section_type': section_type,
                    'matched_content': match.group(0),
                    'extracted_title': match.group(1) if match.groups() else line_stripped
                })
                break
    
    return stats, detected_sections

def test_references_parsing(content: str) -> Dict[str, Any]:
    """ä¸“é—¨æµ‹è¯•å‚è€ƒæ–‡çŒ®è¯†åˆ«å’Œè§£æ"""
    references_info = {
        'found': False,
        'title_variations': [],
        'total_references': 0,
        'reference_formats': [],
        'spacing_issues': [],
        'content_preview': ''
    }
    
    # å¤šç§å‚è€ƒæ–‡çŒ®æ ‡é¢˜æ ¼å¼
    title_patterns = [
        r'å‚\s*è€ƒ\s*æ–‡\s*çŒ®',  # å…è®¸å­—é—´ç©ºæ ¼
        r'REFERENCES?',
        r'References?',
        r'æ–‡\s*çŒ®',
        r'Bibliography'
    ]
    
    # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®æ ‡é¢˜
    for pattern in title_patterns:
        matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            references_info['title_variations'].append({
                'pattern': pattern,
                'matched_text': match.group(0),
                'position': match.start(),
                'line_number': content[:match.start()].count('\n') + 1
            })
    
    if references_info['title_variations']:
        references_info['found'] = True
        
        # é€‰æ‹©æœ€å¯èƒ½çš„å‚è€ƒæ–‡çŒ®å¼€å§‹ä½ç½®
        best_match = min(references_info['title_variations'], 
                        key=lambda x: x['position'])
        start_pos = best_match['position']
        
        # æŸ¥æ‰¾å‚è€ƒæ–‡çŒ®ç»“æŸä½ç½®
        end_patterns = [r'è‡´\s*è°¢', r'é™„\s*å½•', r'ACKNOWLEDGMENT', r'é™„ä»¶']
        end_pos = len(content)
        
        for pattern in end_patterns:
            match = re.search(pattern, content[start_pos:], re.IGNORECASE)
            if match:
                end_pos = start_pos + match.start()
                break
        
        # æå–å‚è€ƒæ–‡çŒ®å†…å®¹
        ref_content = content[start_pos:end_pos]
        references_info['content_preview'] = ref_content[:500]
        
        # åˆ†æå‚è€ƒæ–‡çŒ®æ¡ç›®æ ¼å¼
        ref_patterns = [
            (r'\[\s*\d+\s*\]', 'æ–¹æ‹¬å·ç¼–å·'),
            (r'\(\s*\d+\s*\)', 'åœ†æ‹¬å·ç¼–å·'),
            (r'ã€\s*\d+\s*ã€‘', 'ä¸­æ–‡æ–¹æ‹¬å·ç¼–å·'),
            (r'^\s*\d+\.\s*', 'æ•°å­—ç‚¹å·'),
            (r'^\s*\d+\s+', 'çº¯æ•°å­—')
        ]
        
        found_formats = set()
        reference_count = 0
        
        for pattern, format_name in ref_patterns:
            matches = re.findall(pattern, ref_content, re.MULTILINE)
            if matches:
                found_formats.add(format_name)
                reference_count = max(reference_count, len(matches))
        
        references_info['reference_formats'] = list(found_formats)
        references_info['total_references'] = reference_count
        
        # æ£€æµ‹ç©ºæ ¼å’Œç©ºè¡Œé—®é¢˜
        lines = ref_content.split('\n')
        for i, line in enumerate(lines):
            # æ£€æŸ¥æ ‡é¢˜ä¸­çš„å¼‚å¸¸ç©ºæ ¼
            if i < 5 and 'å‚' in line and 'æ–‡' in line and 'çŒ®' in line:
                spaces = re.findall(r'å‚\s+è€ƒ|è€ƒ\s+æ–‡|æ–‡\s+çŒ®', line)
                if spaces:
                    references_info['spacing_issues'].append({
                        'type': 'title_spacing',
                        'line': i + 1,
                        'content': line.strip(),
                        'spaces_found': spaces
                    })
            
            # æ£€æŸ¥æ¡ç›®é—´å¼‚å¸¸ç©ºè¡Œ
            if line.strip() == '' and i > 0 and i < len(lines) - 1:
                prev_line = lines[i-1].strip()
                next_line = lines[i+1].strip()
                if (re.match(r'[\[\(ã€]?\d+[\]\)ã€‘]?', prev_line) and 
                    re.match(r'[\[\(ã€]?\d+[\]\)ã€‘]?', next_line)):
                    references_info['spacing_issues'].append({
                        'type': 'item_spacing',
                        'line': i + 1,
                        'context': f"...{prev_line}... [ç©ºè¡Œ] ...{next_line}..."
                    })
    
    return references_info

def test_latest_documents():
    """æµ‹è¯•æœ€æ–°çš„2ä¸ªæ–‡æ¡£"""
    print("ğŸ” æµ‹è¯•æœ€æ–°çš„2ä¸ªè®ºæ–‡æ–‡æ¡£ç« èŠ‚è¾¹ç•Œè¯†åˆ«")
    print("=" * 80)
    
    # è·å–æœ€æ–°æ–‡æ¡£
    documents = get_latest_documents(2)
    
    if len(documents) < 2:
        print(f"âŒ åªæ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£ï¼Œéœ€è¦è‡³å°‘2ä¸ª")
        return
    
    print(f"ğŸ“š æ‰¾åˆ°æœ€æ–°çš„2ä¸ªæ–‡æ¡£:")
    for i, doc in enumerate(documents, 1):
        import datetime
        mtime = datetime.datetime.fromtimestamp(doc['mtime'])
        print(f"   {i}. {doc['filename']}")
        print(f"      ğŸ“… ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"      ğŸ“ å¤§å°: {doc['size']:,} å­—ç¬¦")
    
    # åˆ†ææ¯ä¸ªæ–‡æ¡£
    for i, doc in enumerate(documents, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“– æ–‡æ¡£ {i}: {doc['filename']}")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå…ƒæ•°æ®
        if doc['metadata']:
            print(f"\nğŸ“‹ å…ƒæ•°æ®ä¿¡æ¯:")
            key_fields = ['source_file', 'file_type', 'cached_time']
            for key in key_fields:
                if key in doc['metadata']:
                    print(f"   {key}: {doc['metadata'][key]}")
        
        # ç»“æ„åˆ†æ
        stats, sections = analyze_document_structure(doc['content'])
        
        print(f"\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
        print(f"   ğŸ“ æ€»å­—ç¬¦æ•°: {stats['total_chars']:,}")
        print(f"   ğŸ“ æ€»è¡Œæ•°: {stats['total_lines']:,}")
        print(f"   ğŸ“„ æ®µè½æ•°: {stats['paragraphs']:,}")
        print(f"   ğŸ”¤ è‹±æ–‡å•è¯: {stats['english_words']:,}")
        print(f"   ğŸˆ¶ ä¸­æ–‡å­—ç¬¦: {stats['chinese_chars']:,}")
        
        print(f"\nğŸ” ç« èŠ‚ç»“æ„åˆ†æ:")
        if sections:
            print(f"    æ£€æµ‹åˆ° {len(sections)} ä¸ªç« èŠ‚æ ‡é¢˜:")
            
            section_types = {}
            for section in sections:
                section_type = section['section_type']
                if section_type not in section_types:
                    section_types[section_type] = []
                section_types[section_type].append(section)
            
            for section_type, items in section_types.items():
                print(f"\n   ğŸ“ {section_type}:")
                for item in items[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"      è¡Œ {item['line_number']}: {item['line_content'][:50]}{'...' if len(item['line_content']) > 50 else ''}")
                if len(items) > 3:
                    print(f"      ... è¿˜æœ‰ {len(items) - 3} ä¸ª")
        else:
            print(f"   âŒ æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç« èŠ‚ç»“æ„")
        
        # å†…å®¹é¢„è§ˆ
        print(f"\nğŸ“„ å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):")
        preview = doc['content'][:500].replace('\n', ' ')
        print(f"   {preview}{'...' if len(doc['content']) > 500 else ''}")
        
        # AIæå–å™¨æµ‹è¯•
        try:
            import thesis_inno_eval.extract_sections_with_ai as extract_module
            extractor = extract_module.ThesisExtractorPro()
            
            print(f"\nğŸ§  AIæå–å™¨åˆ†æ:")
            sections_ai = extractor._analyze_document_structure(doc['content'])
            
            non_info_sections = [k for k in sections_ai.keys() if not k.endswith('_info')]
            print(f"    AIè¯†åˆ«åˆ° {len(non_info_sections)} ä¸ªç« èŠ‚:")
            
            for section_name in non_info_sections:
                if f'{section_name}_info' in sections_ai:
                    info = sections_ai[f'{section_name}_info']
                    if isinstance(info, dict):
                        title = info.get('title', 'N/A')
                        length = info.get('content_length', 0)
                        confidence = info.get('boundary_confidence', 0)
                        print(f"      ğŸ“ {section_name}: {title} ({length}å­—ç¬¦, ç½®ä¿¡åº¦:{confidence:.2f})")
            
            # å‚è€ƒæ–‡çŒ®ä¸“é¡¹æµ‹è¯•
            print(f"\nğŸ“š å‚è€ƒæ–‡çŒ®ä¸“é¡¹æµ‹è¯•:")
            references_test = test_references_parsing(doc['content'])
            
            if references_test['found']:
                print(f"    æ‰¾åˆ°å‚è€ƒæ–‡çŒ®ç« èŠ‚")
                print(f"   ğŸ“ æ ‡é¢˜å˜ä½“: {len(references_test['title_variations'])} ä¸ª")
                for var in references_test['title_variations']:
                    print(f"      è¡Œ {var['line_number']}: '{var['matched_text']}' (æ¨¡å¼: {var['pattern']})")
                
                print(f"   ğŸ“Š å‚è€ƒæ–‡çŒ®ç»Ÿè®¡:")
                print(f"      æ¡ç›®æ•°é‡: ~{references_test['total_references']} ä¸ª")
                print(f"      ç¼–å·æ ¼å¼: {', '.join(references_test['reference_formats']) if references_test['reference_formats'] else 'æœªè¯†åˆ«'}")
                
                if references_test['spacing_issues']:
                    print(f"   âš ï¸ å‘ç°æ ¼å¼é—®é¢˜ ({len(references_test['spacing_issues'])} ä¸ª):")
                    for issue in references_test['spacing_issues'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        if issue['type'] == 'title_spacing':
                            print(f"      æ ‡é¢˜ç©ºæ ¼: è¡Œ {issue['line']} - {issue['content']}")
                        elif issue['type'] == 'item_spacing':
                            print(f"      æ¡ç›®ç©ºè¡Œ: è¡Œ {issue['line']} - {issue['context']}")
                    if len(references_test['spacing_issues']) > 3:
                        print(f"      ... è¿˜æœ‰ {len(references_test['spacing_issues']) - 3} ä¸ªé—®é¢˜")
                else:
                    print(f"    æ ¼å¼æ£€æŸ¥é€šè¿‡ï¼Œæ— æ˜æ˜¾é—®é¢˜")
                
                print(f"   ğŸ“„ å†…å®¹é¢„è§ˆ:")
                preview_lines = references_test['content_preview'].split('\n')[:3]
                for line in preview_lines:
                    if line.strip():
                        print(f"      {line.strip()[:80]}{'...' if len(line.strip()) > 80 else ''}")
            else:
                print(f"   âŒ æœªæ‰¾åˆ°å‚è€ƒæ–‡çŒ®ç« èŠ‚")
            
        except Exception as e:
            print(f"   âš ï¸ AIæå–å™¨ä¸å¯ç”¨: {e}")    # å¯¹æ¯”åˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ä¸¤æ–‡æ¡£å¯¹æ¯”åˆ†æ")
    print(f"{'='*80}")
    
    if len(documents) >= 2:
        doc1, doc2 = documents[0], documents[1]
        
        print(f"\nğŸ“ è§„æ¨¡å¯¹æ¯”:")
        print(f"   æ–‡æ¡£1: {doc1['size']:,} å­—ç¬¦")
        print(f"   æ–‡æ¡£2: {doc2['size']:,} å­—ç¬¦")
        print(f"   å·®å¼‚: {abs(doc1['size'] - doc2['size']):,} å­—ç¬¦")
        
        # å­¦ç§‘é¢†åŸŸåˆ†æ
        print(f"\nğŸ“ å­¦ç§‘åˆ†æ:")
        for i, doc in enumerate([doc1, doc2], 1):
            filename = doc['filename']
            if 'ç”Ÿç‰©åŒ»å­¦' in filename:
                print(f"   æ–‡æ¡£{i}: ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹é¢†åŸŸ")
            elif 'è·¨æ¨¡æ€' in filename:
                print(f"   æ–‡æ¡£{i}: è®¡ç®—æœºè§†è§‰/åŒ»å­¦å½±åƒé¢†åŸŸ")
            elif 'ç¥ç»ç½‘ç»œ' in filename:
                print(f"   æ–‡æ¡£{i}: äººå·¥æ™ºèƒ½/é€šä¿¡å·¥ç¨‹é¢†åŸŸ")
            elif 'å·¥ç¨‹åŠ›å­¦' in filename:
                print(f"   æ–‡æ¡£{i}: å·¥ç¨‹åŠ›å­¦é¢†åŸŸ")
            else:
                print(f"   æ–‡æ¡£{i}: æœªæ˜ç¡®è¯†åˆ«çš„é¢†åŸŸ")
    
    print(f"\n æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_latest_documents()
