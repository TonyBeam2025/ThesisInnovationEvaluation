#!/usr/bin/env python3
"""
æµ‹è¯•åŸºäºæ­¥éª¤3ç»“æ„åˆ†æç»“æœçš„AIæ™ºèƒ½åˆ†æåŠŸèƒ½
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
import json
import glob
from typing import Dict, Any

# æ·»åŠ æºä»£ç è·¯å¾„
current_dir = str(PROJECT_ROOT)
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

def test_ai_section_analysis():
    """æµ‹è¯•AIç« èŠ‚åˆ†æåŠŸèƒ½"""
    
    print("ğŸ§  AIç« èŠ‚åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    # å¯¼å…¥æå–å™¨
    try:
        import thesis_inno_eval.extract_sections_with_ai as extract_module
        extractor = extract_module.ThesisExtractorPro()
        print("    æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•ç”¨çš„ç®€åŒ–è®ºæ–‡æ–‡æœ¬
    test_text = """
# åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒè¯†åˆ«æŠ€æœ¯ç ”ç©¶

## æ‘˜è¦

æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒè¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡æ”¹è¿›å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ï¼Œå®ç°äº†æ›´é«˜çš„è¯†åˆ«ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†95%çš„å‡†ç¡®ç‡ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡äº†10%ã€‚

å…³é”®è¯ï¼šæ·±åº¦å­¦ä¹ ï¼Œå›¾åƒè¯†åˆ«ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼Œæœºå™¨å­¦ä¹ 

## Abstract

This research proposes a deep learning-based image recognition method that achieves higher recognition accuracy by improving the convolutional neural network structure. Experimental results show that this method achieves 95% accuracy on standard datasets, which is 10% higher than traditional methods.

Keywords: deep learning, image recognition, convolutional neural network, machine learning

## ç¬¬ä¸€ç«  ç»ªè®º

### 1.1 ç ”ç©¶èƒŒæ™¯

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå›¾åƒè¯†åˆ«æŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ä¼ ç»Ÿçš„å›¾åƒè¯†åˆ«æ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾è®¡çš„ç‰¹å¾ï¼Œå­˜åœ¨æ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å…´èµ·ä¸ºå›¾åƒè¯†åˆ«å¸¦æ¥äº†æ–°çš„æœºé‡ã€‚

### 1.2 ç ”ç©¶æ„ä¹‰

æœ¬ç ”ç©¶çš„æ„ä¹‰åœ¨äºï¼š
1. æé«˜å›¾åƒè¯†åˆ«çš„å‡†ç¡®ç‡
2. é™ä½äººå·¥ç‰¹å¾è®¾è®¡çš„å¤æ‚åº¦
3. ä¸ºç›¸å…³åº”ç”¨æä¾›æŠ€æœ¯æ”¯æ’‘

## ç¬¬äºŒç«  ç›¸å…³å·¥ä½œ

### 2.1 ä¼ ç»Ÿå›¾åƒè¯†åˆ«æ–¹æ³•

ä¼ ç»Ÿçš„å›¾åƒè¯†åˆ«æ–¹æ³•ä¸»è¦åŒ…æ‹¬ï¼š
- åŸºäºæ¨¡æ¿åŒ¹é…çš„æ–¹æ³•
- åŸºäºç»Ÿè®¡å­¦ä¹ çš„æ–¹æ³•
- åŸºäºç‰¹å¾å·¥ç¨‹çš„æ–¹æ³•

### 2.2 æ·±åº¦å­¦ä¹ æ–¹æ³•

æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸçš„åº”ç”¨åŒ…æ‹¬ï¼š
- CNNå·ç§¯ç¥ç»ç½‘ç»œ
- ResNetæ®‹å·®ç½‘ç»œ
- Transformeræ¶æ„

## ç¬¬ä¸‰ç«  ç ”ç©¶æ–¹æ³•

### 3.1 ç½‘ç»œæ¶æ„è®¾è®¡

æœ¬ç ”ç©¶æå‡ºçš„ç½‘ç»œæ¶æ„åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š
1. ç‰¹å¾æå–å±‚
2. æ³¨æ„åŠ›æœºåˆ¶
3. åˆ†ç±»å™¨å±‚

### 3.2 è®­ç»ƒç­–ç•¥

é‡‡ç”¨ä»¥ä¸‹è®­ç»ƒç­–ç•¥ï¼š
- æ•°æ®å¢å¼º
- å­¦ä¹ ç‡è°ƒåº¦
- æ­£åˆ™åŒ–æŠ€æœ¯

## ç¬¬å››ç«  å®éªŒç»“æœ

### 4.1 æ•°æ®é›†

ä½¿ç”¨çš„æ•°æ®é›†åŒ…æ‹¬ï¼š
- CIFAR-10
- ImageNet
- è‡ªå»ºæ•°æ®é›†

### 4.2 å®éªŒè®¾ç½®

å®éªŒç¯å¢ƒï¼š
- GPU: NVIDIA RTX 3080
- Framework: PyTorch
- Batch Size: 32

### 4.3 ç»“æœåˆ†æ

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–¹æ³•åœ¨å„ä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ï¼š
- CIFAR-10: 95.2%
- ImageNet: 78.9%
- è‡ªå»ºæ•°æ®é›†: 92.1%

## ç¬¬äº”ç«  ç»“è®º

### 5.1 ç ”ç©¶æ€»ç»“

æœ¬ç ”ç©¶æˆåŠŸæå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ·±åº¦å­¦ä¹ å›¾åƒè¯†åˆ«æ–¹æ³•ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š
1. è®¾è®¡äº†æ–°çš„ç½‘ç»œæ¶æ„
2. æå‡ºäº†æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥
3. å®ç°äº†æ€§èƒ½æå‡

### 5.2 æœªæ¥å·¥ä½œ

æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼š
- ç½‘ç»œæ¶æ„çš„è¿›ä¸€æ­¥ä¼˜åŒ–
- åœ¨æ›´å¤šé¢†åŸŸçš„åº”ç”¨
- è®¡ç®—æ•ˆç‡çš„æå‡

## å‚è€ƒæ–‡çŒ®

[1] LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436-444.
[2] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition.
[3] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks.

## è‡´è°¢

æ„Ÿè°¢å¯¼å¸ˆçš„æ‚‰å¿ƒæŒ‡å¯¼å’Œå®éªŒå®¤åŒå­¦çš„å¸®åŠ©ã€‚
"""
    
    print("\nğŸ” æ­¥éª¤1: æ–‡æ¡£ç»“æ„åˆ†æ")
    print("-" * 50)
    
    # æ‰§è¡Œæ­¥éª¤3çš„ç»“æ„åˆ†æ
    sections = extractor._analyze_document_structure(test_text)
    
    print(f"è¯†åˆ«çš„ç« èŠ‚æ•°é‡: {len([k for k in sections.keys() if not k.endswith('_info')])}")
    
    # æ˜¾ç¤ºè¯†åˆ«çš„ç« èŠ‚
    for key, value in sections.items():
        if not key.endswith('_info') and isinstance(value, str):
            info_key = f"{key}_info"
            info = sections.get(info_key, {})
            title = info.get('title', 'N/A') if isinstance(info, dict) else 'N/A'
            confidence = info.get('boundary_confidence', 0) if isinstance(info, dict) else 0
            print(f"   ğŸ“– {key}: {title} (ç½®ä¿¡åº¦: {confidence:.2f}, é•¿åº¦: {len(value)})")
    
    print("\nğŸ§  æ­¥éª¤2: AIæ™ºèƒ½åˆ†æ")
    print("-" * 50)
    
    # æ‰§è¡ŒAIæ™ºèƒ½åˆ†æ
    if extractor.ai_client:
        ai_analysis = extractor._conduct_ai_analysis_on_sections(test_text, sections)
        
        print(f"\nğŸ“Š AIåˆ†æç»“æœæ¦‚è§ˆ:")
        print(f"   ç« èŠ‚åˆ†ææ•°é‡: {len(ai_analysis.get('section_analysis', {}))}")
        
        # æ˜¾ç¤ºå„ç« èŠ‚åˆ†æç»“æœ
        for section_name, analysis in ai_analysis.get('section_analysis', {}).items():
            if isinstance(analysis, dict):
                overall_score = analysis.get('overall_score', 0)
                summary = analysis.get('summary', '')[:50]
                print(f"   ğŸ“– {section_name}: è¯„åˆ† {overall_score:.1f}/10")
                if summary:
                    print(f"      ğŸ’­ æ‘˜è¦: {summary}...")
                
                # æ˜¾ç¤ºè¯„åˆ†è¯¦æƒ…
                scores = []
                for score_key in ['content_quality_score', 'structure_score', 'academic_value_score', 'language_score']:
                    score = analysis.get(score_key, 0)
                    if score > 0:
                        scores.append(f"{score_key.replace('_score', '')}: {score}")
                
                if scores:
                    print(f"      ğŸ“Š è¯¦ç»†è¯„åˆ†: {', '.join(scores)}")
                
                # æ˜¾ç¤ºä¼˜ç‚¹å’Œå»ºè®®
                strengths = analysis.get('strengths', [])
                if strengths:
                    print(f"       ä¼˜ç‚¹: {', '.join(strengths[:2])}")
                
                suggestions = analysis.get('improvement_suggestions', [])
                if suggestions:
                    print(f"      ğŸ’¡ å»ºè®®: {', '.join(suggestions[:2])}")
        
        # æ˜¾ç¤ºæ•´ä½“ç»“æ„è¯„ä¼°
        structure_eval = ai_analysis.get('structure_evaluation', {})
        if structure_eval:
            print(f"\nğŸ—ï¸ æ•´ä½“ç»“æ„è¯„ä¼°:")
            for key, value in structure_eval.items():
                if key.endswith('_score') or key == 'overall_structure_score':
                    print(f"   ğŸ“Š {key.replace('_', ' ').title()}: {value}")
        
        # æ˜¾ç¤ºå­¦æœ¯è´¨é‡è¯„ä¼°
        quality_assessment = ai_analysis.get('content_quality', {})
        if quality_assessment:
            print(f"\nğŸ“ å­¦æœ¯è´¨é‡è¯„ä¼°:")
            for key, value in quality_assessment.items():
                if key.endswith('_score'):
                    print(f"   ğŸ“Š {key.replace('_', ' ').title()}: {value}")
    
    else:
        print("   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡AIåˆ†æ")
    
    print("\nğŸ§ª æ­¥éª¤3: å®Œæ•´æå–æµ‹è¯•")
    print("-" * 50)
    
    # æµ‹è¯•å®Œæ•´çš„æå–æµç¨‹
    try:
        full_result = extractor.extract_with_integrated_strategy(test_text)
        
        print(f" å®Œæ•´æå–æˆåŠŸ")
        print(f"   ğŸ“Š å­—æ®µæ•°é‡: {len(full_result)}")
        
        # æ˜¾ç¤ºAIåˆ†æç›¸å…³å­—æ®µ
        if 'ai_analysis' in full_result:
            ai_data = full_result['ai_analysis']
            print(f"   ğŸ§  AIåˆ†æ: {len(ai_data.get('section_analysis', {}))} ä¸ªç« èŠ‚")
        
        if 'ai_insights' in full_result:
            insights = full_result['ai_insights']
            print(f"   ğŸ’¡ AIæ´å¯Ÿ: {len(insights)} æ¡")
            for insight in insights[:3]:
                print(f"      - {insight}")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = os.path.join(current_dir, 'ai_analysis_test_result.json')
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(full_result, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"   âŒ å®Œæ•´æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_cached_documents():
    """æµ‹è¯•ç¼“å­˜æ–‡æ¡£çš„AIåˆ†æ"""
    
    print("\n" + "=" * 80)
    print("ğŸ“š ç¼“å­˜æ–‡æ¡£AIåˆ†ææµ‹è¯•")
    print("=" * 80)
    
    # å¯¼å…¥æå–å™¨
    try:
        import thesis_inno_eval.extract_sections_with_ai as extract_module
        extractor = extract_module.ThesisExtractorPro()
    except Exception as e:
        print(f"   âŒ æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # è·å–ç¼“å­˜æ–‡æ¡£
    cache_dir = os.path.join(current_dir, 'cache', 'documents')
    if not os.path.exists(cache_dir):
        print("   âš ï¸ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    md_files = glob.glob(os.path.join(cache_dir, "*.md"))
    if not md_files:
        print("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜çš„markdownæ–‡ä»¶")
        return
    
    # é€‰æ‹©æœ€æ–°çš„æ–‡æ¡£è¿›è¡Œæµ‹è¯•
    md_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    test_file = md_files[0]
    
    print(f"ğŸ“– æµ‹è¯•æ–‡æ¡£: {os.path.basename(test_file)}")
    
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"   ğŸ“„ æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æ‰§è¡ŒAIåˆ†æ
        print(f"   ğŸ§  æ‰§è¡ŒAIåˆ†æ...")
        
        # é¦–å…ˆè¿›è¡Œç»“æ„åˆ†æ
        sections = extractor._analyze_document_structure(content)
        section_count = len([k for k in sections.keys() if not k.endswith('_info')])
        print(f"   ğŸ“– è¯†åˆ«ç« èŠ‚: {section_count} ä¸ª")
        
        # æ‰§è¡ŒAIæ™ºèƒ½åˆ†æ
        if extractor.ai_client:
            ai_analysis = extractor._conduct_ai_analysis_on_sections(content, sections)
            
            analyzed_sections = len(ai_analysis.get('section_analysis', {}))
            print(f"   ğŸ¤– AIåˆ†æå®Œæˆ: {analyzed_sections} ä¸ªç« èŠ‚")
            
            # è®¡ç®—å¹³å‡è¯„åˆ†
            total_scores = []
            for analysis in ai_analysis.get('section_analysis', {}).values():
                if isinstance(analysis, dict):
                    score = analysis.get('overall_score', 0)
                    if score > 0:
                        total_scores.append(score)
            
            if total_scores:
                avg_score = sum(total_scores) / len(total_scores)
                print(f"   ğŸ“Š å¹³å‡è´¨é‡è¯„åˆ†: {avg_score:.2f}/10")
            
            # ä¿å­˜åˆ†æç»“æœ
            result_file = os.path.join(current_dir, f'cached_doc_ai_analysis_{os.path.basename(test_file)}.json')
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(ai_analysis, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ’¾ AIåˆ†æç»“æœå·²ä¿å­˜åˆ°: {os.path.basename(result_file)}")
        
        else:
            print(f"   âš ï¸ AIå®¢æˆ·ç«¯ä¸å¯ç”¨")
            
    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_ai_section_analysis()
    test_cached_documents()
    print(f"\n AIç« èŠ‚åˆ†ææµ‹è¯•å®Œæˆ!")
