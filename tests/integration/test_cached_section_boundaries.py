#!/usr/bin/env python3
"""
æµ‹è¯•ç« èŠ‚è¾¹ç•Œè¯†åˆ«åŠŸèƒ½ - ä½¿ç”¨ç¼“å­˜çš„è®ºæ–‡mdæ–‡ä»¶
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
import json
import glob
import re
from typing import Dict, Any, List

# æ·»åŠ æºä»£ç è·¯å¾„
current_dir = str(PROJECT_ROOT)
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

def load_cached_documents():
    """åŠ è½½ç¼“å­˜çš„è®ºæ–‡æ–‡æ¡£"""
    cache_dir = os.path.join(current_dir, 'cache', 'documents')
    documents = []
    
    if not os.path.exists(cache_dir):
        print(f"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {cache_dir}")
        return documents
    
    # æŸ¥æ‰¾æ‰€æœ‰mdæ–‡ä»¶
    md_files = glob.glob(os.path.join(cache_dir, "*.md"))
    
    for md_file in md_files:
        try:
            # è¯»å–mdæ–‡ä»¶å†…å®¹
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å°è¯•è¯»å–å¯¹åº”çš„metadataæ–‡ä»¶
            metadata_file = md_file.replace('.md', '_metadata.json')
            metadata = {}
            if os.path.exists(metadata_file):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                except:
                    pass
            
            documents.append({
                'filename': os.path.basename(md_file),
                'filepath': md_file,
                'content': content,
                'metadata': metadata,
                'size': len(content)
            })
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {md_file}: {e}")
    
    return documents

def simple_section_analysis(text: str) -> Dict[str, Any]:
    """ç®€åŒ–çš„ç« èŠ‚åˆ†æï¼ˆä¸ä¾èµ–å¤æ‚çš„ç±»ï¼‰"""
    sections = {}
    
    # åŸºæœ¬ç« èŠ‚è¯†åˆ«æ¨¡å¼
    section_patterns = {
        'title': r'([^\n]{10,100})\n.*(?:ä½œè€…|Author)',
        'abstract_cn': r'((?:ä¸­æ–‡)?æ‘˜\s*è¦[\s\S]{100,3000}?)(?=å…³é”®è¯|Keywords?|ABSTRACT)',
        'abstract_en': r'((?:ABSTRACT|Abstract)[\s\S]{100,3000}?)(?=Keywords?|å…³é”®è¯|ç¬¬ä¸€ç« )',
        'keywords_cn': r'(å…³é”®è¯[ï¼š:\s]*[^\n\r]{5,200})',
        'keywords_en': r'((?:Keywords?|KEY\s+WORDS?)[ï¼š:\s]*[^\n\r]{5,200})',
        'chapter_1': r'((?:ç¬¬ä¸€ç« |ç¬¬1ç« |1\.|å¼•\s*è¨€|ç»ª\s*è®º)[\s\S]{200,5000}?)(?=ç¬¬äºŒç« |ç¬¬2ç« |2\.)',
        'chapter_2': r'((?:ç¬¬äºŒç« |ç¬¬2ç« |2\.|æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ)[\s\S]{500,8000}?)(?=ç¬¬ä¸‰ç« |ç¬¬3ç« |3\.)',
        'conclusion': r'((?:ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›)[\s\S]{100,3000}?)(?=å‚è€ƒæ–‡çŒ®|è‡´è°¢|REFERENCES)',
        'references': r'((?:å‚è€ƒæ–‡çŒ®|REFERENCES?)[\s\S]{200,8000}?)(?=è‡´è°¢|é™„å½•|$)',
    }
    
    for section_name, pattern in section_patterns.items():
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            content = match.group(1).strip()
            sections[section_name] = {
                'content': content,
                'start_pos': match.start(),
                'end_pos': match.end(),
                'length': len(content),
                'start_line': text[:match.start()].count('\n') + 1,
                'end_line': text[:match.end()].count('\n') + 1
            }
            
            # æå–æ ‡é¢˜
            if section_name == 'title':
                sections[section_name]['title'] = content.split('\n')[0].strip()
            elif section_name.startswith('abstract'):
                first_line = content.split('\n')[0].strip()
                sections[section_name]['title'] = first_line if len(first_line) < 50 else 'æ‘˜è¦' if 'cn' in section_name else 'ABSTRACT'
            elif section_name.startswith('chapter'):
                first_line = content.split('\n')[0].strip()
                sections[section_name]['title'] = first_line if len(first_line) < 100 else f'ç¬¬{section_name[-1]}ç« '
            else:
                first_line = content.split('\n')[0].strip()
                sections[section_name]['title'] = first_line if len(first_line) < 50 else section_name.replace('_', ' ').title()
    
    return sections

def extract_section_titles_and_boundaries(text: str) -> List[Dict[str, Any]]:
    """æå–æ‰€æœ‰ç« èŠ‚æ ‡é¢˜å’Œè¾¹ç•Œ"""
    titles = []
    
    # åŒ¹é…å„ç§ç« èŠ‚æ ‡é¢˜æ ¼å¼
    title_patterns = [
        (r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1-9]ç« )\s+([^\n\r]{5,100})', 'chapter'),
        (r'^(\d+\.)\s*([^\n\r]{5,100})', 'numbered'),
        (r'^(æ‘˜\s*è¦|ABSTRACT|Abstract)\s*$', 'abstract'),
        (r'^(å…³é”®è¯|Keywords?)\s*[ï¼š:\s]*', 'keywords'),
        (r'^(å¼•\s*è¨€|ç»ª\s*è®º|å‰\s*è¨€)\s*$', 'introduction'),
        (r'^(æ–‡çŒ®ç»¼è¿°|ç›¸å…³å·¥ä½œ|å›½å†…å¤–ç ”ç©¶ç°çŠ¶)\s*$', 'literature'),
        (r'^(ç»“\s*è®º|æ€»\s*ç»“|ç»“è®ºä¸å±•æœ›)\s*$', 'conclusion'),
        (r'^(å‚è€ƒæ–‡çŒ®|REFERENCES?|References?)\s*$', 'references'),
        (r'^(è‡´\s*è°¢|ACKNOWLEDGMENT)\s*$', 'acknowledgment'),
    ]
    
    lines = text.split('\n')
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
            
        for pattern, section_type in title_patterns:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                char_pos = sum(len(l) + 1 for l in lines[:i])  # +1 for \n
                
                title_info = {
                    'line_number': i + 1,
                    'char_position': char_pos,
                    'section_type': section_type,
                    'full_match': match.group(0),
                    'title': line,
                    'confidence': 1.0 if match.group(0) == line else 0.8
                }
                
                # å¦‚æœæœ‰åˆ†ç»„ï¼Œæå–ç« èŠ‚å·å’Œæ ‡é¢˜
                if match.groups() and len(match.groups()) >= 2:
                    title_info['chapter_number'] = match.group(1)
                    title_info['chapter_title'] = match.group(2).strip()
                    title_info['full_title'] = f"{match.group(1)} {match.group(2)}".strip()
                
                titles.append(title_info)
                break
    
    return titles

def test_cached_documents():
    """æµ‹è¯•ç¼“å­˜æ–‡æ¡£çš„ç« èŠ‚è¾¹ç•Œè¯†åˆ«"""
    
    print("ğŸ” å¼€å§‹æµ‹è¯•ç¼“å­˜æ–‡æ¡£çš„ç« èŠ‚è¾¹ç•Œè¯†åˆ«...")
    print("=" * 80)
    
    # åŠ è½½ç¼“å­˜æ–‡æ¡£
    documents = load_cached_documents()
    
    if not documents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç¼“å­˜æ–‡æ¡£")
        return
    
    print(f"ğŸ“š æ‰¾åˆ° {len(documents)} ä¸ªç¼“å­˜æ–‡æ¡£:")
    for doc in documents:
        print(f"   ğŸ“„ {doc['filename']} ({doc['size']} å­—ç¬¦)")
    
    # åˆ†ææ¯ä¸ªæ–‡æ¡£
    for i, doc in enumerate(documents, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“– åˆ†ææ–‡æ¡£ {i}/{len(documents)}: {doc['filename']}")
        print(f"ğŸ“ æ–‡æ¡£å¤§å°: {doc['size']} å­—ç¬¦")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºå…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if doc['metadata']:
            print("\nğŸ“‹ å…ƒæ•°æ®ä¿¡æ¯:")
            for key, value in doc['metadata'].items():
                if isinstance(value, str) and len(value) < 100:
                    print(f"   {key}: {value}")
                else:
                    print(f"   {key}: {type(value).__name__}")
        
        # ç®€åŒ–çš„ç« èŠ‚åˆ†æ
        print("\nğŸ” ç« èŠ‚è¯†åˆ«åˆ†æ:")
        sections = simple_section_analysis(doc['content'])
        
        if sections:
            print(f"    è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚:")
            for section_name, section_info in sections.items():
                print(f"\n   ğŸ“ {section_name}:")
                print(f"      ğŸ“‹ æ ‡é¢˜: {section_info.get('title', 'N/A')}")
                print(f"      ğŸ“ ä½ç½®: è¡Œ {section_info['start_line']}-{section_info['end_line']}")
                print(f"      ğŸ“ é•¿åº¦: {section_info['length']} å­—ç¬¦")
                
                # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                content = section_info['content']
                preview = content[:150].replace('\n', ' ')
                if len(content) > 150:
                    preview += "..."
                print(f"      ğŸ“„ é¢„è§ˆ: {preview}")
        else:
            print("   âŒ æœªè¯†åˆ«åˆ°ä»»ä½•ç« èŠ‚")
        
        # æ ‡é¢˜å’Œè¾¹ç•Œæ£€æµ‹
        print("\nğŸ¯ æ ‡é¢˜è¾¹ç•Œæ£€æµ‹:")
        titles = extract_section_titles_and_boundaries(doc['content'])
        
        if titles:
            print(f"    æ£€æµ‹åˆ° {len(titles)} ä¸ªæ ‡é¢˜:")
            for title_info in titles:
                print(f"\n   ğŸ“Œ è¡Œ {title_info['line_number']}: {title_info['title']}")
                print(f"      ğŸ·ï¸ ç±»å‹: {title_info['section_type']}")
                print(f"      ğŸ“ å­—ç¬¦ä½ç½®: {title_info['char_position']}")
                print(f"      ğŸ¯ ç½®ä¿¡åº¦: {title_info['confidence']:.2f}")
                
                if 'chapter_number' in title_info:
                    print(f"      ğŸ“– ç« èŠ‚å·: {title_info['chapter_number']}")
                if 'chapter_title' in title_info:
                    print(f"      ğŸ“ ç« èŠ‚æ ‡é¢˜: {title_info['chapter_title']}")
        else:
            print("   âŒ æœªæ£€æµ‹åˆ°ä»»ä½•æ ‡é¢˜")
        
        # å¦‚æœæ–‡æ¡£è¾ƒå¤šï¼Œåªåˆ†æå‰3ä¸ª
        if i >= 3:
            remaining = len(documents) - i
            if remaining > 0:
                print(f"\nâ­ï¸ è·³è¿‡å‰©ä½™ {remaining} ä¸ªæ–‡æ¡£çš„è¯¦ç»†åˆ†æ...")
            break
    
    print(f"\n{'='*80}")
    print(" æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š æ€»ç»“: åˆ†æäº† {min(3, len(documents))} ä¸ªæ–‡æ¡£")
    print(f"{'='*80}")

def test_specific_document(filename: str = ""):
    """æµ‹è¯•ç‰¹å®šæ–‡æ¡£"""
    documents = load_cached_documents()
    
    if filename:
        target_docs = [doc for doc in documents if filename in doc['filename']]
    else:
        # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡æ¡£
        target_docs = documents[:1] if documents else []
    
    if not target_docs:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£: {filename}")
        return
    
    doc = target_docs[0]
    print(f"ğŸ” è¯¦ç»†åˆ†ææ–‡æ¡£: {doc['filename']}")
    
    # å°è¯•å¯¼å…¥å®Œæ•´çš„æå–å™¨
    try:
        import thesis_inno_eval.extract_sections_with_ai as extract_module
        extractor = extract_module.ThesisExtractorPro()
        
        print("\nğŸ§  ä½¿ç”¨AIæå–å™¨åˆ†æ...")
        
        # ä½¿ç”¨å®Œæ•´çš„åˆ†ææ–¹æ³•
        if hasattr(extractor, '_analyze_document_structure'):
            sections = extractor._analyze_document_structure(doc['content'])
            print(f"    AIè¯†åˆ«åˆ° {len([k for k in sections.keys() if not k.endswith('_info')])} ä¸ªç« èŠ‚")
            
            # æ˜¾ç¤ºç« èŠ‚ä¿¡æ¯
            for key, value in sections.items():
                if key.endswith('_info') and isinstance(value, dict):
                    section_name = key.replace('_info', '')
                    print(f"\n   ğŸ“ {section_name}:")
                    print(f"      ğŸ“‹ æ ‡é¢˜: {value.get('title', 'N/A')}")
                    boundaries = value.get('boundaries', {})
                    if isinstance(boundaries, dict):
                        print(f"      ğŸ“ ä½ç½®: è¡Œ {boundaries.get('start_line', 'N/A')}-{boundaries.get('end_line', 'N/A')}")
                    print(f"      ğŸ“ é•¿åº¦: {value.get('content_length', 0)} å­—ç¬¦")
                    print(f"      ğŸ¯ ç½®ä¿¡åº¦: {value.get('boundary_confidence', 0):.2f}")
        
        # ä½¿ç”¨ç²¾ç¡®è¾¹ç•Œæ£€æµ‹
        if hasattr(extractor, 'find_precise_section_boundaries'):
            test_sections = ['æ‘˜è¦', 'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç»“è®º', 'å‚è€ƒæ–‡çŒ®']
            
            print(f"\nğŸ¯ ç²¾ç¡®è¾¹ç•Œæ£€æµ‹:")
            for section_title in test_sections:
                try:
                    boundary_info = extractor.find_precise_section_boundaries(doc['content'], section_title)
                    
                    if boundary_info['found']:
                        print(f"\n    {section_title}:")
                        print(f"      ğŸ“‹ æ ‡é¢˜: {boundary_info['title']}")
                        print(f"      ğŸ“ ä½ç½®: å­—ç¬¦ {boundary_info['start_pos']}-{boundary_info['end_pos']}")
                        print(f"      ğŸ“ è¡Œä½ç½®: {boundary_info['start_line']}-{boundary_info['end_line']}")
                        print(f"      ğŸ¯ ç½®ä¿¡åº¦: {boundary_info['confidence']:.2f}")
                        if boundary_info['next_section']:
                            print(f"      â­ï¸ ä¸‹ä¸€ç« èŠ‚: {boundary_info['next_section']}")
                    else:
                        print(f"   âŒ æœªæ‰¾åˆ°: {section_title}")
                except Exception as e:
                    print(f"   âš ï¸ ç²¾ç¡®æ£€æµ‹å¤±è´¥ {section_title}: {e}")
        else:
            print("\nâš ï¸ ç²¾ç¡®è¾¹ç•Œæ£€æµ‹æ–¹æ³•ä¸å¯ç”¨")
        
    except Exception as e:
        print(f"âš ï¸ AIæå–å™¨ä¸å¯ç”¨: {e}")
        print("å›é€€åˆ°ç®€åŒ–åˆ†æ...")
        
        # ä½¿ç”¨ç®€åŒ–åˆ†æ
        sections = simple_section_analysis(doc['content'])
        titles = extract_section_titles_and_boundaries(doc['content'])
        
        print(f"\nğŸ“Š ç®€åŒ–åˆ†æç»“æœ:")
        print(f"   ç« èŠ‚æ•°: {len(sections)}")
        print(f"   æ ‡é¢˜æ•°: {len(titles)}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•ç« èŠ‚è¾¹ç•Œè¯†åˆ«åŠŸèƒ½')
    parser.add_argument('--file', '-f', help='æŒ‡å®šè¦åˆ†æçš„æ–‡ä»¶åï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰')
    parser.add_argument('--detailed', '-d', action='store_true', help='è¯¦ç»†åˆ†ææ¨¡å¼')
    
    args = parser.parse_args()
    
    if args.file:
        test_specific_document(args.file)
    elif args.detailed:
        test_cached_documents()
    else:
        # é»˜è®¤å¿«é€Ÿæµ‹è¯•
        documents = load_cached_documents()
        if documents:
            print(f"ğŸš€ å¿«é€Ÿæµ‹è¯• - åˆ†æç¬¬ä¸€ä¸ªæ–‡æ¡£: {documents[0]['filename']}")
            test_specific_document()
        else:
            print("âŒ æœªæ‰¾åˆ°ç¼“å­˜æ–‡æ¡£")
