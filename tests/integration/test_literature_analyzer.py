#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„æ–‡çŒ®ç»¼è¿°åˆ†æå™¨
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
import json
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(str(PROJECT_ROOT), 'src'))

from thesis_inno_eval.literature_review_analyzer import LiteratureReviewAnalyzer

def test_literature_analyzer():
    """æµ‹è¯•æ–‡çŒ®ç»¼è¿°åˆ†æå™¨"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–‡çŒ®ç»¼è¿°åˆ†æå™¨")
    
    # åˆ›å»ºæµ‹è¯•åˆ†æå™¨
    try:
        analyzer = LiteratureReviewAnalyzer()
        print(" æ–‡çŒ®ç»¼è¿°åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æ£€æŸ¥æµ‹è¯•æ•°æ®
    output_dir = Path("data/output")
    test_files = list(output_dir.glob("*_relevant_papers_Chinese.json"))
    
    if not test_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶")
        return
    
    test_file = test_files[0]
    print(f"ğŸ“ ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {test_file.name}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            chinese_papers = json.load(f)
        
        english_file = test_file.parent / test_file.name.replace('_Chinese.json', '_English.json')
        english_papers = []
        if english_file.exists():
            with open(english_file, 'r', encoding='utf-8') as f:
                english_papers = json.load(f)
        
        papers_by_lang = {
            'Chinese': chinese_papers[:10],  # åªå–å‰10ç¯‡æµ‹è¯•
            'English': english_papers[:10]
        }
        
        print(f"ğŸ“Š åŠ è½½æ–‡çŒ®æ•°æ®: ä¸­æ–‡{len(papers_by_lang['Chinese'])}ç¯‡, è‹±æ–‡{len(papers_by_lang['English'])}ç¯‡")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•å…ƒæ•°æ®åˆ†æ
    try:
        print("\nğŸ” æµ‹è¯•å…ƒæ•°æ®é©±åŠ¨åˆ†æ...")
        metadata_analysis = analyzer._generate_metadata_driven_analysis(papers_by_lang)
        print(" å…ƒæ•°æ®åˆ†æç”ŸæˆæˆåŠŸ")
        print("å‰300å­—ç¬¦é¢„è§ˆ:")
        print("-" * 50)
        print(metadata_analysis[:300] + "...")
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ å…ƒæ•°æ®åˆ†æå¤±è´¥: {e}")
    
    # æµ‹è¯•ä½œè€…åˆ†æ
    try:
        print("\nğŸ‘¥ æµ‹è¯•ä½œè€…åˆ†æ...")
        all_papers = chinese_papers[:5] + english_papers[:5]  # æ··åˆæµ‹è¯•
        authors_analysis = analyzer._analyze_authors_metadata(all_papers)
        print(" ä½œè€…åˆ†ææˆåŠŸ")
        print("ä½œè€…åˆ†æç»“æœ:")
        print(authors_analysis)
        
    except Exception as e:
        print(f"âŒ ä½œè€…åˆ†æå¤±è´¥: {e}")
    
    # æµ‹è¯•æœºæ„åˆ†æ
    try:
        print("\nğŸ›ï¸ æµ‹è¯•æœºæ„åˆ†æ...")
        institutions_analysis = analyzer._analyze_institutions_metadata(all_papers)
        print(" æœºæ„åˆ†ææˆåŠŸ")
        print("æœºæ„åˆ†æç»“æœ:")
        print(institutions_analysis)
        
    except Exception as e:
        print(f"âŒ æœºæ„åˆ†æå¤±è´¥: {e}")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    try:
        print("\nğŸ“‹ æµ‹è¯•åŸºæœ¬åˆ†æåŠŸèƒ½...")
        basic_analysis = analyzer._generate_basic_analysis_sections("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡çŒ®ç»¼è¿°å†…å®¹" * 100, 50, 25)
        print(" åŸºæœ¬åˆ†æåŠŸèƒ½æ­£å¸¸")
        print("åŸºæœ¬åˆ†æé¢„è§ˆ:")
        print(basic_analysis[:200] + "...")
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åˆ†æå¤±è´¥: {e}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_literature_analyzer()
