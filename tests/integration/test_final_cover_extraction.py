#!/usr/bin/env python3
"""
æœ€ç»ˆæµ‹è¯•ï¼šå±•ç¤ºå°é¢ä¿¡æ¯æå–çš„å®Œæ•´æ”¹è¿›æ•ˆæœ
å¯¹æ¯”ä¹‹å‰çš„é—®é¢˜å’Œç°åœ¨çš„è§£å†³æ–¹æ¡ˆ
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
sys.path.insert(0, os.path.join(str(PROJECT_ROOT), 'src'))

import json
from thesis_inno_eval.extract_sections_with_ai import extract_text_from_word, ThesisExtractorPro

def final_test():
    """æœ€ç»ˆæµ‹è¯•ï¼šå®Œæ•´å±•ç¤ºæ”¹è¿›æ•ˆæœ"""
    
    print("ğŸ¯ è®ºæ–‡å°é¢ä¿¡æ¯æå–æ”¹è¿›æ•ˆæœå±•ç¤º")
    print("=" * 60)
    
    file_path = "data/input/50286.docx"
    
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    try:
        # æå–æ–‡æ¡£æ–‡æœ¬
        print("ğŸ“„ æå–æ–‡æ¡£æ–‡æœ¬...")
        text = extract_text_from_word(file_path)
        print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # ä½¿ç”¨æ”¹è¿›åçš„æå–å™¨
        print("\nğŸš€ ä½¿ç”¨æ”¹è¿›åçš„æå–å™¨...")
        extractor = ThesisExtractorPro()
        
        # æµ‹è¯•å°é¢ä¿¡æ¯æå–
        cover_metadata = extractor._extract_front_metadata(text)
        
        print(f"\nğŸ“Š æ ¸å¿ƒæ”¹è¿›æŠ€æœ¯:")
        print(f"    ç²¾å‡†å®šä½ï¼šåœ¨'å­¦ä½è®ºæ–‡ä½¿ç”¨æˆæƒä¹¦'ä¹‹å‰")
        print(f"    æ™ºèƒ½æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰æ ‡ç­¾æ–‡å­—")
        print(f"    å­—æ®µç‰¹åŒ–ï¼šé’ˆå¯¹ä¸åŒå­—æ®µçš„ä¸“é—¨å¤„ç†")
        
        print(f"\nğŸ‰ æ”¹è¿›åçš„æå–ç»“æœ:")
        print("-" * 50)
        
        # é‡ç‚¹å±•ç¤ºä¹‹å‰æœ‰é—®é¢˜çš„å­—æ®µ
        problem_fields = {
            'ThesisNumber': 'å­¦å·',
            'ChineseTitle': 'ä¸­æ–‡æ ‡é¢˜', 
            'ChineseAuthor': 'ä½œè€…å§“å',
            'ChineseUniversity': 'å­¦æ ¡åç§°',
            'DegreeLevel': 'å­¦ä½çº§åˆ«',
            'ChineseMajor': 'ä¸“ä¸šåç§°',
            'College': 'å­¦é™¢åç§°',
            'ChineseSupervisor': 'å¯¼å¸ˆå§“å'
        }
        
        for field, description in problem_fields.items():
            value = cover_metadata.get(field, '')
            status = "" if value else "âŒ"
            print(f"   {status} {description:12}: {value}")
        
        print(f"\nğŸ“ˆ è´¨é‡è¯„ä¼°:")
        extracted_count = len([v for v in cover_metadata.values() if v])
        total_fields = len(problem_fields)
        completeness = extracted_count / total_fields
        print(f"   æå–æˆåŠŸç‡: {extracted_count}/{total_fields} ({completeness:.1%})")
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ ‡ç­¾æ–‡å­—æ®‹ç•™
        has_labels = any('ï¼š' in str(v) or 'å§“å' in str(v) or 'å­¦ä½æˆäºˆå•ä½' in str(v) 
                        for v in cover_metadata.values() if v)
        print(f"   æ ‡ç­¾æ¸…ç†: {'å®Œæˆ' if not has_labels else 'ä»æœ‰æ®‹ç•™'}")
        
        print(f"\nğŸ†š ä¸ä¹‹å‰ç»“æœå¯¹æ¯”:")
        print("-" * 50)
        
        # è¯»å–ä¹‹å‰çš„é—®é¢˜ç»“æœ
        prev_file = "data/output/50286_pro_extracted_info.json"
        if os.path.exists(prev_file):
            with open(prev_file, 'r', encoding='utf-8') as f:
                prev_data = json.load(f)
            prev_info = prev_data.get('extracted_info', {})
            
            print("   ğŸ”´ ä¹‹å‰çš„é—®é¢˜:")
            print(f"      ChineseTitle: {prev_info.get('ChineseTitle', '')}")
            print(f"      ChineseAuthor: {prev_info.get('ChineseAuthor', '')}")
            print(f"      ChineseUniversity: {prev_info.get('ChineseUniversity', '')}")
            
            print("   ğŸŸ¢ ç°åœ¨çš„ç»“æœ:")
            print(f"      ChineseTitle: {cover_metadata.get('ChineseTitle', '(æš‚æœªæå–)')}")
            print(f"      ChineseAuthor: {cover_metadata.get('ChineseAuthor', '')}")
            print(f"      ChineseUniversity: {cover_metadata.get('ChineseUniversity', '')}")
        
        print(f"\nğŸ’¡ æŠ€æœ¯çªç ´ç‚¹:")
        print(f"   1. ç²¾å‡†å®šä½å°é¢åŒºåŸŸ - é¿å…æ··å…¥åç»­å†…å®¹")
        print(f"   2. æ™ºèƒ½æ ‡ç­¾æ¸…ç† - ç§»é™¤'å§“åï¼š'ç­‰æ ¼å¼æ–‡å­—") 
        print(f"   3. å­—æ®µç‰¹åŒ–å¤„ç† - é’ˆå¯¹ä¸åŒç±»å‹å­—æ®µä¸“é—¨ä¼˜åŒ–")
        print(f"   4. å¤šå±‚éªŒè¯æœºåˆ¶ - ç¡®ä¿ç»“æœè´¨é‡")
        
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
        print(f"   - é›†æˆAIæ™ºèƒ½è¯†åˆ«ï¼ˆç›®å‰å› å¯¼å…¥é—®é¢˜æš‚æœªå¯ç”¨ï¼‰")
        print(f"   - å¢å¼ºä¸­æ–‡æ ‡é¢˜æå–é€»è¾‘")
        print(f"   - æ·»åŠ æ›´å¤šå¤§å­¦åç§°å’Œæ ¼å¼æ”¯æŒ")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = "data/output/50286_final_cover_extracted_info.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        result_data = {
            'final_cover_metadata': cover_metadata,
            'extraction_method': 'precise_location_plus_smart_cleaning',
            'improvements': [
                'precise_cover_location',
                'intelligent_label_cleaning', 
                'field_specific_processing',
                'multi_layer_validation'
            ],
            'extraction_time': '2025-08-20T17:25:00',
            'file_path': file_path,
            'quality_metrics': {
                'extracted_fields': extracted_count,
                'total_fields': total_fields,
                'completeness': completeness,
                'labels_cleaned': not has_labels
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {output_file}")
        
        return cover_metadata
        
    except Exception as e:
        print(f"âŒ æœ€ç»ˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    final_test()
