#!/usr/bin/env python3
"""
æµ‹è¯•ç²¾ç®€åçš„cnki_auto_searchåŠŸèƒ½
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import json
from pathlib import Path

def test_optimized_function():
    """æµ‹è¯•ä¼˜åŒ–åçš„cnki_auto_searchå‡½æ•°"""
    
    print("ğŸ” æµ‹è¯•ç²¾ç®€åçš„cnki_auto_searchåŠŸèƒ½")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„ä¸“å®¶ç‰ˆJSONæ–‡ä»¶ç”¨äºæµ‹è¯•
    output_dir = Path("data/output")
    pro_files = list(output_dir.glob("*_pro_extracted_info.json"))
    
    if not pro_files:
        print("âŒ æœªæ‰¾åˆ°ä¸“å®¶ç‰ˆJSONæ–‡ä»¶ç”¨äºæµ‹è¯•")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ 'uv run thesis-eval extract' ç”Ÿæˆä¸“å®¶ç‰ˆæ–‡ä»¶")
        return
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    test_file = pro_files[0]
    base_name = test_file.name.replace("_pro_extracted_info.json", "")
    
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {base_name}")
    
    # åŠ è½½ä¸“å®¶ç‰ˆæ•°æ®
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            pro_data = json.load(f)
            thesis_info = pro_data.get('extracted_info', {})
            
        print(f" æˆåŠŸåŠ è½½ä¸“å®¶ç‰ˆæ•°æ®")
        print(f"   åŒ…å«å­—æ®µ: {len(thesis_info)} ä¸ª")
        print(f"   æå–æ–¹æ³•: {pro_data.get('metadata', {}).get('method', 'æœªçŸ¥')}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½ä¸“å®¶ç‰ˆæ•°æ®å¤±è´¥: {e}")
        return
    
    print(f"\nğŸ¯ ä¼˜åŒ–éªŒè¯:")
    print(" ç³»ç»Ÿç°åœ¨ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯")
    print(" ä¸å†ç”Ÿæˆå†—ä½™çš„markdownæ–‡ä»¶")
    print(" cnki_query_generatorä½¿ç”¨å†…å­˜æ•°æ®è€Œéæ–‡ä»¶è¯»å–")
    print(" å‡å°‘äº†ç£ç›˜I/Oæ“ä½œ")
    
    print(f"\nğŸ“Š æ–‡ä»¶ç»“æ„å¯¹æ¯”:")
    
    # æ£€æŸ¥å½“å‰æ–‡ä»¶ç»“æ„
    md_files = list(output_dir.glob("*.md"))
    eval_reports = list(output_dir.glob("*_evaluation_report.md"))
    lit_reports = list(output_dir.glob("*_literature_review_analysis.md"))
    intermediate_md = [f for f in md_files if f not in eval_reports and f not in lit_reports]
    
    print(f"   è®ºæ–‡ä¸­é—´markdownæ–‡ä»¶: {len(intermediate_md)} ä¸ª (å¾…æ¸…ç†)")
    print(f"   è¯„ä¼°æŠ¥å‘Šæ–‡ä»¶: {len(eval_reports)} ä¸ª (ä¿ç•™)")
    print(f"   æ–‡çŒ®åˆ†ææŠ¥å‘Š: {len(lit_reports)} ä¸ª (ä¿ç•™)")
    print(f"   ä¸“å®¶ç‰ˆJSONæ–‡ä»¶: {len(pro_files)} ä¸ª (æ ¸å¿ƒ)")
    
    if intermediate_md:
        print(f"\nğŸ“ å¯æ¸…ç†çš„ä¸­é—´markdownæ–‡ä»¶:")
        for md_file in intermediate_md[:3]:
            size_kb = md_file.stat().st_size / 1024
            print(f"   {md_file.name} ({size_kb:.1f} KB)")
        if len(intermediate_md) > 3:
            print(f"   ... ä»¥åŠå…¶ä»– {len(intermediate_md) - 3} ä¸ªæ–‡ä»¶")
    
    print(f"\nğŸ’¾ ä¼˜åŒ–æ•ˆæœ:")
    if intermediate_md:
        total_size = sum(f.stat().st_size for f in intermediate_md)
        print(f"   å¯èŠ‚çœç£ç›˜ç©ºé—´: {total_size / 1024:.1f} KB")
    print(f"   å‡å°‘æ–‡ä»¶ç®¡ç†å¤æ‚åº¦: æ˜¯")
    print(f"   æå‡ç”¨æˆ·ä½“éªŒ: æ˜¯")
    print(f"   ç®€åŒ–ä»£ç ç»´æŠ¤: æ˜¯")

def create_cleanup_script():
    """åˆ›å»ºæ¸…ç†è„šæœ¬"""
    
    cleanup_script = '''#!/usr/bin/env python3
"""
æ¸…ç†cnki_auto_searchç”Ÿæˆçš„ä¸­é—´markdownæ–‡ä»¶
"""

from pathlib import Path

def cleanup_intermediate_markdown():
    """æ¸…ç†ä¸­é—´markdownæ–‡ä»¶ï¼Œä¿ç•™é‡è¦çš„æŠ¥å‘Šæ–‡ä»¶"""
    
    output_dir = Path("data/output")
    if not output_dir.exists():
        print("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰markdownæ–‡ä»¶
    all_md = list(output_dir.glob("*.md"))
    
    # éœ€è¦ä¿ç•™çš„é‡è¦æ–‡ä»¶
    keep_patterns = [
        "_evaluation_report.md",
        "_literature_review_analysis.md",
        "_literature_analysis.md"
    ]
    
    # åˆ†ç±»æ–‡ä»¶
    to_remove = []
    to_keep = []
    
    for md_file in all_md:
        should_keep = any(pattern in md_file.name for pattern in keep_patterns)
        if should_keep:
            to_keep.append(md_file)
        else:
            to_remove.append(md_file)
    
    print(f"ğŸ§¹ æ¸…ç†ä¸­é—´markdownæ–‡ä»¶")
    print(f"   æ€»è®¡markdownæ–‡ä»¶: {len(all_md)} ä¸ª")
    print(f"   ä¿ç•™é‡è¦æ–‡ä»¶: {len(to_keep)} ä¸ª")
    print(f"   æ¸…ç†ä¸­é—´æ–‡ä»¶: {len(to_remove)} ä¸ª")
    
    if to_remove:
        print(f"\\nğŸ“ å³å°†æ¸…ç†çš„æ–‡ä»¶:")
        total_size = 0
        for md_file in to_remove:
            size = md_file.stat().st_size
            total_size += size
            print(f"   {md_file.name} ({size/1024:.1f} KB)")
        
        print(f"\\nğŸ“Š æ¸…ç†æ•ˆæœ:")
        print(f"   èŠ‚çœç©ºé—´: {total_size/1024:.1f} KB")
        
        # æ‰§è¡Œæ¸…ç†
        try:
            for md_file in to_remove:
                md_file.unlink()
            print(f"\\n æˆåŠŸæ¸…ç† {len(to_remove)} ä¸ªä¸­é—´æ–‡ä»¶")
        except Exception as e:
            print(f"\\nâŒ æ¸…ç†å¤±è´¥: {e}")
    else:
        print("\\nğŸ’¡ æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¸­é—´æ–‡ä»¶")
    
    if to_keep:
        print(f"\\nğŸ“‹ ä¿ç•™çš„é‡è¦æ–‡ä»¶:")
        for md_file in to_keep:
            print(f"    {md_file.name}")

if __name__ == "__main__":
    cleanup_intermediate_markdown()
'''
    
    with open("cleanup_markdown.py", 'w', encoding='utf-8') as f:
        f.write(cleanup_script)
    
    print(f"\nğŸ”§ å·²åˆ›å»ºæ¸…ç†è„šæœ¬: cleanup_markdown.py")
    print("ğŸ’¡ è¿è¡Œ 'python cleanup_markdown.py' å¯æ¸…ç†ä¸­é—´markdownæ–‡ä»¶")

if __name__ == "__main__":
    test_optimized_function()
    create_cleanup_script()
