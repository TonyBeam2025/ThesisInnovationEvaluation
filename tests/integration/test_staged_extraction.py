#!/usr/bin/env python3
"""
æµ‹è¯•åˆ†æ­¥å­¦ä½è®ºæ–‡æŠ½å–åŠŸèƒ½
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = PROJECT_ROOT
sys.path.insert(0, str(project_root))

try:
    from src.thesis_inno_eval.extract_sections_with_ai import extract_thesis_with_staged_approach, _is_likely_thesis, extract_sections_with_ai
    from src.thesis_inno_eval.cached_evaluator import CachedEvaluator
    from src.thesis_inno_eval.gemini_client import GeminiClient
    from src.thesis_inno_eval.config_manager import ConfigManager
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

def test_staged_extraction():
    """æµ‹è¯•åˆ†æ­¥æŠ½å–åŠŸèƒ½"""
    
    # ç›®æ ‡è®ºæ–‡
    target_file = "15-åŸºäºé£é™©ç®¡ç†è§†è§’çš„äº’è”ç½‘åŒ»é™¢æ”¿ç­–æ–‡æœ¬é‡åŒ–åˆ†æä¸ä¼˜åŒ–è·¯å¾„ç ”ç©¶.docx"
    file_path = project_root / "data" / "input" / target_file
    
    if not file_path.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"ğŸ¯ å¼€å§‹æµ‹è¯•åˆ†æ­¥æŠ½å–: {target_file}")
    
    # åˆå§‹åŒ–é…ç½®å’Œå®¢æˆ·ç«¯
    try:
        config_manager = ConfigManager()
        config = config_manager.get_config()
        ai_client = GeminiClient(config)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºç¼“å­˜è¯„ä¼°å™¨
    evaluator = CachedEvaluator(config)
    
    # è·å–ç¼“å­˜çš„Markdownå†…å®¹
    print("ğŸ“– è¯»å–ç¼“å­˜çš„Markdownå†…å®¹...")
    try:
        cached_text = evaluator.document_cache.get_cached_markdown(str(file_path))
    except Exception as e:
        print(f"âŒ è·å–ç¼“å­˜å†…å®¹å¤±è´¥: {e}")
        return
    
    if not cached_text:
        print("âŒ æ— æ³•è·å–ç¼“å­˜çš„Markdownå†…å®¹")
        return
    
    print(f"ğŸ“Š æ–‡æ¡£é•¿åº¦: {len(cached_text):,} å­—ç¬¦")
    
    # æ£€æµ‹æ˜¯å¦ä¸ºå­¦ä½è®ºæ–‡
    is_thesis = _is_likely_thesis(cached_text)
    print(f"ğŸ“‹ å­¦ä½è®ºæ–‡æ£€æµ‹ç»“æœ: {'æ˜¯' if is_thesis else 'å¦'}")
    
    # å¦‚æœæ˜¯å­¦ä½è®ºæ–‡ï¼Œä½¿ç”¨åˆ†æ­¥æŠ½å–
    if is_thesis:
        print("ğŸ“ ä½¿ç”¨åˆ†æ­¥æŠ½å–æ¨¡å¼...")
        result = extract_thesis_with_staged_approach(cached_text, ai_client, "test_staged")
    else:
        print("ğŸ“„ ä½¿ç”¨å¸¸è§„æŠ½å–æ¨¡å¼...")
        result = extract_sections_with_ai(cached_text, ai_client, "test_regular")
    
    if result:
        print(" æŠ½å–æˆåŠŸï¼")
        
        # åˆ†æç»“æœ
        total_fields = len(result)
        non_empty_fields = len([k for k, v in result.items() if v and str(v).strip()])
        
        print(f"ğŸ“Š æŠ½å–ç»“æœç»Ÿè®¡:")
        print(f"   - æ€»å­—æ®µæ•°: {total_fields}")
        print(f"   - éç©ºå­—æ®µæ•°: {non_empty_fields}")
        print(f"   - å®Œæ•´åº¦: {non_empty_fields/total_fields*100:.1f}%")
        
        # æ˜¾ç¤ºå…³é”®å­—æ®µ
        key_fields = ['ChineseTitle', 'ChineseAuthor', 'ChineseUniversity', 'DegreeLevel', 'ChineseAbstract']
        print(f"\nğŸ“‹ å…³é”®å­—æ®µé¢„è§ˆ:")
        for field in key_fields:
            value = result.get(field, '')
            if value:
                preview = value[:100] + "..." if len(value) > 100 else value
                print(f"   {field}: {preview}")
            else:
                print(f"   {field}: [ç©º]")
        
        # ä¿å­˜ç»“æœ
        output_path = project_root / "data" / "output" / "staged_extraction_test.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    else:
        print("âŒ æŠ½å–å¤±è´¥")

if __name__ == "__main__":
    test_staged_extraction()
