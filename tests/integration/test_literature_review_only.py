#!/usr/bin/env python3
"""
å•ç‹¬ç”Ÿæˆæ–‡çŒ®ç»¼è¿°æ·±åº¦åˆ†ææŠ¥å‘Šçš„æµ‹è¯•è„šæœ¬
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import json
import os
from pathlib import Path
from src.thesis_inno_eval.literature_review_analyzer import LiteratureReviewAnalyzer

def main():
    # æ•°æ®è·¯å¾„
    base_name = "è·¨æ¨¡æ€å›¾åƒèåˆæŠ€æœ¯åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­çš„ç ”ç©¶"
    data_dir = Path("data/output")
    
    # åŠ è½½è®ºæ–‡æå–ä¿¡æ¯
    extracted_info_file = data_dir / f"{base_name}_extracted_info.json"
    if extracted_info_file.exists():
        with open(extracted_info_file, 'r', encoding='utf-8') as f:
            extracted_data = json.load(f)
            thesis_extracted_info = extracted_data.get('extracted_info', {})
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æå–ä¿¡æ¯æ–‡ä»¶: {extracted_info_file}")
        return
    
    # åŠ è½½æ–‡çŒ®æ•°æ®
    papers_by_lang = {}
    
    # åŠ è½½ä¸­æ–‡æ–‡çŒ®
    chinese_file = data_dir / f"{base_name}_relevant_papers_Chinese.json"
    if chinese_file.exists():
        with open(chinese_file, 'r', encoding='utf-8') as f:
            chinese_papers = json.load(f)
            papers_by_lang['Chinese'] = chinese_papers
            print(f" åŠ è½½ä¸­æ–‡æ–‡çŒ®: {len(chinese_papers)} ç¯‡")
    
    # åŠ è½½è‹±æ–‡æ–‡çŒ®
    english_file = data_dir / f"{base_name}_relevant_papers_English.json"
    if english_file.exists():
        with open(english_file, 'r', encoding='utf-8') as f:
            english_papers = json.load(f)
            papers_by_lang['English'] = english_papers
            print(f" åŠ è½½è‹±æ–‡æ–‡çŒ®: {len(english_papers)} ç¯‡")
    
    if not papers_by_lang:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡çŒ®æ•°æ®")
        return
    
    # è°ƒè¯•æ•°æ®ç»“æ„
    print(f"\n=== æ•°æ®ç»“æ„è°ƒè¯• ===")
    all_papers = []
    for lang, papers in papers_by_lang.items():
        all_papers.extend(papers)
        print(f"{lang} æ–‡çŒ®æ•°é‡: {len(papers)}")
        if papers:
            first_paper = papers[0]
            authors = first_paper.get('Authors', [])
            print(f"  ç¬¬ä¸€ç¯‡è®ºæ–‡Authorsç±»å‹: {type(authors)}")
            print(f"  ç¬¬ä¸€ç¯‡è®ºæ–‡Authorsæ•°é‡: {len(authors) if isinstance(authors, list) else 'N/A'}")
            if isinstance(authors, list) and authors:
                print(f"  ç¬¬ä¸€ä¸ªä½œè€…: {authors[0]}")
    
    print(f"æ€»æ–‡çŒ®æ•°é‡: {len(all_papers)}")
    
    # åˆ›å»ºåˆ†æå™¨å¹¶ç”ŸæˆæŠ¥å‘Š
    analyzer = LiteratureReviewAnalyzer()
    
    try:
        print(f"\n=== å¼€å§‹ç”Ÿæˆæ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Š ===")
        report_file = analyzer.analyze_literature_review(
            f"data/input/{base_name}.docx",
            thesis_extracted_info,
            papers_by_lang,
            "data/output"
        )
        print(f" æ–‡çŒ®ç»¼è¿°åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # æ£€æŸ¥ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
        with open(report_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æŸ¥æ‰¾ä½œè€…åˆ†æéƒ¨åˆ†
        if "### ğŸ‘¥ ä½œè€…ç½‘ç»œåˆ†æ" in content:
            start_idx = content.find("### ğŸ‘¥ ä½œè€…ç½‘ç»œåˆ†æ")
            end_idx = content.find("### ğŸ›ï¸ æœºæ„åˆ†å¸ƒåˆ†æ", start_idx)
            if end_idx == -1:
                end_idx = start_idx + 500
            author_section = content[start_idx:end_idx]
            print(f"\n=== ä½œè€…ç½‘ç»œåˆ†æéƒ¨åˆ† ===")
            print(author_section)
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
