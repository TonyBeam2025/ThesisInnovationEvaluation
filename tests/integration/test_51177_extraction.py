#!/usr/bin/env python3
"""
æµ‹è¯•æŠ½å–51177.docxæ–‡ä»¶çš„ç»“æ„åŒ–ä¿¡æ¯
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import os
import sys
import json
from pathlib import Path
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = PROJECT_ROOT
sys.path.insert(0, str(project_root))

def test_extract_51177():
    """æµ‹è¯•æŠ½å–51177.docxæ–‡ä»¶"""
    
    target_file = "51177.docx"
    file_path = project_root / "data" / "input" / target_file
    
    if not file_path.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"ğŸ¯ å¼€å§‹æµ‹è¯•æŠ½å–: {target_file}")
    file_size_mb = file_path.stat().st_size / 1024 / 1024
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
    
    try:
        # ä½¿ç”¨ç°æœ‰çš„è¯„ä¼°ç³»ç»Ÿ
        from src.thesis_inno_eval.cached_evaluator import CachedEvaluator
        from src.thesis_inno_eval.config_manager import ConfigManager
        
        # åˆå§‹åŒ–é…ç½®
        config_file = project_root / "config" / "conf.yaml"
        config_mgr = ConfigManager(str(config_file))
        
        # åˆ›å»ºç¼“å­˜è¯„ä¼°å™¨
        evaluator = CachedEvaluator(config_mgr)
        
        print("ğŸ“‹ å¼€å§‹ç»“æ„åŒ–ä¿¡æ¯æŠ½å–...")
        start_time = time.time()
        
        # å…ˆè·å–AIå®¢æˆ·ç«¯
        from src.thesis_inno_eval.ai_client import get_ai_client
        ai_client = get_ai_client()
        
        if not ai_client:
            print("âŒ æ— æ³•è·å–AIå®¢æˆ·ç«¯")
            return
        
        # æ‰§è¡ŒæŠ½å–
        result = evaluator._extract_thesis_info(str(file_path), ai_client, "test_51177")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if result:
            print(f" æŠ½å–æˆåŠŸï¼è€—æ—¶: {processing_time:.1f} ç§’")
            
            # åˆ†æç»“æœ
            total_fields = len(result)
            non_empty_fields = len([k for k, v in result.items() if v and str(v).strip()])
            
            print(f"\nğŸ“Š æŠ½å–ç»“æœç»Ÿè®¡:")
            print(f"   - æ€»å­—æ®µæ•°: {total_fields}")
            print(f"   - éç©ºå­—æ®µæ•°: {non_empty_fields}")
            print(f"   - å®Œæ•´åº¦: {non_empty_fields/total_fields*100:.1f}%")
            
            # æ˜¾ç¤ºå…³é”®å­—æ®µ
            key_fields = [
                'ChineseTitle', 'ChineseAuthor', 'ChineseUniversity', 
                'DegreeLevel', 'ChineseAbstract', 'ReferenceList', 'ResearchConclusions'
            ]
            
            print(f"\nğŸ“‹ å…³é”®å­—æ®µæ£€æŸ¥:")
            for field in key_fields:
                value = result.get(field, '')
                if value:
                    if field == 'ReferenceList' and isinstance(value, list):
                        print(f"    {field}: {len(value)} æ¡å‚è€ƒæ–‡çŒ®")
                    elif isinstance(value, str):
                        preview = value[:80] + "..." if len(value) > 80 else value
                        print(f"    {field}: {preview}")
                    else:
                        print(f"    {field}: {str(value)[:80]}...")
                else:
                    print(f"   âŒ {field}: [ç©º]")
            
            # æ£€æŸ¥å­¦ä½è®ºæ–‡ç‰¹å¾
            is_thesis_detected = any([
                result.get('DegreeLevel'),
                result.get('ChineseUniversity'),
                'å­¦ä½è®ºæ–‡' in str(result.get('ChineseTitle', '')),
                'ç¡•å£«' in str(result.get('ChineseTitle', '')),
                'åšå£«' in str(result.get('ChineseTitle', ''))
            ])
            
            print(f"\nğŸ“ å­¦ä½è®ºæ–‡æ£€æµ‹: {'æ˜¯' if is_thesis_detected else 'å¦'}")
            
            # ä¿å­˜ç»“æœ
            output_file = project_root / "data" / "output" / f"{target_file}_extracted_info.json"
            
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file.name}")
                
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†åˆ†æ­¥æŠ½å–
            print(f"\nğŸ” æŠ½å–æ–¹å¼åˆ†æ:")
            if result.get('ChineseTitle') and result.get('ChineseAbstract'):
                print(f"   - æ ‡é¢˜å’Œæ‘˜è¦éƒ½å·²æå–ï¼Œå¯èƒ½ä½¿ç”¨äº†åˆ†æ­¥æŠ½å–")
            
            if result.get('ReferenceList'):
                print(f"   - å‚è€ƒæ–‡çŒ®å·²æå–ï¼Œæ­£æ–‡å¤„ç†æˆåŠŸ")
            else:
                print(f"   - å‚è€ƒæ–‡çŒ®ç¼ºå¤±ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–æ­£æ–‡å¤„ç†")
            
        else:
            print(f"âŒ æŠ½å–å¤±è´¥ï¼Œè€—æ—¶: {processing_time:.1f} ç§’")
            return
            
    except Exception as e:
        print(f"âŒ æŠ½å–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")

def check_cache_status():
    """æ£€æŸ¥ç¼“å­˜çŠ¶æ€"""
    
    print("\nğŸ“ æ£€æŸ¥ç¼“å­˜çŠ¶æ€:")
    
    target_file = "51177.docx"
    
    # æ£€æŸ¥æ–‡æ¡£ç¼“å­˜
    cache_dir = project_root / "cache" / "documents"
    if cache_dir.exists():
        cache_files = list(cache_dir.glob("*51177*"))
        print(f"   æ–‡æ¡£ç¼“å­˜: {len(cache_files)} ä¸ªç›¸å…³æ–‡ä»¶")
        for file in cache_files:
            size_kb = file.stat().st_size / 1024
            print(f"     - {file.name} ({size_kb:.1f} KB)")
    else:
        print(f"   æ–‡æ¡£ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
    
    # æ£€æŸ¥è¾“å‡ºç¼“å­˜
    output_dir = project_root / "data" / "output"
    if output_dir.exists():
        output_files = list(output_dir.glob("*51177*"))
        print(f"   è¾“å‡ºç¼“å­˜: {len(output_files)} ä¸ªç›¸å…³æ–‡ä»¶")
        for file in output_files:
            size_kb = file.stat().st_size / 1024
            print(f"     - {file.name} ({size_kb:.1f} KB)")
    else:
        print(f"   è¾“å‡ºç›®å½•ä¸å­˜åœ¨")

if __name__ == "__main__":
    check_cache_status()
    test_extract_51177()
