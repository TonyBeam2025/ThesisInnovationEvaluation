#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„ä¸»æå–å™¨ - éªŒè¯æ•°å­—æ ¼å¼ç« èŠ‚æ£€æµ‹
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import os
import sys
import json
import time
from pathlib import Path

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
src_path = PROJECT_ROOT / "src"
sys.path.insert(0, str(src_path))

from thesis_inno_eval.extract_sections_with_ai import ThesisExtractorPro

def test_enhanced_main_extractor():
    """æµ‹è¯•å¢å¼ºçš„ä¸»æå–å™¨"""
    
    print("=== æµ‹è¯•å¢å¼ºçš„ä¸»æå–å™¨ ===\n")
    
    # æ£€æŸ¥MDæ–‡æ¡£æ–‡ä»¶
    md_file = r"cache\documents\1_å·¥ç¨‹åŠ›å­¦_21703014_åˆ˜åŠ›å¤«_LW_76c5b96231292b26dbeab5065ab7f040.md"
    if not os.path.exists(md_file):
        print(f"âŒ MDæ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {md_file}")
        return
    
    print(f"ğŸ“„ è¯»å–MDæ–‡æ¡£: {md_file}")
    
    try:
        # è¯»å–æ–‡æ¡£
        with open(md_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print(f" æ–‡æ¡£è¯»å–æˆåŠŸï¼Œé•¿åº¦: {len(text):,} å­—ç¬¦")
        
        # åˆ›å»ºæå–å™¨ï¼ˆä¸ä½¿ç”¨AIå®¢æˆ·ç«¯ï¼‰
        extractor = ThesisExtractorPro()
        
        print("\nğŸ” å¼€å§‹ç« èŠ‚æ£€æµ‹...")
        start_time = time.time()
        
        # ä½¿ç”¨æ–°çš„ _analyze_document_structure æ–¹æ³•
        structure_analysis = extractor._analyze_document_structure(text)
        
        detection_time = time.time() - start_time
        print(f"â±ï¸ æ£€æµ‹å®Œæˆï¼Œè€—æ—¶: {detection_time:.2f} ç§’")
        
        # ç»Ÿè®¡ç»“æœ
        info_sections = {k: v for k, v in structure_analysis.items() if k.endswith('_info')}
        content_sections = {k: v for k, v in structure_analysis.items() if not k.endswith('_info')}
        total_sections = len(content_sections)
        
        print(f"\nğŸ“Š æ£€æµ‹ç»“æœç»Ÿè®¡:")
        print(f"   æ€»è®¡æ£€æµ‹åˆ° {total_sections} ä¸ªç« èŠ‚")
        
        # è¯¦ç»†æ˜¾ç¤ºæ¯ä¸ªç« èŠ‚
        print(f"\nğŸ“‹ è¯¦ç»†ç« èŠ‚åˆ—è¡¨:")
        for i, (section_name, section_content) in enumerate(content_sections.items(), 1):
            # è·å–å¯¹åº”çš„ä¿¡æ¯
            info_key = f"{section_name}_info"
            section_info = info_sections.get(info_key, {})
            
            title = section_info.get('title', 'æœªçŸ¥æ ‡é¢˜')
            start_pos = section_info.get('start_position', 0)
            end_pos = section_info.get('end_position', 0)
            length = section_info.get('content_length', len(section_content) if isinstance(section_content, str) else 0)
            confidence = section_info.get('boundary_confidence', 0.0)
            
            print(f"   {i:2d}. {section_name:<15} | {title:<25} | ä½ç½®: {start_pos:6d}-{end_pos:6d} | é•¿åº¦: {length:5d} | ç½®ä¿¡åº¦: {confidence:.2f}")
        
        # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°æ•°å­—æ ¼å¼ç« èŠ‚
        numeric_chapters = [name for name in content_sections.keys() if name.startswith('chapter_')]
        if numeric_chapters:
            print(f"\n æˆåŠŸæ£€æµ‹åˆ°æ•°å­—æ ¼å¼ç« èŠ‚: {len(numeric_chapters)} ä¸ª")
            for chapter in numeric_chapters:
                info_key = f"{chapter}_info"
                chapter_info = info_sections.get(info_key, {})
                print(f"   - {chapter}: {chapter_info.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
        else:
            print(f"\nâš ï¸ æœªæ£€æµ‹åˆ°æ•°å­—æ ¼å¼ç« èŠ‚")
        
        # ä¿å­˜ç»“æœ
        output_file = "enhanced_main_extractor_result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(structure_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¸åŸå§‹ç»“æœå¯¹æ¯”
        original_file = "pro_extracted_info.json"
        if os.path.exists(original_file):
            print(f"\nğŸ”„ ä¸åŸå§‹ç»“æœå¯¹æ¯”...")
            with open(original_file, 'r', encoding='utf-8') as f:
                original_data = json.load(f)
            
            original_sections = original_data.get('section_boundaries', {})
            original_count = len(original_sections)
            
            print(f"   åŸå§‹æ£€æµ‹: {original_count} ä¸ªç« èŠ‚")
            print(f"   å¢å¼ºæ£€æµ‹: {total_sections} ä¸ªç« èŠ‚")
            print(f"   æ”¹è¿›ç¨‹åº¦: +{total_sections - original_count} ä¸ªç« èŠ‚")
            
            if total_sections > original_count:
                print(f"   ğŸ‰ æ£€æµ‹èƒ½åŠ›æ˜¾è‘—æå‡ï¼")
            else:
                print(f"   ğŸ“ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒ
    os.chdir(r"c:\MyProjects\thesis_Inno_Eval")
    
    # è¿è¡Œæµ‹è¯•
    success = test_enhanced_main_extractor()
    
    if success:
        print(f"\n æµ‹è¯•å®Œæˆ")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥")
