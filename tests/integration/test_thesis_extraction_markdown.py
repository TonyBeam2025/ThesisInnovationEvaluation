#!/usr/bin/env python3
"""
æµ‹è¯•è®ºæ–‡å…¨é¢æŠ½å–èƒ½åŠ›å’ŒMarkdownæ ¼å¼è½¬æ¢èƒ½åŠ›
æµ‹è¯•ç›®æ ‡ï¼šé«˜åˆ†å­ææ–™è®ºæ–‡ - å”é‡‘é‡‘
"""
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT

import sys
import os
import time
from pathlib import Path

sys.path.append('./src')

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import thesis_inno_eval.smart_reference_extractor as sre_module
from thesis_inno_eval.extract_sections_with_ai import extract_text_from_word, ThesisExtractorPro
from thesis_inno_eval.ai_client import get_ai_client

def test_thesis_extraction_and_markdown():
    """æµ‹è¯•è®ºæ–‡æŠ½å–å’ŒMarkdownè½¬æ¢çš„å®Œæ•´æµç¨‹"""
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    docx_path = r"C:\MyProjects\thesis_Inno_Eval\data\input\1_é«˜åˆ†å­ææ–™_21807119_å”é‡‘é‡‘_LW.docx"
    
    print("ğŸ”¬ æµ‹è¯•é«˜åˆ†å­ææ–™è®ºæ–‡æŠ½å–å’ŒMarkdownè½¬æ¢èƒ½åŠ›")
    print("=" * 80)
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {os.path.basename(docx_path)}")
    
    # 1. æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(docx_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ - {docx_path}")
        return False
    
    # 2. è¯»å–Wordæ–‡æ¡£
    print("\nğŸ“š æ­¥éª¤1ï¼šè¯»å–Wordæ–‡æ¡£...")
    start_time = time.time()
    try:
        text = extract_text_from_word(docx_path)
        read_time = time.time() - start_time
        print(f"   âœ… æ–‡æ¡£è¯»å–æˆåŠŸ")
        print(f"   ğŸ“Š æ–‡æ¡£æ€»é•¿åº¦: {len(text):,} å­—ç¬¦")
        print(f"   â±ï¸ è¯»å–è€—æ—¶: {read_time:.2f}ç§’")
        
        # æ˜¾ç¤ºæ–‡æ¡£å¼€å¤´é¢„è§ˆ
        print(f"\n   ğŸ“– æ–‡æ¡£å¼€å¤´é¢„è§ˆ (å‰500å­—ç¬¦):")
        print("   " + "-" * 50)
        print("   " + text[:500].replace('\n', '\n   '))
        print("   " + "-" * 50)
        
    except Exception as e:
        print(f"   âŒ æ–‡æ¡£è¯»å–å¤±è´¥: {str(e)}")
        return False
    
    # 3. æµ‹è¯•SmartReferenceExtractorå‚è€ƒæ–‡çŒ®æŠ½å–
    print("\nğŸ“– æ­¥éª¤2ï¼šæµ‹è¯•æ™ºèƒ½å‚è€ƒæ–‡çŒ®æŠ½å–...")
    try:
        # SmartReferenceExtractorä¸“é—¨ç”¨äºDOCXï¼Œä¸éœ€è¦AIå®¢æˆ·ç«¯
        extractor = sre_module.SmartReferenceExtractor(ai_client=None)
        
        ref_start_time = time.time()
        references, ref_stats = extractor.extract_references(
            text, 
            source_format='docx',
            source_path=docx_path
        )
        ref_time = time.time() - ref_start_time
        
        print(f"   âœ… å‚è€ƒæ–‡çŒ®æŠ½å–å®Œæˆ")
        print(f"   ğŸ“Š æå–æ•°é‡: {len(references)} æ¡")
        print(f"   ğŸ”§ æå–æ–¹æ³•: {ref_stats.get('method_used', 'unknown')}")
        print(f"   â±ï¸ æŠ½å–è€—æ—¶: {ref_time:.2f}ç§’")
        print(f"   âœ… æå–çŠ¶æ€: {'æˆåŠŸ' if ref_stats.get('success', False) else 'å¤±è´¥'}")
        
        if references:
            print(f"\n   ğŸ“ å‚è€ƒæ–‡çŒ®é¢„è§ˆ (å‰5æ¡):")
            for i, ref in enumerate(references[:5], 1):
                print(f"   {i:2d}. {ref[:80]}{'...' if len(ref) > 80 else ''}")
                
    except Exception as e:
        print(f"   âŒ å‚è€ƒæ–‡çŒ®æŠ½å–å¤±è´¥: {str(e)}")
        references = []
        ref_stats = {}
    
    # 4. æµ‹è¯•AIæ™ºèƒ½æŠ½å–åŠŸèƒ½
    print("\nğŸ¤– æ­¥éª¤3ï¼šæµ‹è¯•AIæ™ºèƒ½è®ºæ–‡ç»“æ„æŠ½å–...")
    try:
        # åˆå§‹åŒ–ä¸“ä¸šç‰ˆæå–å™¨ï¼ˆåŒ…å«AIåŠŸèƒ½ï¼‰
        extractor_pro = ThesisExtractorPro()
        
        ai_start_time = time.time()
        # è°ƒç”¨AIæ™ºèƒ½æŠ½å–æ–¹æ³•
        extracted_data = extractor_pro.extract_with_integrated_strategy(text, docx_path)
        ai_time = time.time() - ai_start_time
        
        print(f"   âœ… AIæ™ºèƒ½æŠ½å–å®Œæˆ")
        print(f"   ğŸ“Š æå–å­—æ®µæ•°: {len(extracted_data)} ä¸ª")
        print(f"   â±ï¸ AIæŠ½å–è€—æ—¶: {ai_time:.2f}ç§’")
        
        # æ˜¾ç¤ºæå–çš„å…³é”®ä¿¡æ¯
        key_fields = ['title_cn', 'author_cn', 'university_cn', 'major_cn', 'degree_level']
        print(f"\n   ğŸ“‹ å…³é”®ä¿¡æ¯é¢„è§ˆ:")
        for field in key_fields:
            if field in extracted_data and extracted_data[field]:
                value = extracted_data[field][:100] + "..." if len(str(extracted_data[field])) > 100 else extracted_data[field]
                print(f"      {field}: {value}")
        
        # ç»Ÿè®¡æå–æˆåŠŸçš„å­—æ®µ
        extracted_fields = [k for k, v in extracted_data.items() if v and str(v).strip()]
        print(f"   ğŸ“Š æˆåŠŸæå–å­—æ®µ: {len(extracted_fields)}/{len(extracted_data)}")
        
    except Exception as e:
        print(f"   âŒ AIæ™ºèƒ½æŠ½å–å¤±è´¥: {str(e)}")
        extracted_data = {}
        ai_time = 0
    
    # 5. æµ‹è¯•ç« èŠ‚ç»“æ„åˆ†æ
    print("\nğŸ“‘ æ­¥éª¤4ï¼šåˆ†æè®ºæ–‡ç»“æ„...")
    try:
        # æŸ¥æ‰¾å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜
        lines = text.split('\n')
        potential_chapters = []
        
        # å¸¸è§çš„å­¦æœ¯è®ºæ–‡ç« èŠ‚æ¨¡å¼
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚',
            r'^\d+\s+\S+',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\.',
            r'æ‘˜\s*è¦',
            r'Abstract',
            r'å…³é”®è¯',
            r'Keywords',
            r'å¼•\s*è¨€',
            r'å‰\s*è¨€',
            r'ç»ª\s*è®º',
            r'ç»“\s*è®º',
            r'è‡´\s*è°¢',
            r'å‚è€ƒæ–‡çŒ®',
            r'é™„\s*å½•'
        ]
        
        import re
        for i, line in enumerate(lines):
            line = line.strip()
            if len(line) > 0 and len(line) < 50:  # å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜é•¿åº¦
                for pattern in chapter_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        potential_chapters.append((i+1, line))
                        break
        
        print(f"   âœ… å‘ç° {len(potential_chapters)} ä¸ªæ½œåœ¨ç« èŠ‚æ ‡é¢˜")
        if potential_chapters:
            print(f"   ğŸ“‹ ç« èŠ‚ç»“æ„é¢„è§ˆ (å‰10ä¸ª):")
            for line_num, title in potential_chapters[:10]:
                print(f"   L{line_num:4d}: {title}")
                
    except Exception as e:
        print(f"   âš ï¸ ç« èŠ‚ç»“æ„åˆ†æé‡åˆ°é—®é¢˜: {str(e)}")
        potential_chapters = []
    
    # 6. æµ‹è¯•Markdownè½¬æ¢èƒ½åŠ›
    print("\nğŸ“ æ­¥éª¤5ï¼šæµ‹è¯•Markdownæ ¼å¼è½¬æ¢...")
    try:
        markdown_content = convert_to_markdown(text, references, potential_chapters, extracted_data)
        
        # ä¿å­˜Markdownæ–‡ä»¶
        output_path = docx_path.replace('.docx', '_converted.md')
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"   âœ… Markdownè½¬æ¢å®Œæˆ")
        print(f"   ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"   ğŸ“Š Markdowné•¿åº¦: {len(markdown_content):,} å­—ç¬¦")
        
        # æ˜¾ç¤ºMarkdowné¢„è§ˆ
        print(f"\n   ğŸ“– Markdowné¢„è§ˆ (å‰1000å­—ç¬¦):")
        print("   " + "-" * 50)
        for line in markdown_content[:1000].split('\n'):
            print("   " + line)
        print("   " + "-" * 50)
        
    except Exception as e:
        print(f"   âŒ Markdownè½¬æ¢å¤±è´¥: {str(e)}")
        markdown_content = ""
        output_path = ""
        extracted_data = extracted_data if 'extracted_data' in locals() else {}
    
    # 7. æ€»ç»“æŠ¥å‘Š
    print("\nğŸ“Š æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 80)
    print(f"ğŸ“„ æºæ–‡ä»¶: {os.path.basename(docx_path)}")
    print(f"ğŸ“ æ–‡æ¡£é•¿åº¦: {len(text):,} å­—ç¬¦")
    print(f"ğŸ“– å‚è€ƒæ–‡çŒ®: {len(references)} æ¡")
    print(f"ğŸ“‘ ç« èŠ‚æ ‡é¢˜: {len(potential_chapters)} ä¸ª")
    if 'extracted_data' in locals() and extracted_data:
        print(f"ğŸ¤– AIæŠ½å–å­—æ®µ: {len(extracted_data)} ä¸ª")
        success_fields = [k for k, v in extracted_data.items() if v and str(v).strip()]
        print(f"âœ… æˆåŠŸå­—æ®µ: {len(success_fields)} ä¸ª")
    print(f"ğŸ“ Markdownæ–‡ä»¶: {'âœ… å·²ç”Ÿæˆ' if markdown_content else 'âŒ ç”Ÿæˆå¤±è´¥'}")
    if output_path:
        print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
    
    total_time = time.time() - start_time
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    
    return True

from typing import Optional

def convert_to_markdown(text: str, references: list, chapters: list, extracted_data: Optional[dict] = None) -> str:
    """å°†è®ºæ–‡å†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    
    markdown_lines = []
    
    # ä½¿ç”¨AIæå–çš„æ•°æ®ä½œä¸ºæ ‡é¢˜å’Œå…ƒä¿¡æ¯
    title = "é«˜åˆ†å­ææ–™è®ºæ–‡ - å”é‡‘é‡‘"
    author = "å”é‡‘é‡‘"
    
    if extracted_data:
        title = extracted_data.get('title_cn', title)
        author = extracted_data.get('author_cn', author)
    
    # æ·»åŠ æ ‡é¢˜å’Œå…ƒä¿¡æ¯
    markdown_lines.append(f"# {title}")
    markdown_lines.append("")
    markdown_lines.append(f"> ä½œè€…: {author}")
    markdown_lines.append("> è‡ªåŠ¨ä»DOCXæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼")
    markdown_lines.append(f"> è½¬æ¢æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    markdown_lines.append("")
    
    # æ·»åŠ AIæå–çš„ç»“æ„åŒ–ä¿¡æ¯
    if extracted_data:
        markdown_lines.append("## ğŸ“‹ è®ºæ–‡ä¿¡æ¯")
        markdown_lines.append("")
        info_fields = {
            'university_cn': 'å­¦æ ¡',
            'major_cn': 'ä¸“ä¸š',
            'degree_level': 'å­¦ä½çº§åˆ«',
            'supervisor_cn': 'å¯¼å¸ˆ',
            'defense_date': 'ç­”è¾©æ—¥æœŸ'
        }
        
        for field, label in info_fields.items():
            if field in extracted_data and extracted_data[field]:
                markdown_lines.append(f"- **{label}**: {extracted_data[field]}")
        markdown_lines.append("")
    
    # æ·»åŠ ç›®å½•
    if chapters:
        markdown_lines.append("## ğŸ“‹ ç›®å½•")
        markdown_lines.append("")
        for i, (line_num, title) in enumerate(chapters, 1):
            # ç®€å•çš„ç›®å½•ç”Ÿæˆ
            anchor = title.replace(' ', '-').replace('ã€', '').replace('.', '')
            markdown_lines.append(f"{i}. [{title}](#{anchor})")
        markdown_lines.append("")
    
    # å¤„ç†æ­£æ–‡å†…å®¹
    markdown_lines.append("## ğŸ“„ è®ºæ–‡æ­£æ–‡")
    markdown_lines.append("")
    
    # å°†æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²å¹¶æ ¼å¼åŒ–
    paragraphs = text.split('\n\n')
    for para in paragraphs:
        para = para.strip()
        if para:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            is_chapter = False
            for _, chapter_title in chapters:
                if chapter_title in para and len(para) < 100:
                    # è½¬æ¢ä¸ºMarkdownæ ‡é¢˜
                    level = "###" if any(word in para for word in ['ç¬¬', 'ç« ', 'èŠ‚']) else "####"
                    markdown_lines.append(f"{level} {para}")
                    markdown_lines.append("")
                    is_chapter = True
                    break
            
            if not is_chapter:
                # æ™®é€šæ®µè½
                markdown_lines.append(para)
                markdown_lines.append("")
    
    # æ·»åŠ å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
    if references:
        markdown_lines.append("## ğŸ“š å‚è€ƒæ–‡çŒ®")
        markdown_lines.append("")
        for i, ref in enumerate(references, 1):
            markdown_lines.append(f"{i}. {ref}")
        markdown_lines.append("")
    
    # æ·»åŠ ç”Ÿæˆä¿¡æ¯
    markdown_lines.append("---")
    markdown_lines.append("")
    markdown_lines.append("*æœ¬æ–‡æ¡£ç”±SmartReferenceExtractorè‡ªåŠ¨ç”Ÿæˆ*")
    markdown_lines.append("")
    
    return '\n'.join(markdown_lines)

if __name__ == "__main__":
    success = test_thesis_extraction_and_markdown()
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼")
