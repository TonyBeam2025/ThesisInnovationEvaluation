#!/usr/bin/env python3
import pytest
pytest.skip("integration tests require manual setup (API keys, large data)", allow_module_level=True)
from tests.integration import PROJECT_ROOT
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä½¿ç”¨æ™ºèƒ½ç›®å½•æå–ç±»çš„ç« èŠ‚åˆ†æåŠŸèƒ½
"""

import sys
import os
sys.path.append('src')

from thesis_inno_eval.extract_sections_with_ai import ThesisExtractorPro

def test_smart_toc_extraction():
    """æµ‹è¯•æ™ºèƒ½ç›®å½•æå–å’Œç« èŠ‚åˆ†æ"""
    print("ğŸš€ æµ‹è¯•æ™ºèƒ½ç›®å½•æå–çš„ç« èŠ‚åˆ†æåŠŸèƒ½")
    print("=" * 80)
    
    # æµ‹è¯•è®¡ç®—æœºåº”ç”¨æŠ€æœ¯è®ºæ–‡
    extractor = ThesisExtractorPro()
    doc_path = "data/input/1_è®¡ç®—æœºåº”ç”¨æŠ€æœ¯_17211204005-è‹æ…§å©§-åŸºäºMLPå’ŒSepCNNæ¨¡å‹çš„è—æ–‡æ–‡æœ¬åˆ†ç±»ç ”ç©¶ä¸å®ç°-è®¡ç®—æœºåº”ç”¨æŠ€æœ¯-ç¾¤è¯º.docx"
    
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {os.path.basename(doc_path)}")
    
    try:
        # å…ˆè¯»å–æ–‡æ¡£æ–‡æœ¬
        print(f"\nğŸ“– è¯»å–æ–‡æ¡£å†…å®¹...")
        if doc_path.endswith('.docx'):
            from thesis_inno_eval.extract_sections_with_ai import extract_text_from_word
            text = extract_text_from_word(doc_path)
        else:
            print("âš ï¸ å½“å‰ä»…æ”¯æŒWordæ–‡æ¡£æ ¼å¼")
            return None
        
        print(f"   æ–‡æ¡£é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æå–å†…å®¹
        print(f"\nğŸ”„ å¼€å§‹æ™ºèƒ½æå–...")
        result = extractor.extract_with_integrated_strategy(text, doc_path)
        
        print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
        print(f"  - æ€»å­—æ®µæ•°: {extractor.extraction_stats['total_fields']}")
        print(f"  - å·²æå–å­—æ®µ: {extractor.extraction_stats['extracted_fields']}")
        print(f"  - æå–ç‡: {extractor.extraction_stats['extracted_fields']/extractor.extraction_stats['total_fields']*100:.1f}%")
        print(f"  - æ•´ä½“ç½®ä¿¡åº¦: {extractor.extraction_stats['confidence']:.2f}")
        print(f"  - å¤„ç†æ—¶é—´: {extractor.extraction_stats['processing_time']:.2f}s")
        
        # æ£€æŸ¥ç›®å½•åˆ†æç»“æœ
        if 'toc_analysis' in result:
            toc_info = result['toc_analysis']
            print(f"\nğŸ“‹ ç›®å½•æå–ç»“æœ:")
            print(f"  - æå–æ–¹æ³•: {toc_info.get('extraction_method', 'unknown')}")
            print(f"  - ç½®ä¿¡åº¦: {toc_info.get('confidence_score', 0):.2f}")
            print(f"  - æ€»æ¡ç›®æ•°: {toc_info.get('total_entries', 0)}")
            print(f"  - æœ€å¤§å±‚çº§: {toc_info.get('max_level', 0)}")
            
            # æ˜¾ç¤ºç›®å½•ç»“æ„
            chapters = toc_info.get('table_of_contents', [])
            if chapters:
                print(f"\nğŸ“š ç« èŠ‚ç»“æ„ ({len(chapters)} ä¸ª):")
                print("-" * 60)
                for i, chapter in enumerate(chapters[:15], 1):  # æ˜¾ç¤ºå‰15ä¸ª
                    level_indent = "  " * (chapter.get('level', 1) - 1)
                    title = chapter.get('title', 'Unknown')
                    number = chapter.get('number', '')
                    confidence = chapter.get('confidence', 0)
                    print(f"{i:2d}. {level_indent}[L{chapter.get('level', 1)}] {number} {title} (ç½®ä¿¡åº¦: {confidence:.2f})")
                
                if len(chapters) > 15:
                    print(f"    ... è¿˜æœ‰ {len(chapters) - 15} ä¸ªç« èŠ‚")
            
            # æ˜¾ç¤ºç« èŠ‚åˆ†æç»“æœ
            chapter_summaries = toc_info.get('chapter_summaries', {})
            if chapter_summaries:
                print(f"\nğŸ§  ç« èŠ‚AIåˆ†æç»“æœ ({len(chapter_summaries)} ä¸ª):")
                print("-" * 60)
                for chapter_title, summary in list(chapter_summaries.items())[:5]:
                    print(f"ğŸ“– {chapter_title[:40]}...")
                    if isinstance(summary, dict):
                        print(f"   æ‘˜è¦: {summary.get('summary', 'N/A')[:100]}...")
                        print(f"   å…³é”®ç‚¹: {summary.get('key_points', [])[:3]}")
                    else:
                        print(f"   å†…å®¹: {str(summary)[:100]}...")
                    print()
            
            # æ˜¾ç¤ºæ–‡çŒ®ç»¼è¿°åˆ†æ
            lit_analysis = toc_info.get('literature_analysis', {})
            if lit_analysis:
                print(f"ğŸ“š æ–‡çŒ®ç»¼è¿°åˆ†æ:")
                print(f"   ç ”ç©¶ä¸»é¢˜: {lit_analysis.get('research_themes', [])[:3]}")
                print(f"   æ–¹æ³•è®º: {lit_analysis.get('methodologies', [])[:3]}")
                print(f"   ç ”ç©¶å·®è·: {lit_analysis.get('research_gaps', [])[:2]}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = "smart_toc_analysis_result.json"
        try:
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                # ç®€åŒ–ç»“æœä»¥ä¾¿JSONåºåˆ—åŒ–
                simplified_result = {}
                for key, value in result.items():
                    if isinstance(value, (str, int, float, bool, list, dict)):
                        simplified_result[key] = value
                    else:
                        simplified_result[key] = str(value)
                
                json.dump(simplified_result, f, ensure_ascii=False, indent=2)
            
            print(f"\n è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {output_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        
        return result
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_smart_toc_extraction()
